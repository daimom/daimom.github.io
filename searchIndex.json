[
{
		"title": "Steam deck安裝筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/191. Steam deck安裝除錯記/",
		"content": "前言\n好久沒買聖誕禮物了，\n前陣子又有人說steam deck oled很推，\n看了幾次都沒現貨，\n上週五，剛好喵到pchome有貨，下訂隔天到。\n禮拜六吃喜酒，又逛了一下lalaport，\n禮拜天中午跟朋友吃飯，才開始動工，\n然後就是一整個踩坑記。\n禮拜天開始安裝，第一次搞steam一堆問題。\n正文\n安裝時，一直卡在安裝剩餘1秒\n我試了幾種方法，\n\n重開機（沒用），重開了五六次，還是卡住\n\n看redit說 不要連2.4g的wifi，重開機安裝，一樣卡\n\n不斷關閉wifi後，重連我也試過，沒用(設定開關在右下方的那三個點）。\n\n最後改連手機的熱點分享，終於過了\n\n外掛推薦(不裝也可以）\n目前看到的都是一些比較個人化的東西，\n也可能我遊戲玩不多，\n之前主要都在玩p4\n第一步先安裝 Decky loader，可以參考 版上的 這篇\n（桌面模式，按左下的steam按鈕-&gt;電源-&gt;切換至桌面）\n（沒外接鍵盤的話，需按下 steam按鈕+X ，開啓螢幕小鍵盤)\n前面的設定先改好，後面安裝不需要輸入指令，在網站上點擊即可下載，\n然後對着下載下來的檔案，點兩下安裝，\n最後從桌面上的 圖示，回到掌機畫面。\n目前我裝的有\n\nBluetooth ： 據說是能讓藍牙耳機自動連線\n\nAnimation Changer：關機(長按電源）、暫停（短按電源）的畫面修改\n\nProtonDB Badges：判斷遊戲在裝置上的可玩性\n\nSteamGridDB：修改掌機模式下遊戲的圖片，裝這個是掌機模式下的Chrome沒圖案真的太醜了。\n\nCSS-Loader：桌面佈景，可以到官網看有哪些\n\nAnimation Changer ，影片放大\n安裝CSS-Loader，然後搜尋  Full Suspend ，安裝完後即可。\n補丁安裝\n前面步驟，請看這一篇steam deck夏日狂想曲中文18+補丁，如果你bottle無法建立的話再回來。\n同事一直推坑我夏日狂想曲，然後bottle的bottles無法建立，一直卡在Arial font的字體安裝失敗，\n查原文有人推薦用Q4wine，就試試看了。\n但在flatsel需將Q4wine底下的Filesystem中的All system files打開，\n安裝時才能夠讀取到steam deck夏日狂想曲中文18+補丁 這邊說的\nZ:\\home\\deck\\Summer Memories\n\n額外補充\n\nprotonDB ：查遊戲相容性的網站，不同獎牌代表的意思\n\n白金：完美執行。\n金：調整後完美執行。\n銀：有小問題，但可玩。\n銅：可玩，但經常遇到遊戲崩潰等問題。\n無法執行：如題。\nOled燈號閃爍意思\n\n狀態\n說明\n\n三閃紅燈\n電池電力不足無法開機\n\n啟動中(綠燈)\n主機在PSU連接狀態且電池已經充滿\n\n啟動中(白燈)\n主機在PSU連接狀態且充電功率與PD協定有成功辨識並在電量達到觸發閾值後進入旁路充電\n\n啟動中(橘燈)\n主機在PSU連接狀態且充電功率不足或PD協定無法辨識,簡單說就是以低功率充電\n\n啟動中(紅燈)\n偵測到主機溫度高於95度C,無法開機\n\n呼吸白燈\n主機開機中\n\n呼吸藍燈\n主機更新韌體或系統中\n\n無亮燈\n主機關機,且沒有偵測到PSU連接\n\nsteamdb：查遊戲特價歷史價格\n重置sudo 密碼\n如果你忘記密碼（因爲我就忘了），\n只剩下重刷這一條路了。\n將steam deck關機，按住右下方的『...』與開機按鈕開機，\n然後清除資料吧。\n組合鍵\n長按Steam按鈕，有快速鍵列表\nsteam + R1 ： 截圖\nsteam +X ： 開啓螢幕鍵盤\n『 ... 』+類比上下：調整螢幕亮度\n『 ... 』+電源： Recovery Manager\n音量『 - 』+電源 ：Boot Manager\n音量『 + 』+電源  ：BiOS\n配件\n未來應該再來下面幾個東西，再觀察看看需不需要了。\n\ntomtoc Steam Deck硬殼收納包 ：看起來跟steam deck很合\nInnergie C6 Duo 63瓦 USB-C 雙孔：臺達的充電器，現在有一個單孔的，但有時要拔來拔去不方便。\ntype c hub：感覺還是頗需要，如果要用桌面模式接鍵盤、滑鼠或外接螢幕的話，這個要再找找看了。\n\n目前應該就這樣，\n剩下的等有碰到再研究。\nref.\n\nSteam Deck 大樓！\nSuspend animations are extremely small using Deck Loader\n[閒聊] SteamDeck兩周心得(配件、調校、套件)",
		"tags": [ "note","🎮"]
},

{
		"title": "Obsidian發佈到github.io個人網站",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/193. Obsidian發佈到github.io個人網站/",
		"content": "前言\n前陣子弄好了Obsidian自動發佈到vercel，\n成果也還不錯，但就是偶爾有點問題，\n因為是免費用戶，所以會碰到無法更新。\n有天一怒之下，就自己改成用github了。\n正文\n首先前面還是一樣<a class=\"internal-link\" data-note-icon=\"\" href=\"/🆒 SideProject/160.Obsidian發佈網站/\">160.Obsidian發佈網站</a>，\n步驟要先設定好從Obsidiant傳到你的github。\n\n當你在Obsidian選擇publish時，會將內容傳到public_vercel 這個repository。\n\n所以，接下來的流程是，在public_vercel 使用github action，\n將obsidian的內容編譯成html，再轉到github的網站上面。\n\n到github建立個人網頁\n到github，建立一個自己使用者的子網域，\nrepository的名稱會是 user.github.io 這種名稱，\n只有user這個字可以改。\n建立完後，可以先弄個index.html到github裏面，\n然後訪問剛剛建立的網址，\n我建立的名稱為daimom.github.io\n\n所以網址就是 daimom.github.io\nref. github_page\n\n將npm轉成pnpm\n其實這一步也可以不做，\n那就是寫github action的時候，不能用pnpm只能用npm，\n程式應該也要小改一下，\n但pnpm會比較快，我就順便改一改了。\n\n到 public_vercel的資料夾底下，執行下面三行，參考從 npm 遷移到 pnpm，\n有些步驟我測試是不需要，\n重點是 pnpm import，要將package-lock.json 轉成 pnpm-lock.yaml。\n# 刪除npm package\nrm -rf node_modules\n# 安裝pnpm\nnpm install -g pnpm\n# 轉換相關檔案\npnpm import\n\n完成後，上傳到github。\n\n建立github action\n資料夾結構如下\n\n能改檔案名稱的只有build-site.yaml以及 install-pnpm，\n但後者要改的話，程式內容也需要改，不熟的話不建議更改。\n\nbuild-site.yaml的程式如下\nname: 'Build Sites'\n\non:\nworkflow_dispatch:\ninputs:\ndeployment-version:\ndescription: 指定版本\nrequired: false\nschedule:\n- cron: '0 10 * * *'\njobs:\nbuild-site-test:\nruns-on: ubuntu-latest\nsteps:\n# [Checkout]\n- name: Checkout code repository\nuses: actions/checkout@v4\nwith:\nfetch-depth: 0\n\n# [執行腳本] 構建 pnpm 及 Node 環境\n- name: '[Macro] Prepare environment'\nuses: ./.github/actions/install-pnpm\n\n# [執行腳本] 構建html\n- name: '[Macro] Build sites '\nshell: bash\nrun: pnpm build\n\n- name: Push to HTML Repository\nrun: |\ngit config --global user.email $\ngit config --global user.name &quot;daimom&quot;\ngit config --list\ngit clone https://$@github.com/daimom/daimom.github.io.git temp-html-repo\ncp -r dist/. temp-html-repo/\n\ncd temp-html-repo\ngit add .\ngit commit -m &quot;Update from Build Repository&quot;\ngit remote -v\ngit push origin main\n\ninstall-pnpm/action.yaml的程式如下\nname: 'prepare'\ndescription: 'Prepare environment'\n\nruns:\nusing: 'composite'\nsteps:\n- name: Setup pnpm\nuses: pnpm/action-setup@v2\nwith:\nversion: 8\n- name: Setup node.js\nuses: actions/setup-node@v4\nwith:\nnode-version: 20\ncache: 'pnpm'\n\n- name: Install dependencies\nshell: bash\nrun: pnpm install\n\n簡易說明\n這邊的設定是每天的UTC 10:00 執行一次，\n如果要自定時間的話，可以參考Crontab 中文線上 編輯器\n測試的時候可以先把\nschedule:\n- cron: '0 10 * * *'\n\n改成\npush:\nbranches:\n\t- 'main'\n\n這樣只要推上去main就會觸發動作了(注意排版)。\nref. 以 GitHub Action 來發推文\n或者是直接手動部署，\n在public_vercel的github action選擇你的Action ，再選 Run workflow\n\n程式裏面的 $ 是要到repository裏面設定。\n\n也可以直接輸入你的email，反正email這東西應該已經...。\n但重點是底下的 $ ，\n這個一定要用secret隱藏，而且不能外流。\n產生的方式，詳閱， <a class=\"internal-link\" data-note-icon=\"\" href=\"/🆒 SideProject/160.Obsidian發佈網站/#github-token\">160.Obsidian發佈網站#建立github token</a>\n再來將產生的token，新增到sercret裏面。\nref.\n\nGitHub Actions 文件\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/110.github action pipeline筆記/\">110.github action pipeline筆記</a>",
		"tags": [ "note","🆒"]
},

{
		"title": "200. Net core SDK安裝至container",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/200. Net core SDK安裝至container/",
		"content": "前言\n後端之前發了一個版本上去後，\n記憶體就一路往上飆，\n他們也查不出個所以，\n後來幫他們查，有請他們加個程式碼試試，看來也成功了。\n但為了以後，還是知道一下要怎麼抓記憶體的用量比較好，\n就交給他去看了。\n&lt;PropertyGroup&gt;\n&lt;ServerGarbageCollection&gt;false&lt;/ServerGarbageCollection&gt;\n&lt;/PropertyGroup&gt;\n\nref. 【譯】Asp.net core應用在 Kubernetes上記憶體使用率過高問題分析\n正文\n以下為同事執行的指令，\n可能需要先安裝wget之類的東西。\n\n裝 dotnet SDK\n\nwget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh\nchmod +x ./dotnet-install.sh\n./dotnet-install.sh --version latest\n\n執行\n\n裝完後，最好先用 find / -iname dotnet ，\n查一下安裝後的dotnet在哪。\n執行的時候，記得用 ./dotnet --version ，的方式執行。\n不然，預設是用/usr/bin 裏面的dotnet執行的。\n\n除錯\n沒研究，但同事是參考下面這篇去實作。\n參考連結 Identifying Memory Leaks with dotnet-dump and dotnet-gcdump\n\nref.\n\n已編寫指令碼的安裝\nIdentifying Memory Leaks with dotnet-dump and dotnet-gcdump",
		"tags": [ "note","🐳"]
},

{
		"title": "201. kustomize patch",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/201. kustomize patch/",
		"content": "前言\n懶癌發作，要部署新環境時，\n想到每個deployment要一個一個改，就好懶。\n該換擬出來了 kustomize，\nkubectl 現在預設就有支援，以前還要單獨裝指令。\n正文\n查看kustomize的結果\nkubectl kustomize ./\n\n執行方式\nkubectl apply -k &lt;folder path&gt;\n\n修改映像檔\n要注意得點是， deployment.yaml 的image裏面的映像檔名稱，\n要對應到kustomize.yaml 的 images.name。\n以下面的範例來說， my-image:1.2.3 其中my-image就是映像檔名稱，\n所以在kustomize.yaml裏面的image name要寫 my-image\ndeployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: my-app\nspec:\ntemplate:\nspec:\ncontainers:\n- name: my-container\nimage: my-image:1.2.3\n\nkustomize.yaml\nresources:\n- ../../../base\nimages:\n- name: my-image\nnewName: my.image.registry/nginx\nnewTag: 1.4.0\n\n修改其他欄位\n除了一般能修改的東西，能用 cross-cutting field 修改以外，\n其他要修改只能透過patch的方式。\n其中 patchesJson6902跟 patchesStrategicMerge，\n目前已經被patches取代，也將於後續版本deprecated.\n且 patches能夠在 name以及namespace上使用regex。\nref. What is the difference between patches vs patchesJson6902 in Kustomize\ncross-cutting 基本有這些\n\n如果上面有用到 cross-cutting 的參數，下面的原始檔，\n只要寫原始的就好，不用寫修改過的參數。\n例如 namespace 在原始的deployment.yaml是default，\n然後在 kustomize.yaml 裏面 修改成 game，\n那Patches還是寫原本的default。\nPatches底下的patch，只要有以下資訊即可，\n然後就是要修改的設定了。\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: systemmanageservice\n\n下面範例是，修改container底下的env變數。\npatches:\n- target:\nkind: Deployment\nname: systemmanageservice\npatch: |-\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: systemmanageservice\nspec:\ntemplate:\nspec:\ncontainers:\n- name: systemmanageservice\nenv:\n- name: ASPNETCORE_ENVIRONMENT\nvalue: QA\n\nref.\n\nKustomize 功能特性列表\npatches\n\npatchesJson6902\n上面有提到patches是 patchesJson6902 與\npatchesStrategicMerge的結合。\n於是 patches也支援下面這種用法。\n使用 op 來作新增或取代。\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\npatchesJson6902:\n- target:\nversion: v1\nkind: Deployment\nname: my-deployment\npatch: |-\n- op: add\npath: /some/new/path\nvalue: value\n- op: replace\npath: /some/existing/path\nvalue: &quot;new value&quot;",
		"tags": [ "note","⎈"]
},

{
		"title": "GCP多帳號切換",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/202. GCP多帳號切換/",
		"content": "前言\n最近適逢專案搬家時期，帳號要切來切去，\n結果一切過去就不能用了。\n錯誤寫，你的B帳號無法在A專案使用。\n正文\nOS是 MacOS，window會不會有相同錯誤不清楚\n試了幾次，切換GCP帳號的指令都沒用，\n最後摸出一套可行的步驟\n\n先確定目前使用的gcp帳號是哪一個，* 代表使用中的帳號\n\ngcloud auth list\n\n然後確認config設定有沒有錯\ngcloud config list\n\n如果帳號或project有錯誤，就修改\n# 修改帳號\ngcloud config set account abc@abc.com\n# 修改project\ngcloud config set project project_id\n\n確認要使用專案的context設定\n指令\n\n# 取得所有cluster設定\nkubectl config get-contexts\n# 切換要使用的cluster\nkubectl config use-context [NAME]\n# 檢查目前使用中的cluster\nkubectl config current-context\n\nDocker 切換\n選擇環境後滑鼠點一下\n\n完成後，重開 Terminal\n一定要重開！！！！不然會像鬼打牆一樣，一直跳錯誤。\n\n差點都想直接用 gcloud auth revoke &lt;account&gt; 把帳號強制登出了。\n2024/04/11\n要切帳號時，terraform又掛了，一直提示我用另一個帳號。\n但用kubectl 指令時，都正常。\n後來執行這行，\ngcloud auth application-default login\n\n重新登入後正常。\nref.\n\n[GCP]gcloud切換帳號、專案\ngcloud auth",
		"tags": [ "note","☁️"]
},

{
		"title": "istio Authorization 拒絕指定檔案瀏覽",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/203. istio Authorization 拒絕指定檔案瀏覽/",
		"content": "前言\n接到個需求，要限定網站的某個副檔名不能被瀏覽。\n正文\nAuthorization-policy的yaml\nRule規則分成三大塊\nFrom\n請求來源\nTo\n請求目的\nWhen\n指定的請求附加條件\nTo\nnginx查一下，是有類似的東西。\n但istio我看了官方文件，\n寫法是\n- to:\n- operation:\npaths: [&quot;/user/profile/*&quot;]\n\n目前只有支援單個 *\n\nExact match: abc will match on value abc.\n完全匹配： abc 將匹配值 abc 。\nPrefix match: abc* will match on value abc and abcd.\n前綴匹配： abc* 將匹配值 abc 和 abcd 。\nSuffix match: *abc will match on value abc and xabc.\n後綴匹配： *abc 將匹配值 abc 和 xabc 。\nPresence match: * will match when value is not empty.\n存在匹配：當值不為空時， * 將匹配。\n\n現在的目標是要阻擋 index-erscvs.js.map ，禁止任何人觀看。\n但 index-erscvs.js 不能被deny\n於是\nkind: AuthorizationPolicy\nmetadata:\nname: frontend-map-policy\nspec:\nselector:\nmatchLabels:\ngroup: frontend\naction: DENY\nrules:\n- to:\n- operation:\nhosts: [&quot;abc.def.com&quot;]\nports: [&quot;80&quot;]\npaths: [&quot;*.js.map&quot;]\n\n優先順序評估表 Custom &gt; Deny &gt; Allow\nref. Authorization Policy\nWhen\n注意，values不支援CIDR，需用* 代替私有網段\n這邊沿用 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/186. istio的Authorization policy(白名單)/\">186. istio的Authorization policy(白名單)</a> 的方式，\n設定白名單。\n於是，下面的設定方式，label 是 group=frontend的deploy，\n不允許訪問*.js.map的檔案，除了來源是 123.123.123.123\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\nname: frontend-map-policy\nspec:\nselector:\nmatchLabels:\ngroup: frontend\naction: DENY\nrules:\n- to:\n- operation:\nhosts: [&quot;abc.def.com&quot;]\nports: [&quot;80&quot;]\npaths: [&quot;*.js.map&quot;]\nwhen:\n- key: request.headers[X-Envoy-External-Address]\nnotValues:\n- &quot;123.123.123.123&quot;\n\n驗證時，可以用下面的指令協助查詢，\n要注意是不是被其他的policy影響了\n# 先開啟debug模式，資訊比較多，查完後將deploy重啟即可\nistioctl proxy-config log deploy/httpbin --level &quot;rbac:debug&quot; | grep rbac\n\nfor i in {1..20}; do curl -H 'Cache-Control: no-cache, no-store' https://abc.def.com/assets/index-d74bceac.js -s -o /dev/null -w &quot;%{http_code}\\n&quot;; done\n\nkubectl logs &lt;pod name&gt; -c istio-proxy\n\n注意\nwhen 跟 to 如果用 下面方式的話，表示是兩條規則\n...\n\t- to:\n\t\t...\n\t- when:\n\t\t...\n\n這樣才是同一條規則，\n我犯了這個蠢錯誤\n\t- to:\n\t\t...\n\t when:\n\t\t...\n\nref.\n\nAuthorization Policy\nAuthorization Policy Conditions\nauthorizationPolicy詳解",
		"tags": [ "note","⛵️"]
},

{
		"title": "205. Grafana 筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/205. Grafana 筆記/",
		"content": "前文\n正文\n單位換算\nbyte(IEC)：以1024轉換計算。2MB = 2042 kb\nbyte(SI) ： 以十進位計算。 2MB= 2000kb\n更換線條顏色\n\n先把Legend 打開\n點選Legend的顏色\n\n隱藏\nOverrides裡面的選項，有時會被打勾，導致線圖不會出現\n匯出\n\n點選標題的右邊分享按鈕\n\n選擇Export ，再來看要存檔或檢視Json",
		"tags": [ "note","👁"]
},

{
		"title": "steamDeck 的 Cheat Engine替代品",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/206. steamDeck 的 Cheat Engine替代品/",
		"content": "前言\n春特入手了幾款 跟弟弟一起玩的遊戲，\n但有些時候重複刷的有點煩，\n就想說改一下省點時間，\n查了一些資訊，發現CheatEngine不是不能裝，\n但有夠麻煩！！！！！\n正文\n鎮重聲明，修改有一定風險。\n感謝巴友C.C告知另一套軟體『memory deck』\n直接在Decky loader 搜尋，安裝即可。\n搜尋方式也很簡單，直接按掌機右下的 ... 就能搜尋。\n缺點是每次要改就要重新搜尋一次。\n最後在reddit找到了 memory_hack\n這款的好處在於，steam deck(桌面模式)開啟遊戲後，\n用手機或電腦在同一個wifi底下，開啟網頁即可修改。\n安裝方式也很簡單，\n\n建立一個資料夾，要放安裝檔\n\npython3 &lt;(wget -qO- https://github.com/primetime00/memory_hack/raw/master/app/patches/install.py)\n\n安裝完後，會問你要不要當成預設程式，我是選n ，當有需要時再到剛剛的資料夾啟動就好。\n\n啟動指令，到該資料夾底下執行   ./run.sh\n安裝說明連結\n然後就可以先在steam deck上面開網頁，http://localhost:5000 看有沒有資料。\n如果要用手機或電腦連的話，\n可以另外開啟konsole ，執行 ip -4 addr  就會看到你的ip，假設是192.168.0.125，\n在手機上開啟 http://192.168.0.125:5000 ，就會看到畫面了。\n簡易操作\n\n到 Search 的頁籤上，選擇 Process的 遊戲名稱\n\n輸入數值查詢，通常要兩次查詢，第一次有很多資料，當數值變動時，再搜尋一次，就會有了\n\n複製記憶體位置\n\n到 Code的頁籤，選擇 Process的遊戲名稱\n\n輸入數值，旁邊的方格，打勾表示鎖定數值。\n\np.s 改完後，數值可能不會馬上重現，讓遊戲的數值變動一下。\n完整操作說明，請參考 Memory Hack Beginner Tutorial\n之前有人介紹另一款，\nMemory Deck ，直接在掌機模式下的Decky loader 商店搜尋，\n裝起來就好了。",
		"tags": [ "note","🎮"]
},

{
		"title": "207. mapgenie整批unfound",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/207. mapgenie整批unfound/",
		"content": "前言\n前陣子玩遊戲時發現一個網站，mapgenie.io\n裡面有很多遊戲的地圖，\n按下found以後，不想一個一個把勾拿掉。\n正文\n有餘力的話，應該是買VIP會員才對，\n但我覺得我用不到那麼多功能，\n可能這個功能也是VIP專用。\n畢竟花錢就是要省時間，不想花錢又想省時間的話，\n洗洗睡吧。\n\n打開網頁的開發者工具，\n找到 body &gt; div#app\n底下的scripts，會長的像這樣\n\n最上面有一些數字，那些數字就是我們要的。\n\n選擇網路頁籤\n再來將一個found關掉，會看到底下的Fetch/XHR，新增一筆。\n點選後，注意看箭頭是不是寫DELETE\n\n複製成CURL\n對著剛剛的58673按右鍵，選擇複製成CURL\n\n然後找個記事本貼上，你會發現一長串的東西。\n我測試過，可以把 -H 'cookie:' 整段拿掉，\n其他東西應該就不好刪了。\n\n再來就是更換最上面的數字，就不用一個一個點標誌，然後把勾拿掉了。\n上面的圖片，我只節錄了三行，完整的不只這樣。\n然後將curl丟去 命令提是字元執行就好了。\n更懶一點，弄個for迴圈，把數字自動帶進去網址。\n可以參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/135. gsutil整批複製/\">135. gsutil整批複製</a>\n還有另一種就是去找newman去搞自動化帶入參數應該也能做到。",
		"tags": [ "note","🆒"]
},

{
		"title": "208. 架設sftp",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/208. 架設 sftp/",
		"content": "事由\n需要一個地方放大量的資料，日後提供給客戶下載，\n本來想說用GCS就好，但如果要一次下載好幾個檔案就沒辦法了。\n只好乖乖弄FTP。\n正文\n一開始用vsftp，搞個docker 大概沒10分鐘就好了。\n有一半的時間在找image上面，\n但如果要達成『一個使用者，可以看到所有使用者的目錄，其他使用者需要獨立』就懵了。\n後續找資料時，看到鳥哥說了一句話，『所以，建議您，除非必要，否則的話，使用 SSH 提供的 sftp-server 功能即可』\nref. FTP 的安全性問題與替代方案\n所以我就轉往sftp了。\n先來釐清目標，\n\n一個最高權限的帳號，\n可以在所有使用者的目錄新增檔案，\n然後所有使用者間是不能互相看到彼此的檔案。\n\ndocker image 使用 corilus/sftp，\n下載量已達1M。\n如果只是簡單的ftp功能，直接架起來就好了。\n但要做成上面的目標，要用mount的方式，將資料夾掛載到主目錄底下。\n詳細作法參考下面。\ndocker-compose.yaml\nversion: '3'\nservices:\nsftp:\ncontainer_name: sftp\nimage: corilus/sftp\nprivileged: true\nvolumes:\n- ./mount.sh:/etc/sftp.d/bindmount.sh\n- ./userlist.conf:/etc/sftp/users.conf:ro\n- ./upload:/home\nports:\n- &quot;2222:22&quot;\nrestart: always\n\n指令說明，可參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/108. linux shell 進階指令/#bdeaee\">108. linux shell 進階指令#^bdeaee</a>\nmount.sh\n#!/bin/bash\n# File mounted as: /etc/sftp.d/bindmount.sh\n# Just an example (make your own)\n\nfunction bindmount() {\nif [[ ! -d &quot;$1&quot; ]]; then\nmkdir -p &quot;$1&quot;\nfi\n\nif [[ ! -d &quot;$2&quot; ]]; then\n\t mkdir -p &quot;$2&quot;\nfi\n\nchown -R :100 &quot;$1&quot; &quot;$2&quot;\nchmod 775 &quot;$1&quot; &quot;$2&quot;\nmount --bind &quot;$1&quot; &quot;$2&quot;\n}\n\n# Remember permissions, you may have to fix them:\n# chown -R :users /data/common\nbindmount &quot;/home/mike/upload&quot; &quot;/home/ms/mike&quot;\nbindmount &quot;/home/aaron/upload&quot; &quot;/home/ms/aaron&quot;\n\nuserlist.conf\nms:ms:1001:100\nmike:mike:1002:100\naaron:aaron:1003:100\n\n本來想用ln的方式去操作，\n從terminal 能夠進去資料夾，但用FTP登入就是說 no such file or dirctory\n問了老半天GPT也都是失敗，\n從一開始說要加個不存在的參數 FollowSymlinks ，\n到後面的一連串重複回答。\n最後看到 Symbolic link and filezilla over sftp\n看來是ln的先天限制，所以改用mount後，調個權限，好了。\nref.\n\nCorilus/sftp Github",
		"tags": [ "note","🐧"]
},

{
		"title": "209.k8s中srs的各個pod  metrics取得",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/209. k8s中srs的各個pod  metrics取得/",
		"content": "事由\n同事正在弄SRS串流，照我以前的寫法，\n是透過service去取得單一個pod的metrics，\n這樣只會有一個pod的資料，\n所以要想辦法取得所有pod的資料。\n正文\n由於我有套用istio，\n於是有套istio-proxy的pod通通會預設塞\nprometheus.io/path: /stats/prometheus\nprometheus.io/port: '15020'\nprometheus.io/scrape: 'true'\n\n所以直接在deploy加參數進去時，也會被改寫。\n於是只好額外寫job 如下\n- job_name: 'srs'\nscrape_interval: 10s\nstatic_configs:\n- targets:\n- 'srs.default.svc.cluster.local:9972'\n\n呼叫 service 的 srs ，然後去取得metrics的metrics。\n但當有多個pod的時候，這就會出問題了。\n因為service到pod的流量平均分配，你不會知道他連去哪個pod。\n但今天我要抓全部metrics，這種寫法就會出問題。\n其中 static_configs 這個只適用於 固定字串，\n所以對於會變來變去的pod ip沒有用。\n於是要改成\n- job_name: 'srs'\nscrape_interval: 10s\nkubernetes_sd_configs:\n- role: pod\nrelabel_configs:\n- source_labels: [__meta_kubernetes_pod_name]\naction: keep\nregex: srs-.*\n- source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\naction: replace\ntarget_label: __metrics_path__\nreplacement: /metrics # 強制指定路徑為 /metrics\n- source_labels: [__meta_kubernetes_pod_container_port_name]\naction: replace\ntarget_label: portname\nreplacement: 9972\n\n規則由上往下依序執行，有符合才會往下。\nkubernetes_sd_configs 有這5種方式(GPT抄來的)\n\nrole: endpoints：\n\n這個組態項告訴 Prometheus 去發現 Kubernetes 中的端點對象。端點對象代表了服務的後端 Pod 的 IP 地址和連接埠。\n通過此組態項，Prometheus 可以直接與後端 Pod 進行通訊，收集指標資料。\n\nrole: service：\n\n這個組態項告訴 Prometheus 去發現 Kubernetes 中的服務對象。\nPrometheus 將會為每個服務發現服務的所有後端 Pod，並從每個 Pod 中抓取指標資料。\n\nrole: pod：\n\n這個組態項告訴 Prometheus 去發現 Kubernetes 中的 Pod 對象。\nPrometheus 將會為每個發現的 Pod 收集指標資料。\n\nrole: ingress：\n\n這個組態項告訴 Prometheus 去發現 Kubernetes 中的 Ingress 對象。\nPrometheus 將會監視 Ingress 對象，並收集與之相關的指標資料。\n\nrole: node：\n\n這個組態項告訴 Prometheus 去發現 Kubernetes 中的節點對象。\nPrometheus 將會為每個節點收集相關的指標資料。\n\n再來是重點 relabel_configs\n分別有\n\nsource_labels：來源標籤，可以參考prometheus的UI，上面有顯示很多label\n\naction：四種動作（keep,drop,replace,labelmap）。\n決定你要將值保留、終止、取代或標籤對應\ntarget_label：目的標籤，\n抓資料的url，由下面三種特殊標籤組成__scheme__ + __address__ + __metrics_path__。\n範例： https://192.168.103.224:9527/metrics\n主要是決定特殊標籤要用什麼，不然隨便取名都可以。\nregex，當作是條件判斷這裡面的值要符合規則能往下。\n下面regex意思是，pod name要符合srs-.*的才保留。\n\n- source_labels: [__meta_kubernetes_pod_name]\naction: keep\nregex: srs-.*\n\nreplacement：變更的值\n\nref.\n\nSRS-Prometheus Exporter\nRelabeling 重新標記\n[Prometheus] Service Discovery &amp; Relabel",
		"tags": [ "note","👁"]
},

{
		"title": "210. Arc Browser筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/210. Arc Browser筆記/",
		"content": "前言\n最近看到一個頗神的瀏覽器，Arc from The Browser Company\n最近剛出了win11版本，mac則要12版本以上才能使用。\n我的mac是2015年的可以安裝。\n正文\n可以先看一下這個Arc 真香！腦洞大開的功能讓超多 Chrome 死忠粉決定把 Arc 當作預設瀏覽器了！ 裡面有簡單的教學。\n\n要注意的是，現在的版本已經把內建Note砍掉了，\n如果要使用只能用線上的。\n打開 Preferences &gt; Profiles &gt; New documents\n選擇你要設定的線上服務\n\n瀏覽器設定檔\nARC 本身也是用chrome的核心去做的，\n所以除了Arc本身的設定以外，還有chromium的設定。\n進去的方式 Preferences &gt; Profiles &gt; Search Settings..\n\n目前會常用到的快速鍵\n⌘+S：開關左邊的SideBar\n⌘+T：開啟新分頁\n\n注意，開啟的分頁預設只會維持12hour，\n更改可到 圖1 的Archive tabs after修改\n\n⌘＋⌥(options)＋G：快速開啟chatGPT\n\nArchive的歷史紀錄\n在sidebar 往左滑即可。\n或直接點選",
		"tags": [ "note","🆒"]
},

{
		"title": "211. k8s service endpoint無法連結",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/211. k8s service與 endpoint無法連結/",
		"content": "前言\n某天在建立新的叢集，\n服務都已經架好準備給RD了，\n最後才發現，平常用的連線方式連不到GCP的Memorystore.Redis。\n這個問題卡了整整一週，\n不斷的刪除重建、比對、找差異。\n故事展開\nGKE 版本： 1.28.7-gke.1026000\n先簡單說一下架構，\nistio增加6379 的port ，\n新增redis的svc以及endpoint，\n由於redis是GCP的服務，故本身沒有pod，\n讓istio能夠透過virtusalService連到這個service，\n然後RD透過這個ip以及port連到 GCP的Redis。\n先來看錯誤的使用方式，\n\n在port name上面設定redis的名稱，\n導致svc的endpoints上面無法綁定。\n不設定port name的話\n\n則可以正常綁定。\n目前猜測，可能跟 IANA的port name有關係。\n但為什麼會這樣，\n就不清楚了。\n# Service Name and Transport Protocol Port Number Registry\n目前測試出來，只要有設定Name就會綁不到endpoints。\n另外，EndpointSlice目前(2024/05/20)仍會無法綁定，\n請先改用Endpoint的api。\n\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\nname: redis # must be the same as service name\nnamespace: istio-system\nlabels:\n# You should set the &quot;kubernetes.io/service-name&quot; label.\n# Set its value to match the name of the Service\nkubernetes.io/service-name: redis\naddressType: IPv4\nports:\n- name: redis # should match with the name of the service port defined above\nappProtocol: redis\nprotocol: TCP\nport: 6379\nendpoints:\n- addresses:\n- &quot;10.1.11.43&quot;\n\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis\nnamespace: istio-system\nspec:\nports:\n- protocol: TCP\nport: 6379\ntargetPort: 6379\nname: redis\n\n參照官方文件的作法也不會綁定。\nServices without selectors\n2024/05/8\n有另一個工程師告知，\nendpoint那邊的Port Name也加上去的話，\n就好了。\n結論\n頂多改改 Endpoints的address就好了，\nport name那些，別亂加。\n要加就要一起加，且名稱也要一樣。\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\nname: redis # must be the same as service name\nnamespace: istio-system\nsubsets:\n- addresses:\n- ip: 10.122.11.43\nports:\n- port: 6379\n\t name: redis\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis\nnamespace: istio-system\nspec:\nports:\n- protocol: TCP\nport: 6379\ntargetPort: 6379\nname: redis",
		"tags": [ "note","⎈"]
},

{
		"title": "212. k8s的親和性與反親和性",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/212. k8s的親和性與反親和性/",
		"content": "緣由\n後端的服務，沒有HPA，所以目前都單Pod在執行。\n但很不巧的，都在同一個Node裡面。\n所以當一個Node出問題時，其他服務會一起陣亡。\n但只有一個NodePool，也沒辦法用nodeSelector。\n正文\nGKE版本：1.27\n先上範例，詳細說明在下面(格式有點跑掉，複製使用時要注意）。\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: bms\nlabels:\ngroup: svc\napp: backend\nspec:\nreplicas: 1\nselector:\nmatchLabels:\ngroup: svc\napp: backend\ntemplate:\nmetadata:\nlabels:\ngroup: svc\napp: backend\nspec:\ncontainers:\n- name: service\nimage: my-image:v1.0.6\nports:\n- name: http\ncontainerPort: 80\nprotocol: TCP\naffinity:\npodAntiAffinity:\n\t\t\tpreferredDuringSchedulingIgnoredDuringExecution:\n\t - weight: 100\n\t\t podAffinityTerm:\n\t labelSelector:\n\t matchExpressions:\n\t - key: app\n\t operator: In\n\t values:\n\t - gcp\n\t topologyKey: kubernetes.io/hostname\n\n這個的用途，主要是讓pod決定要跟哪個服務在一起，或不在一起。\n其中比較重要的參數，\n反親和性與親和性\n\npodAntiAffinity\npodAffinity\n\n前者是反親和性，不要跟哪個服務在一起\n後者是親和性，要跟哪個服務在一起\n必要與最好可以\n\nrequiredDuringSchedulingIgnoredDuringExecution\npreferredDuringSchedulingIgnoredDuringExecution\n\n前者是一定要達成下面的條件，\n後者是最好是達成這樣的條件，並設成比重分數\n選擇條件\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- gcp\ntopologyKey: kubernetes.io/hostname\n\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 100\npodAffinityTerm:\nlabelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- gcp\ntopologyKey: kubernetes.io/hostname\n\n大致上的邏輯一樣，\n但preferredDuringSchedulingIgnoredDuringExecution ，\n必須加上podAffinityTerm 。\n簡單說明規則，\n選擇這個服務要跟哪個label當比較值。\n然後，topologyKey 有點類似SQL group的概念，你要根據哪個來分群。\n上面的例子是根據 node name來區分，\n還有根據區域 kind.zone 來分。\n如果想知道有哪些的話，可以下指令看一下。\nkubectl get nodes --show-labels\n\n上面是簡單的寫法，\n詳細過程可以參考 矽谷牛大大的文章，連結在下方。\nref.\n\n矽谷牛-解密 Assigning Pod To Nodes(下)\nK8s文件-將 Pod 指派給節點\n[Kubernetes] Assigning Pods to Nodes",
		"tags": [ "note","⎈"]
},

{
		"title": "214. Elastic API建立 Index_template",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/214. Elastic API建立 Index_template/",
		"content": "前言\n幫同事測試不同LOG的刪除時間，\n每次要去點一點又怕忘記，\n弄個API了，\n不知道之後能不能在部屬時直接掛上去，再研究看看。\n這次就先簡單建立個 index template跟life cycle了\n正文\n首先要先確定你的filebeat有將資料丟到elastic裡面，可參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/57.filebeat 補充說明/\">57.filebeat 補充說明</a>。\n再來根據你建的是indices還是datastream，來決定你的index_template。\n下面的指令，都在dev tools中執行。\n\nAPI後面的最後一個路徑為名稱\n\nindices\n\nAPI建立 Index Lifecycle Policies\n\nPUT _ilm/policy/worker\n{\n&quot;policy&quot;: {\n&quot;_meta&quot;: {\n&quot;description&quot;: &quot;used for worker log&quot;,\n&quot;author&quot;: &quot;Ezio&quot;,\n&quot;project&quot;: {\n&quot;name&quot;: &quot;srs&quot;,\n&quot;department&quot;: &quot;lerouge&quot;\n}\n},\n&quot;phases&quot;: {\n&quot;hot&quot;: {\n&quot;min_age&quot;: &quot;0ms&quot;,\n&quot;actions&quot;: {\n&quot;set_priority&quot;: {\n&quot;priority&quot;: 100\n},\n&quot;rollover&quot;: {\n&quot;max_age&quot;: &quot;3d&quot;,\n&quot;max_primary_shard_size&quot;: &quot;50gb&quot;\n}\n}\n},\n&quot;delete&quot;: {\n&quot;min_age&quot;: &quot;3d&quot;,\n&quot;actions&quot;: {\n&quot;delete&quot;: {\n&quot;delete_searchable_snapshot&quot;: true\n}\n}\n}\n}\n}\n}\n\n建立 index_template\n\nPUT _index_template/worker\n{\n&quot;index_patterns&quot; : [&quot;worker-*&quot;],\n&quot;template&quot;: {\n&quot;settings&quot;: {\n&quot;index&quot;: {\n&quot;lifecycle&quot;: {\n&quot;name&quot;: &quot;videoworker&quot;\n}\n}\n}\n},\n&quot;composed_of&quot;: [&quot;ecs@mappings&quot;, &quot;logs@mappings&quot;,&quot;logs@settings&quot;],\n&quot;priority&quot; : 200,\n&quot;version&quot;: 1,\n&quot;_meta&quot;: {\n&quot;description&quot;: &quot;test by Ezio&quot;,\n&quot;latest_modify_date&quot;: &quot;2024-05-28&quot;\n}\n}\n\n檢查 Index Lifecycle Policies 及 index template\n\nGET _ilm/policy/worker # Index Lifecycle Policies\nGET _index_template/worker # index template\n\nref.\n\nCreate or update lifecycle policy API\nIndex templates\n\ndata stream\n\n建立 index template\n與上面的差異點在於，有沒有增加data_stream與 template.lifecycle.data_retention\n\nPUT _index_template/videowork\n{\n&quot;index_patterns&quot; : [&quot;videoworker-*&quot;],\n&quot;template&quot;: {\n&quot;lifecycle&quot;: {\n&quot;data_retention&quot;: &quot;3d&quot;\n}\n},\n&quot;data_stream&quot;: { },\n&quot;composed_of&quot;: [&quot;ecs@mappings&quot;, &quot;logs@mappings&quot;,&quot;logs@settings&quot;],\n&quot;priority&quot; : 200,\n&quot;version&quot;: 1,\n&quot;_meta&quot;: {\n&quot;description&quot;: &quot;test by Ezio&quot;,\n&quot;latest_modify_date&quot;: &quot;2024-05-28&quot;\n}\n}\n\nref. Create a data stream with a lifecycle\nindices vs data stream\n下面資料是用gpt-4o提供的\n\n特性\nIndices\nData Streams\n\n管理方式\n手動管理或使用 ILM\n自動管理滾動和索引\n\n數據存儲\n適合任意結構數據\n專為時間序列數據設計\n\n對應組態\n每個索引獨立組態\n透過索引範本自動組態\n\n滾動策略\n需手動或 ILM 組態\n自動滾動\n\n查詢性能\n基於標準索引\n優化時間序列數據查詢\n\n索引創建\n使用 PUT /index_name\n透過索引範本自動創建\n\n適用數據\n任意數據，包括非時間序列數據\n主要為日誌、指標等時間序列數據\n\n學習曲線\n需要學習和管理索引的細節\n簡化，特別適合大規模時間序列數據\n\n生命週期管理\n需要組態 ILM\n自動管理\n\n存儲效率\n依賴手動優化\n為時間序列數據設計，具備存儲效率\n\n優缺點\n\n項目\nIndices\nData Streams\n\n優點\n- 更靈活，可用於任意類型數據\n- 自動管理滾動和索引\n\n- 完全自定義對應和設置\n- 簡化組態和管理\n\n- 支援複雜的查詢和聚合\n- 優化時間序列數據查詢和存儲\n\n- 適合大規模日誌和指標數據\n\n缺點\n- 需手動管理索引和滾動策略\n- 主要針對時間序列數據\n\n- 組態和管理相對複雜\n- 不適合非時間序列數據\n\n- 需要組態和維護 ILM\n- 對應和設置相對不靈活\n\n推薦使用場景\n\n使用場景\nIndices\nData Streams\n\n日誌管理\n可以使用，但需要手動組態滾動和管理\n最佳選擇，自動管理，簡化運維\n\n應用監控\n可以使用，但需自定義索引和對應\n最佳選擇，專為時間序列數據設計\n\n定期報表\n適合任何類型報表\n適合時間序列報表\n\n歷史數據查詢\n最佳選擇，可針對具體需求優化查詢\n適合基於時間範圍的歷史數據查詢\n\n複雜數據分析\n最佳選擇，支援靈活的對應和查詢\n適合時間序列數據的快速查詢和聚合\n\n非時間序列數據存儲\n最佳選擇，適合所有類型數據\n不推薦，僅適合時間序列數據\n\n結論\n其實從API來看的話，直接用data stream比較快，也比較省事。\n加上目前用的是ECK，\n我根本沒去設計 warm phase 或 cold phase的硬碟。\nref. ES 的超前佈署 - Index Template",
		"tags": [ "note","🗒"]
},

{
		"title": "216. GKE掛載ReadWrite Many的PVC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/216. GKE掛載ReadWrite Many的PVC/",
		"content": "緣由\nGKE上面的pvc是不支援ReadWrite Many的。\n用途是全部的Log寫到同一個資料夾，\n然後再用filebeat擷取到EFK。\n主題\n這邊要做的是自建一個NFS server。\ndeploy裡面，也可以直接在GCE上面建立一個硬碟，\n然後在deploy上面指定。就是下面這段，\n這原本是前任的方式，我直接改掉了。\n直接在一個yaml裡面全部弄好。\ngcePersistentDisk:\npdName: gke-log-nfs-disk\nfsType: ext4\n\nNFS-server.yaml\n---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: nfs\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: nfs-pvc\nnamespace: nfs\nspec:\nstorageClassName: &quot;standard&quot;\nresources:\nrequests:\nstorage: 100Gi\naccessModes:\n- ReadWriteOnce\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nfs-server\nnamespace: nfs\nspec:\nreplicas: 1\nselector:\nmatchLabels:\nrole: nfs-server\ntemplate:\nmetadata:\nlabels:\nrole: nfs-server\nspec:\ncontainers:\n- name: nfs-server\nimage: gcr.io/google_containers/volume-nfs:0.8\nports:\n- name: nfs\ncontainerPort: 2049\n- name: mountd\ncontainerPort: 20048\n- name: rpcbind\ncontainerPort: 111\nsecurityContext:\nprivileged: true\nvolumeMounts:\n- mountPath: /exports\nname: nfs-pvc\nvolumes:\n- name: nfs-pvc\npersistentVolumeClaim:\nclaimName: nfs-pvc\n# gcePersistentDisk:\n# pdName: gke-log-nfs-disk\n# fsType: ext4\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: nfs-server\nnamespace: nfs\nspec:\nports:\n- name: nfs\nport: 2049\n- name: mountd\nport: 20048\n- name: rpcbind\nport: 111\nselector:\nrole: nfs-server\n\n再來是掛載一個provisioner\nnfs-provisioner.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nfs-client-provisioner\nnamespace: nfs\nlabels:\napp: nfs-client-provisioner\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: nfs-client-provisioner\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp: nfs-client-provisioner\nspec:\nserviceAccountName: nfs-client-provisioner\ncontainers:\n- name: nfs-client-provisioner\nimage: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.2\n# image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\nvolumeMounts:\n- name: nfs-client-root\nmountPath: /persistentvolumes\nenv:\n- name: PROVISIONER_NAME\nvalue: nfs-log\n- name: NFS_SERVER\nvalue: nfs-server.nfs.svc.cluster.local\n- name: NFS_PATH\nvalue: /\nvolumes:\n- name: nfs-client-root\nnfs:\nserver: nfs-server.nfs.svc.cluster.local\npath: /\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: nfs-log\nprovisioner: nfs-log\nparameters:\narchiveOnDelete: &quot;false&quot;\n\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: nfs-client-provisioner-runner\nrules:\n- apiGroups: [&quot;&quot;]\nresources: [&quot;nodes&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n- apiGroups: [&quot;&quot;]\nresources: [&quot;persistentvolumes&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]\n- apiGroups: [&quot;&quot;]\nresources: [&quot;persistentvolumeclaims&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]\n- apiGroups: [&quot;storage.k8s.io&quot;]\nresources: [&quot;storageclasses&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n- apiGroups: [&quot;&quot;]\nresources: [&quot;events&quot;]\nverbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: run-nfs-client-provisioner\nsubjects:\n- kind: ServiceAccount\nname: nfs-client-provisioner\nnamespace: nfs\nroleRef:\nkind: ClusterRole\nname: nfs-client-provisioner-runner\napiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: nfs-client-provisioner\nnamespace: nfs\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: leader-locking-nfs-client-provisioner\nnamespace: nfs\nrules:\n- apiGroups: [&quot;&quot;]\nresources: [&quot;endpoints&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: leader-locking-nfs-client-provisioner\nnamespace: nfs\nsubjects:\n- kind: ServiceAccount\nname: nfs-client-provisioner\nroleRef:\nkind: Role\nname: leader-locking-nfs-client-provisioner\napiGroup: rbac.authorization.k8s.io\n\n要使用的話，\n建立一個pvc, storageClassName改成上面建立的storageClass\n其他deploy掛載方式，跟掛pvc一樣。\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\nname: logs-nfs-pvc\nnamespace: default\nspec:\naccessModes:\n- ReadWriteMany\nstorageClassName: nfs-log\nresources:\nrequests:\nstorage: 100Gi\n\nref.\n\n在GKE上使用ReadWrite Many的Disk\nnfs-subdir-external-provisioner",
		"tags": [ "note","⎈"]
},

{
		"title": "217. iptables 筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/217. iptables 筆記/",
		"content": "緣由\nistio重新複習概念的時候，\n又碰到了iptable，然後，我又忘了。\n沒有用自己理解過的作法，\n果然很容易忘記啊。\n主題\niptables簡單說是個管高速公路大門的。\n基本有三條路(chain)，\n分別是\n\nfilter：這條路會直接到管高速公路大門的管理單位大樓裡面。\nnat：要去這條高速公路的其他地方，就往這邊。\nmangle：特權道路，有高官要過時，就走這條。\n\n鳥哥的說明：\n\nfilter (過濾器)：主要跟進入 Linux 本機的封包有關，這個是預設的 table 喔！\n\nINPUT：主要與想要進入我們 Linux 本機的封包有關；\nOUTPUT：主要與我們 Linux 本機所要送出的封包有關；\nFORWARD：這個咚咚與 Linux 本機比較沒有關係， 他可以『轉遞封包』到後端的電腦中，與下列 nat table 相關性較高。\n\nnat (位址轉換)：是 Network Address Translation 的縮寫， 這個表格主要在進行來源與目的之 IP 或 port 的轉換，與 Linux 本機較無關，主要與 Linux 主機後的區域網路內電腦較有相關。\n\nPREROUTING：在進行路由判斷之前所要進行的規則(DNAT/REDIRECT)\nPOSTROUTING：在進行路由判斷之後所要進行的規則(SNAT/MASQUERADE)\nOUTPUT：\n\nmangle (破壞者)：這個表格主要是與特殊的封包的路由旗標有關， 早期僅有 PREROUTING 及 OUTPUT 鏈，不過從 kernel 2.4.18 之後加入了 INPUT 及 FORWARD 鏈。 由於這個表格與特殊旗標相關性較高，所以像咱們這種單純的環境當中，較少使用 mangle 這個表格。\n\nref. iptables 的表格 (table) 與鏈 (chain)\n\n每個chain裡面都有自己rule，\nrule 1 是去彰化，\nrule 2 是去員林，\nrule 3是去嘉義。\n從台中南下要去彰化的話，rule 1 符合就直接去彰化了。\n但如果是要去嘉義，前面不符合，就跳過，直到rule 3判斷符合才會執行。\nref. Istio 中的 Sidecar 注入、透明流量劫持及流量路由過程詳解",
		"tags": [ "note","🐧"]
},

{
		"title": "219. Google Cloud Managed Service for Prometheus  費用",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/219. Google Cloud Managed Service for Prometheus  費用/",
		"content": "緣由\n由於GKE預設惠啟用 Google Cloud Managed Service for Prometheus，\n來比較一下，看之後是不是能夠一個網站看到全部的圖表。\n不過也要先看下價格，如果太貴，那也只能分開了。\n主題\n2024/06/07\n價格\n\n$0.06/百萬個樣本†：注入的前 0-50 億個樣本†\n$0.048/百萬樣本：接下來的 50-2500 億個樣本\n$0.036/百萬樣本：接下來注入的 250-5,000 億個樣本\n注入的每百萬個樣本 $0.024：超過 5000 億個樣本\n\n這數字以百萬計。\n一下子真的不知道該怎麼算，\n那就先算出 每10秒一個metric，那一個月會有多少資料。\nans: 259,200筆。所以一個月 $0.06USD\n加減算一下50億個樣本，要多少錢。\nans: $300 USD\n現在來算一下，一般建pvc要多少，\n建立一個PVC，預設使用 Balanced persistent disk。\npvc通常掛個100G，每個月約$10 USD。\n\nType\nPrice (monthly in USD)\n\nStandard provisioned space\n$0.04 per GiB\n\nSSD provisioned space\n$0.17 per GiB\n\nBalanced provisioned space\n$0.1 per GiB\n\n結論\n這價格看起來還頗有吸引力的，來試試看吧。\n日後如果要從帳單看有多少筆資料的話，\nService -&gt; Cloud Monitoring\nSKUs -&gt; Prometheus Samples Ingested &amp; Monitoring API Requests\nref.\n\nCloud Monitoring 價格摘要\n費用控制和歸因\n硬碟價格",
		"tags": [ "note","☁️"]
},

{
		"title": "220. winSW將程式變成服務的好東西",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/220. windows將程式變成服務的好東西-winSW/",
		"content": "緣由\n本來在Linux上面跑的docker環境，\n這次在客戶那邊突然要變成用windows，\n非常好，之前弄的架構完全不一樣。\n主題\n今天有一個程式，你想讓他開機時自己啟動，\n不是到msconfig設定開機啟動，就是弄成服務。\n但開機啟動，如果沒有開機就沒用了，\n所以弄成服務，一般是最穩定的方式。\n用nginx為範例說明，\n我也是找nginx啟動時，才發現這個好東西。\n\n首先，先下載winSW，\n下載回來的程式，改名成nginx-server.exe。\n\n同一層目錄下，建立nginx-server.xml\n\n&lt;!-- nginx-service.xml --&gt;\n&lt;service&gt;\n&lt;id&gt;nginx&lt;/id&gt;\n&lt;name&gt;nginx&lt;/name&gt;\n&lt;description&gt;nginx&lt;/description&gt;\n&lt;logpath&gt;C:\\nginx-1.26.1\\server-logs\\&lt;/logpath&gt;\n&lt;logmode&gt;roll&lt;/logmode&gt;\n&lt;depend&gt;&lt;/depend&gt;\n&lt;executable&gt;C:\\nginx-1.26.1\\nginx.exe&lt;/executable&gt;\n&lt;stopexecutable&gt;C:\\nginx-1.26.1\\nginx.exe -s stop&lt;/stopexecutable&gt;\n&lt;/service&gt;\n\n安裝\ncommand執行 nginx-server.exe install\n如果要刪除則是 nginx-server.exe uninstall\n\n除錯\nnginx.conf如果寫錯，導致服務無法啟動時，\n可到上面的&lt;logpaht&gt;位置查看錯誤訊息，\n照上面的範例，是C:\\nginx-1.26.1\\server-logs\\。\n以上是最簡單的用法。\nref.\n\ngithub文件\nWinSW",
		"tags": [ "note","🖼"]
},

{
		"title": "L7mp stunner監控",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/221. L7mp stunner監控/",
		"content": "Why\n有人搞了一個webRTC for k8s用的即時媒體串流方案，\n然後我要來接監控了。\nL7mp\nSolution\n參考文件\n\n加 enableMetricsEndpoint 參數加到Dataplane裡面\n\n...\nspec:\nargs:\n- '-w'\n- '--udp-thread-num=16'\ncommand:\n- stunnerd\nenableMetricsEndpoint: true\nhostNetwork: false\nimage: docker.io/l7mp/stunnerd:0.18.0\nimagePullPolicy: Always\nreplicas: 1\nresources:\nlimits:\ncpu: 2\nmemory: 512Mi\nrequests:\ncpu: 500m\nmemory: 128Mi\nterminationGracePeriodSeconds: 3600\n\n弄個暫時的pod測試看看metrics有沒有資料\n\nkubectl run alpine-pod -n istio-system --image=alpine --restart=Never -- /bin/sleep 600\n\n先取得stunner的pod ip，再來就看有沒有資料了\ncurl http://192.168.1.25:8080/metrics\n\n有資料後再來下一步。\n這邊要注意一下，剛開後，stunner開頭的指標通通找不到，\n像是stunner_listener_connections_total、stunner_listener_bytes_total...等。\n我放了一陣子(一兩個小時？)，就突然跑出來了，\n我不確定為什麼，等之後再測試看看。\n\nprometheus 設定 discover\n詳細說明，參考<a class=\"internal-link is-unresolved\" href=\"/404\">209. k8s中srs的各個pod metrics取得</a>\n\nscrape_configs:\n- job_name: 'ome'\nscrape_interval: 10s\nkubernetes_sd_configs:\n- role: pod\nrelabel_configs:\n- source_labels: [__meta_kubernetes_pod_name]\naction: keep\nregex: tcp-gateway.*\n- source_labels: [__meta_kubernetes_pod_container_port_name]\naction: replace\ntarget_label: portname\nreplacement: 8080\n\n建立Grafana\n\n{\n&quot;annotations&quot;: {\n&quot;list&quot;: [\n{\n&quot;builtIn&quot;: 1,\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;grafana&quot;,\n&quot;uid&quot;: &quot;-- Grafana --&quot;\n},\n&quot;enable&quot;: true,\n&quot;hide&quot;: true,\n&quot;iconColor&quot;: &quot;rgba(0, 211, 255, 1)&quot;,\n&quot;name&quot;: &quot;Annotations &amp; Alerts&quot;,\n&quot;type&quot;: &quot;dashboard&quot;\n}\n]\n},\n&quot;editable&quot;: true,\n&quot;fiscalYearStartMonth&quot;: 0,\n&quot;graphTooltip&quot;: 0,\n&quot;id&quot;: 14,\n&quot;links&quot;: [],\n&quot;liveNow&quot;: false,\n&quot;panels&quot;: [\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;fieldConfig&quot;: {\n&quot;defaults&quot;: {\n&quot;color&quot;: {\n&quot;mode&quot;: &quot;palette-classic&quot;\n},\n&quot;custom&quot;: {\n&quot;axisCenteredZero&quot;: false,\n&quot;axisColorMode&quot;: &quot;text&quot;,\n&quot;axisLabel&quot;: &quot;&quot;,\n&quot;axisPlacement&quot;: &quot;auto&quot;,\n&quot;barAlignment&quot;: 0,\n&quot;drawStyle&quot;: &quot;line&quot;,\n&quot;fillOpacity&quot;: 0,\n&quot;gradientMode&quot;: &quot;none&quot;,\n&quot;hideFrom&quot;: {\n&quot;legend&quot;: false,\n&quot;tooltip&quot;: false,\n&quot;viz&quot;: false\n},\n&quot;lineInterpolation&quot;: &quot;linear&quot;,\n&quot;lineWidth&quot;: 1,\n&quot;pointSize&quot;: 5,\n&quot;scaleDistribution&quot;: {\n&quot;type&quot;: &quot;linear&quot;\n},\n&quot;showPoints&quot;: &quot;auto&quot;,\n&quot;spanNulls&quot;: false,\n&quot;stacking&quot;: {\n&quot;group&quot;: &quot;A&quot;,\n&quot;mode&quot;: &quot;none&quot;\n},\n&quot;thresholdsStyle&quot;: {\n&quot;mode&quot;: &quot;off&quot;\n}\n},\n&quot;mappings&quot;: [],\n&quot;thresholds&quot;: {\n&quot;mode&quot;: &quot;absolute&quot;,\n&quot;steps&quot;: [\n{\n&quot;color&quot;: &quot;green&quot;,\n&quot;value&quot;: null\n},\n{\n&quot;color&quot;: &quot;red&quot;,\n&quot;value&quot;: 80\n}\n]\n}\n},\n&quot;overrides&quot;: []\n},\n&quot;gridPos&quot;: {\n&quot;h&quot;: 8,\n&quot;w&quot;: 12,\n&quot;x&quot;: 0,\n&quot;y&quot;: 0\n},\n&quot;id&quot;: 3,\n&quot;options&quot;: {\n&quot;legend&quot;: {\n&quot;calcs&quot;: [],\n&quot;displayMode&quot;: &quot;list&quot;,\n&quot;placement&quot;: &quot;bottom&quot;,\n&quot;showLegend&quot;: true\n},\n&quot;tooltip&quot;: {\n&quot;mode&quot;: &quot;single&quot;,\n&quot;sort&quot;: &quot;none&quot;\n}\n},\n&quot;targets&quot;: [\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;editorMode&quot;: &quot;code&quot;,\n&quot;expr&quot;: &quot;stunner_listener_connections&quot;,\n&quot;legendFormat&quot;: &quot;__auto&quot;,\n&quot;range&quot;: true,\n&quot;refId&quot;: &quot;A&quot;\n}\n],\n&quot;title&quot;: &quot;stunner_listener_connections&quot;,\n&quot;type&quot;: &quot;timeseries&quot;\n},\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;fieldConfig&quot;: {\n&quot;defaults&quot;: {\n&quot;color&quot;: {\n&quot;mode&quot;: &quot;palette-classic&quot;\n},\n&quot;custom&quot;: {\n&quot;axisCenteredZero&quot;: false,\n&quot;axisColorMode&quot;: &quot;text&quot;,\n&quot;axisLabel&quot;: &quot;&quot;,\n&quot;axisPlacement&quot;: &quot;auto&quot;,\n&quot;barAlignment&quot;: 0,\n&quot;drawStyle&quot;: &quot;line&quot;,\n&quot;fillOpacity&quot;: 0,\n&quot;gradientMode&quot;: &quot;none&quot;,\n&quot;hideFrom&quot;: {\n&quot;legend&quot;: false,\n&quot;tooltip&quot;: false,\n&quot;viz&quot;: false\n},\n&quot;lineInterpolation&quot;: &quot;linear&quot;,\n&quot;lineWidth&quot;: 1,\n&quot;pointSize&quot;: 5,\n&quot;scaleDistribution&quot;: {\n&quot;type&quot;: &quot;linear&quot;\n},\n&quot;showPoints&quot;: &quot;auto&quot;,\n&quot;spanNulls&quot;: false,\n&quot;stacking&quot;: {\n&quot;group&quot;: &quot;A&quot;,\n&quot;mode&quot;: &quot;none&quot;\n},\n&quot;thresholdsStyle&quot;: {\n&quot;mode&quot;: &quot;off&quot;\n}\n},\n&quot;mappings&quot;: [],\n&quot;thresholds&quot;: {\n&quot;mode&quot;: &quot;absolute&quot;,\n&quot;steps&quot;: [\n{\n&quot;color&quot;: &quot;green&quot;,\n&quot;value&quot;: null\n},\n{\n&quot;color&quot;: &quot;red&quot;,\n&quot;value&quot;: 80\n}\n]\n}\n},\n&quot;overrides&quot;: [\n{\n&quot;matcher&quot;: {\n&quot;id&quot;: &quot;byName&quot;,\n&quot;options&quot;: &quot;{__name__=\\&quot;stunner_listener_connections_total\\&quot;, instance=\\&quot;192.168.58.102:8080\\&quot;, job=\\&quot;ome\\&quot;, name=\\&quot;stunner/tcp-gateway/tcp-listener\\&quot;, portname=\\&quot;8080\\&quot;}&quot;\n},\n&quot;properties&quot;: [\n{\n&quot;id&quot;: &quot;color&quot;,\n&quot;value&quot;: {\n&quot;fixedColor&quot;: &quot;dark-yellow&quot;,\n&quot;mode&quot;: &quot;fixed&quot;\n}\n}\n]\n}\n]\n},\n&quot;gridPos&quot;: {\n&quot;h&quot;: 8,\n&quot;w&quot;: 12,\n&quot;x&quot;: 12,\n&quot;y&quot;: 0\n},\n&quot;id&quot;: 1,\n&quot;options&quot;: {\n&quot;legend&quot;: {\n&quot;calcs&quot;: [],\n&quot;displayMode&quot;: &quot;list&quot;,\n&quot;placement&quot;: &quot;bottom&quot;,\n&quot;showLegend&quot;: true\n},\n&quot;tooltip&quot;: {\n&quot;mode&quot;: &quot;single&quot;,\n&quot;sort&quot;: &quot;none&quot;\n}\n},\n&quot;targets&quot;: [\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;editorMode&quot;: &quot;code&quot;,\n&quot;expr&quot;: &quot;rate(stunner_listener_connections_total[1m])&quot;,\n&quot;legendFormat&quot;: &quot;__auto&quot;,\n&quot;range&quot;: true,\n&quot;refId&quot;: &quot;A&quot;\n}\n],\n&quot;title&quot;: &quot;listener_connections_total&quot;,\n&quot;type&quot;: &quot;timeseries&quot;\n},\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;description&quot;: &quot;監聽器上發送或接收的字節總數。&quot;,\n&quot;fieldConfig&quot;: {\n&quot;defaults&quot;: {\n&quot;color&quot;: {\n&quot;mode&quot;: &quot;palette-classic&quot;\n},\n&quot;custom&quot;: {\n&quot;axisCenteredZero&quot;: false,\n&quot;axisColorMode&quot;: &quot;text&quot;,\n&quot;axisLabel&quot;: &quot;&quot;,\n&quot;axisPlacement&quot;: &quot;auto&quot;,\n&quot;barAlignment&quot;: 0,\n&quot;drawStyle&quot;: &quot;line&quot;,\n&quot;fillOpacity&quot;: 0,\n&quot;gradientMode&quot;: &quot;none&quot;,\n&quot;hideFrom&quot;: {\n&quot;legend&quot;: false,\n&quot;tooltip&quot;: false,\n&quot;viz&quot;: false\n},\n&quot;lineInterpolation&quot;: &quot;linear&quot;,\n&quot;lineWidth&quot;: 1,\n&quot;pointSize&quot;: 5,\n&quot;scaleDistribution&quot;: {\n&quot;type&quot;: &quot;linear&quot;\n},\n&quot;showPoints&quot;: &quot;auto&quot;,\n&quot;spanNulls&quot;: false,\n&quot;stacking&quot;: {\n&quot;group&quot;: &quot;A&quot;,\n&quot;mode&quot;: &quot;none&quot;\n},\n&quot;thresholdsStyle&quot;: {\n&quot;mode&quot;: &quot;off&quot;\n}\n},\n&quot;mappings&quot;: [],\n&quot;thresholds&quot;: {\n&quot;mode&quot;: &quot;absolute&quot;,\n&quot;steps&quot;: [\n{\n&quot;color&quot;: &quot;green&quot;,\n&quot;value&quot;: null\n},\n{\n&quot;color&quot;: &quot;red&quot;,\n&quot;value&quot;: 80\n}\n]\n},\n&quot;unit&quot;: &quot;bytes&quot;\n},\n&quot;overrides&quot;: [\n{\n&quot;matcher&quot;: {\n&quot;id&quot;: &quot;byName&quot;,\n&quot;options&quot;: &quot;{__name__=\\&quot;stunner_listener_bytes_total\\&quot;, direction=\\&quot;rx\\&quot;, instance=\\&quot;192.168.58.102:8080\\&quot;, job=\\&quot;ome\\&quot;, name=\\&quot;stunner/tcp-gateway/tcp-listener\\&quot;, portname=\\&quot;8080\\&quot;}&quot;\n},\n&quot;properties&quot;: [\n{\n&quot;id&quot;: &quot;color&quot;,\n&quot;value&quot;: {\n&quot;fixedColor&quot;: &quot;purple&quot;,\n&quot;mode&quot;: &quot;fixed&quot;\n}\n}\n]\n},\n{\n&quot;matcher&quot;: {\n&quot;id&quot;: &quot;byName&quot;,\n&quot;options&quot;: &quot;rx&quot;\n},\n&quot;properties&quot;: [\n{\n&quot;id&quot;: &quot;color&quot;,\n&quot;value&quot;: {\n&quot;fixedColor&quot;: &quot;dark-red&quot;,\n&quot;mode&quot;: &quot;fixed&quot;\n}\n}\n]\n}\n]\n},\n&quot;gridPos&quot;: {\n&quot;h&quot;: 9,\n&quot;w&quot;: 24,\n&quot;x&quot;: 0,\n&quot;y&quot;: 8\n},\n&quot;id&quot;: 2,\n&quot;options&quot;: {\n&quot;legend&quot;: {\n&quot;calcs&quot;: [],\n&quot;displayMode&quot;: &quot;list&quot;,\n&quot;placement&quot;: &quot;bottom&quot;,\n&quot;showLegend&quot;: true\n},\n&quot;tooltip&quot;: {\n&quot;mode&quot;: &quot;single&quot;,\n&quot;sort&quot;: &quot;none&quot;\n}\n},\n&quot;targets&quot;: [\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;editorMode&quot;: &quot;code&quot;,\n&quot;expr&quot;: &quot;sum(irate(stunner_listener_bytes_total[1m])) by (direction)&quot;,\n&quot;hide&quot;: false,\n&quot;legendFormat&quot;: &quot;__auto&quot;,\n&quot;range&quot;: true,\n&quot;refId&quot;: &quot;A&quot;\n}\n],\n&quot;title&quot;: &quot;stunner_listener_bytes_total&quot;,\n&quot;type&quot;: &quot;timeseries&quot;\n},\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;description&quot;: &quot;發送到後端或從後端接收的字節總數。&quot;,\n&quot;fieldConfig&quot;: {\n&quot;defaults&quot;: {\n&quot;color&quot;: {\n&quot;mode&quot;: &quot;palette-classic&quot;\n},\n&quot;custom&quot;: {\n&quot;axisCenteredZero&quot;: false,\n&quot;axisColorMode&quot;: &quot;text&quot;,\n&quot;axisLabel&quot;: &quot;&quot;,\n&quot;axisPlacement&quot;: &quot;auto&quot;,\n&quot;barAlignment&quot;: 0,\n&quot;drawStyle&quot;: &quot;line&quot;,\n&quot;fillOpacity&quot;: 0,\n&quot;gradientMode&quot;: &quot;none&quot;,\n&quot;hideFrom&quot;: {\n&quot;legend&quot;: false,\n&quot;tooltip&quot;: false,\n&quot;viz&quot;: false\n},\n&quot;lineInterpolation&quot;: &quot;linear&quot;,\n&quot;lineWidth&quot;: 1,\n&quot;pointSize&quot;: 5,\n&quot;scaleDistribution&quot;: {\n&quot;type&quot;: &quot;linear&quot;\n},\n&quot;showPoints&quot;: &quot;auto&quot;,\n&quot;spanNulls&quot;: false,\n&quot;stacking&quot;: {\n&quot;group&quot;: &quot;A&quot;,\n&quot;mode&quot;: &quot;none&quot;\n},\n&quot;thresholdsStyle&quot;: {\n&quot;mode&quot;: &quot;off&quot;\n}\n},\n&quot;mappings&quot;: [],\n&quot;thresholds&quot;: {\n&quot;mode&quot;: &quot;absolute&quot;,\n&quot;steps&quot;: [\n{\n&quot;color&quot;: &quot;green&quot;,\n&quot;value&quot;: null\n},\n{\n&quot;color&quot;: &quot;red&quot;,\n&quot;value&quot;: 80\n}\n]\n},\n&quot;unit&quot;: &quot;bytes&quot;\n},\n&quot;overrides&quot;: [\n{\n&quot;matcher&quot;: {\n&quot;id&quot;: &quot;byName&quot;,\n&quot;options&quot;: &quot;rx&quot;\n},\n&quot;properties&quot;: [\n{\n&quot;id&quot;: &quot;color&quot;,\n&quot;value&quot;: {\n&quot;fixedColor&quot;: &quot;dark-red&quot;,\n&quot;mode&quot;: &quot;fixed&quot;\n}\n}\n]\n}\n]\n},\n&quot;gridPos&quot;: {\n&quot;h&quot;: 8,\n&quot;w&quot;: 24,\n&quot;x&quot;: 0,\n&quot;y&quot;: 17\n},\n&quot;id&quot;: 4,\n&quot;options&quot;: {\n&quot;legend&quot;: {\n&quot;calcs&quot;: [],\n&quot;displayMode&quot;: &quot;list&quot;,\n&quot;placement&quot;: &quot;bottom&quot;,\n&quot;showLegend&quot;: true\n},\n&quot;tooltip&quot;: {\n&quot;mode&quot;: &quot;single&quot;,\n&quot;sort&quot;: &quot;none&quot;\n}\n},\n&quot;targets&quot;: [\n{\n&quot;datasource&quot;: {\n&quot;type&quot;: &quot;prometheus&quot;,\n&quot;uid&quot;: &quot;PBFA97CFB590B2093&quot;\n},\n&quot;editorMode&quot;: &quot;code&quot;,\n&quot;expr&quot;: &quot;sum(irate(stunner_cluster_bytes_total[1m])) by (direction)&quot;,\n&quot;legendFormat&quot;: &quot;__auto&quot;,\n&quot;range&quot;: true,\n&quot;refId&quot;: &quot;A&quot;\n}\n],\n&quot;title&quot;: &quot;stunner_cluster_bytes_total&quot;,\n&quot;type&quot;: &quot;timeseries&quot;\n}\n],\n&quot;refresh&quot;: false,\n&quot;schemaVersion&quot;: 38,\n&quot;style&quot;: &quot;dark&quot;,\n&quot;tags&quot;: [],\n&quot;templating&quot;: {\n&quot;list&quot;: []\n},\n&quot;time&quot;: {\n&quot;from&quot;: &quot;2024-06-21T01:25:08.227Z&quot;,\n&quot;to&quot;: &quot;2024-06-21T04:12:21.776Z&quot;\n},\n&quot;timepicker&quot;: {},\n&quot;timezone&quot;: &quot;&quot;,\n&quot;title&quot;: &quot;stunner&quot;,\n&quot;uid&quot;: &quot;cbd52e5c-df69-45e6-b2ba-84433a4cd6ec&quot;,\n&quot;version&quot;: 5,\n&quot;weekStart&quot;: &quot;&quot;\n}\n\nref.\n\nPromQL聚合操作\nstunner Github\nstunner Document",
		"tags": [ "note","👁"]
},

{
		"title": "222. elastic 整批刪除indics",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/222. elastic 整批刪除indics/",
		"content": "Why\n不知道為什麼，ILM沒有動作，沒將舊的indics刪除。\n導致elastic直接報錯。\nthis action would add [2] shards, but this cluster currently has [1000]/[1000] maximum normal shards open;\n\n緊急處理先將node的 shared開成1500\nPUT /_cluster/settings\n{\n&quot;persistent&quot;: {\n&quot;cluster.max_shards_per_node&quot;: 1500\n}\n}\n\nSolution\n再來是檢查為什麼有那麼多shared沒刪除，\n但一整批，一個一個刪除會瘋掉。\n用wildcard卻會發生錯誤。\nDELETE /qa-backend-2024.02.*\n\n錯誤\n{\n&quot;error&quot;: {\n&quot;root_cause&quot;: [\n{\n&quot;type&quot;: &quot;illegal_argument_exception&quot;,\n&quot;reason&quot;: &quot;Wildcard expressions or all indices are not allowed&quot;\n}\n],\n&quot;type&quot;: &quot;illegal_argument_exception&quot;,\n&quot;reason&quot;: &quot;Wildcard expressions or all indices are not allowed&quot;\n},\n&quot;status&quot;: 400\n}\n\n不想寫shellscript，一個一個撈index去刪除，\n於是先開放wildcard吧。\nPUT _cluster/settings\n{\n&quot;transient&quot;: {\n&quot;action.destructive_requires_name&quot;: false // allow wildcards\n}\n}\n\n再來執行\nDELETE /qa-backend-2024.02.*\n\n結束。\nref. elasticsearch, how to delete multiple indexes with wildcard",
		"tags": [ "note","🗒"]
},

{
		"title": "223. 檢查TCP or UDP是否有通",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/223. 檢查TCP or UDP是否有通/",
		"content": "Why\n搞SRS的時候，\n要測試webRTC的功能，\n開了UDP但不知道到底有沒有通。\nSolution\nUDP\nnc -u YOUR_PUBLIC_IP 30000\n\n通的話，應該是會發現你可以在底下打字，然後不會結束。\n\n如果有另一台伺服器可以玩的話，\n可以做server\nnc -ulvp 30000\n\nref.\n\nUDP連接埠連通性檢查\n\nNetcat\n安裝方式\nMac\nbrew install netcat\n\nLinux\napt install netcat\n\nCentOS\nyum install nc\n\nAlpine Linux\napk add netcat-openbsd\n\nDocker\napt install netcat-traditional\n\n檢查特定的port 是否開啟\n\nnc -v 192.168.0.175 5000\n\n掃描對方機器的port\n\nnc -vnz -w 1 192.168.233.208 1-1000 2000-3000\n\n再來還有送出http請求、寄信、簡易網頁伺服器...\nref. Netcat（Linux nc 指令）網路管理者工具實用範例",
		"tags": [ "note","🐧"]
},

{
		"title": "224. GKE的節點上限",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/224. GKE的節點上限採坑記/",
		"content": "WHY\n最近同事在搞壓力測試，\n然後他就突然跟我說，欸，我的pod起不來。\n看了一下，發現ip 應該是還夠的啊，怎麼會噴 scale.up.error.ip.space.exhausted\nHow\n因為GKE會根據每個node上，所設定的最大pod數量，\n以及pod的CIDR範圍，來計算你這個cluster可以開到幾個node。\nref.組態每個節點的 Pod 數上限\n文章的最後有寫到，\n\n如果您將默認最大 Pod 數設定為 110 並將 Pod 的次要 IP 地址範圍設定為 /21，Kubernetes 會為叢集中的節點分配 /24 CIDR 範圍。這允許叢集中最多有2(24−21) = 23 =  8 個節點。\n\n我的最大pod數為110，次要IP範圍也是/21，\n所以node開到8個，就達到上限了。\n\n然後，為什麼kubernetes替叢集的節點分配的範圍是 /24。\n要往下看表格。\n\n每個節點的最大 Pod 數量\n每個節點的 CIDR 範圍\nIP 地址數量\n\n8\n/28\n16\n\n9 - 16\n/27\n32\n\n17 - 32\n/26\n64\n\n33 - 64\n/25\n128\n\n65 - 128\n/24\n256\n\n129 - 256\n/23\n512\n\n假設最大pod數是64，那最大節點數則是 2(25−21) = 24 = 16\nSolution\n後來才知道原因，當時有其他案子在手中，沒空查原因。\n但當下直接加大機器規格，減少node的數量。\n或者直接在vpc上面增加新的pod range，應該也是可以的。",
		"tags": [ "note","⎈"]
},

{
		"title": "225.  Debian安裝後設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/225.  Debian安裝後設定/",
		"content": "WHY\n被要求寫個SOP安裝服務的手冊，\n從作業系統開始。\nNote\n\n*** is not in the sudoers file.  This incident will be reported.”\n修改sudoers檔案\n\nsu root\nchmod 777 /etc/sudoers\nvi /etc/sudoers\n\n# 在 root    ALL=(ALL:ALL) ALL 下面加上\n &lt;user&gt;    ALL=(ALL)ALL\n\nchmod 440 /etc/sudoers\n\n關閉休眠模式\n\nsudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target\n\n執行完後，需重開機\n\n關閉安裝來源\n最快的方法是直接從桌面模式關掉。\n程式介面搜尋 update，找到software &amp; update ，\n進去後切到Other software的頁籤，將選項拿掉。\n\n安裝deb\n\napt install ./anydesk_6.3.2-1_amd64.deb",
		"tags": [ "note","🐧"]
},

{
		"title": "228. Cloudsql PITR還原",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/228. Cloudsql PITR還原/",
		"content": "Why\n為了之後的服務，要來驗證一下有哪些的還原方式。\nSolution\n目前看到的是PITR，看說明文件，可以還原到秒的時間。\n開啓方式可以參考官方文件。\n但有一點文件沒提到，\n你只能選取五分鐘前的時間。\n例如\n現在時間是 15:30 ，你還原的時間點，只能選擇15:25以前。\n當然還有秒數的問題，\n如果選擇的時間是 15:25~15:30的話，會出現這個錯誤。\n\n如果要確認可以退的時間點，\n可以執行下面指令。\ngcloud sql instances get-latest-recovery-time &lt;instanceName&gt;\n\n會顯示\nkind: sql#getLatestRecoveryTime\nlatestRecoveryTime: '2024-08-15T06:48:48Z'\n\n下面就是你能夠還原的最早時間。\nref. gcloud sql instances get-latest-recovery-time",
		"tags": [ "note","🗄"]
},

{
		"title": "231. GKE node cpu limit 計算",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/231. GKE node cpu limit 計算/",
		"content": "WHY\n線上服務的pod，CPU飆高，\n導致所屬的node，CPU也跟著衝高，合理。\n但同仁問了一句那個limit是啥。\nNote\n\n那條紅色的線，\n原本以為那是某個node的cpu最大值，\n但我這個node的最大cpu只有4而已。\n所以排除了CPU的上限，檢視 Metrics Explorer\n指標上面寫Limit cores\n再根據 GKE system metrics，得知這個metrics是指 CPU cores limit of the container。\n但我又懶得一個一個查這個node有多少limit。\n於是\nkubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=&lt;node name&gt; -o custom-columns=:metadata.name,:metadata.namespace --no-headers | while read -r name ns; do\necho &quot;$name and $ns&quot;\nkubectl get pod $name -n $ns -o jsonpath='{.spec.containers[*].resources.limits.cpu}' | xargs -n1 echo &quot; CPU Limit: &quot;\ndone\n\n本來是用for，但在切割name 與 namespace出了點問題。\n最後才改用while。\n這邊將值總和計算出來後是6.2，數值一樣，謎底解開了。\nref. Command line tool (kubectl)",
		"tags": [ "note","⎈"]
},

{
		"title": "232. GCE Observability 安裝agent失敗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/232. GCE Observability 安裝agent失敗/",
		"content": "WHY\n利用terraform裝好GCE後，\n要來裝一下監控，才能把資料丟去grafana裏面統一管。\n結果一直卡在pending。\nSolution\n\n以爲只是單純安裝失敗，先強制關閉更新後再看看。\n# 關閉安裝\nsudo bash add-monitoring-agent-repo.sh --remove-repo\n\n重新啓用後還會卡在pending，\n還是只能查 log ，到 /var/log/google-cloud-ops-agent 底下，\n查看health-checks.log 。發現原因出現permission denied\n後來才發現原因，\n我用terraform建立GCE時，沒有設定的service account。\n導致他要安裝時會卡在那邊。\n所以，只要把機器停機，\n再到 API and identity management 選擇 Service account即可。\n\nref. 排查 Ops Agent 安裝和啟動問題",
		"tags": [ "note","☁️"]
},

{
		"title": "233. 不同區域 GCE與GKE LB連線",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/233. 不同區域 GCE與GKE LB連線/",
		"content": "Why\n突然來了個要求，要從美東的GCE 連到臺灣的GKE。\nGKE的機器，我建了內部 LB 。\n但只有同一個區域的GCE機器能夠連。\n連去GCE 直接ping各個區域的機器，都正常。\n但連GKE的LB 就不通。\nSolution\n諮詢了顧問公司，\n他們的回答是VPC內部的子網路都是互通的（沒錯，測試有通），\n但要連到GKE的internal LB，\nLB要開啓 Global access\n\nref. 客戶端訪問\nTroubleshoot\n這幾天發現個問題，\n手動更改Global access後，會自動變成Disable。\n通報給Google了，後續看狀況。\n但有建議說直接在server上面加 annotations\napiVersion: v1\nkind: Service\nmetadata:\nannotations:\n# add globle access\nnetworking.gke.io/internal-load-balancer-allow-global-access: &quot;true&quot;\nnetworking.gke.io/load-balancer-type: &quot;Internal&quot;\nname: srs-source-internal\nnamespace: srs-prod\nspec:\ntype: LoadBalancer\nselector:\napp: srs-source\nports:\n- protocol: TCP\nport: 1935\ntargetPort: 1935\nname: rtmp\n\nref. Global access",
		"tags": [ "note","☁️"]
},

{
		"title": "234. Debian新增軟體源",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/234. Debian新增軟體源/",
		"content": "Cause\n客戶那邊要新增一臺機器，\n連過去裝時，才發現他沒有software repositories\nSolution\nOS： Debian 10\n編輯 /etc/apt/sources.list ，\n新增\ndeb http://ftp.debian.org/debian stable main contrib non-free\n\n然後開啓terminal，執行下面指令，收工\napt update\n\nref. DebianRepository",
		"tags": [ "note","🐧"]
},

{
		"title": "235. Grafana Alert Template",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/235. Grafana Alert Template/",
		"content": "Why\n新服務要上線了，\n要趕緊把 Observability做一做，\n之前的版本都會有一長串資訊，\n看了就很煩，不能很直觀的知道是哪邊出問題。\n訊息又都要一個一個打，\n所以，我能不能弄成一個範本，減少我人工的作業？\nSolution\n\n注意，因有特殊符號，導致編譯出問題。\n故將裡面的所有{ } 改為全形，請自行改為半形\n故將裡面的所有{ } 改為全形，請自行改為半形\n故將裡面的所有{ } 改為全形，請自行改為半形\n\n很重要，說三次\n原始的範本，通常長成這樣。\n有很多資訊，但有些是用不到的。\n[FIRING:1] (cpu-dealer GKE Cluster normal backend)\n\nFiring\n\nValue: B=2.632\nLabels:\n- alertname = cpu-abc\n- grafana_folder = GKE Cluster\n- level = normal\n- service-type = backend\nAnnotations:\n- summary = 環境：Prod\n機器名稱：dealer\n告警訊息：CPU使用量1core\nSource: http://localhost:3000/alerting/grafana/uRU7sCEnz/view?orgId=1\nSilence: http://localhost:3000/alerting/silence/new?alertmanager=grafana&amp;matcher=alertname%3Dcpu-dealer&amp;matcher=grafana_folder%3DGKE+Cluster&amp;matcher=level%3Dnormal&amp;matcher=service-type%3Dbackend\nDashboard: http://localhost:3000/d/2-jy90-nk?orgId=1\nPanel: http://localhost:3000/d/2-jy90-nk?orgId=1&amp;viewPanel=13\n\n最後的成果\n[FIRING:1] sftp-qa Disk Space alarm-GCE (normal sftp-qa value_percent_used_mean_aggregate)\n\nAnnotations:\n- summary: 環境：alarm-GCE\n機器名稱：sftp-qa\n告警項目：sftp-qa Disk Space\n告警訊息：硬碟空間目前 46.22% ，已達上限 70%\nDashboard URL:\n\n[View Dashboard](http://&lt;ip&gt;/grafana/d/f649857c-4763-442f-9919-b41c3832d5b8?orgId=1)\n\nStep 1. 確認版本\n首先確認Grafana的版本，目前我使用的是Grafana v11.2.0。\n版本不一樣，指令可能會不一樣，需要再去查文件。\nStep 2. 設定Notification Template\n\n開啟 Home &gt; Alerting &gt; Cotact points\n切換到分頁 Notification Templates ，選擇 Add Notification Template\n\n在左上的Template框框，輸入下面程式碼。\n如果指令沒錯，可以在右邊的preview看到結果。\n｛｛ define &quot;alert_message&quot; ｝｝\n｛｛ range .Alerts ｝｝\n｛｛ if gt (len .Annotations) 0 ｝｝\nAnnotations:\n｛｛ range $key, $value := .Annotations ｝｝\n- **｛｛ $key ｝｝**:\n｛｛ $value ｝｝\n｛｛ end ｝｝\n｛｛ else ｝｝\nNo Annotations found.\n｛｛ end ｝｝\n\nDashboard URL:\n[View Dashboard](｛｛ .DashboardURL ｝｝)\n｛｛ end ｝｝\n｛｛ end ｝｝\n\n這程式我是叫GPT寫的，再稍微小改一下。\n如果想自己開發，請參考Configure notification messages\np.s. 注意版本，版本不一樣從網頁的版本號調整\nStep 3. 綁定notification policy\n\n回到Contact Points，\n選擇你的notification policy\n\n選擇剛剛新增的Template\n可以按一下Test驗證一下有沒有通。\nStep 4. 設定 Alert Rules\n基本的設定就參考Alert rules\n在Summary則是輸入\n環境：｛｛ index $labels &quot;grafana_folder&quot; ｝｝\n機器名稱：｛｛ index $labels &quot;metadata.label.system.name&quot; ｝｝\n告警項目：｛｛ index $labels &quot;alertname&quot; ｝｝\n告警訊息：硬碟空間目前 ｛｛ humanize (index $values &quot;B&quot;).Value ｝｝% ，已達閥值 70%\n\nindex是用來顯示標籤中的內容，\n可以更方便得到你想要的數值。\n可以在測試時用 ｛｛ $labels ｝｝ 看有哪些標籤可用。\np.s 版本不一樣，取值的方式也不一樣。\n同樣可以使用的是 ｛｛ $value ｝｝ ，這邊注意只有value沒有s。\n但在使用index時，需要加上 s。\nref. Notification.Templates\n\nhumanize 是比較人性化的format函數。\nhumanize 會將多餘的小數點位數移除或改為比較易讀的方式。\nhumanizePercentage 將小數點轉為百分比\nhumanizeDuration 將時間轉為可讀的時間\n更多可參考下面連結，裡面也有其他function的用法。\nref. humanize\n\nTroubleshooting\n當有一個變數寫錯時，發送告警時，全部的變數都不會顯示。\n會變成下面的情況。\n- summary:\n｛｛ $value ｝｝\n環境：｛｛ index $labels &quot;grafana_folder&quot; ｝｝\n機器名稱：｛｛ (index $values &quot;B&quot;).persistentvolumeclaim ｝｝\n告警項目：｛｛ index $labels &quot;alertname&quot; ｝｝\n告警訊息：硬碟空間目前 ｛｛ humanizePercentage (index $values &quot;B&quot;).Value ｝｝ ，已達閥值 70%\n\n此時就看一下是不是那邊要加空格沒空格，\n或是指定的values的欄位有錯誤。\n像上面的錯誤，機器名稱的部分應該要改成這樣。\n機器名稱：｛｛ (index $values &quot;B&quot;).Labels.persistentvolumeclaim ｝｝",
		"tags": [ "note","👁"]
},

{
		"title": "236. Telegram傳送檔案",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/236. Telegram傳送檔案/",
		"content": "Why\n突然想要懶一下，\n定時送個檔案到TG上面，\n怎麼傳文字我很清楚，但檔案還真的沒想法。\n只打算弄個sh，不想寫code。\nSolution\n問了GPT很簡單，\n就一行。\n987654321 是 chat_id\nbot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11是TG的機器人token\nbackup.zip是檔案\nsamplefile是附註\ncurl -F chat_id=987654321 \\\n-F document=@/path/to/backup.zip \\\n-F caption=samplefile \\\nhttps://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/sendDocument\n\nref. sendDocument\n小補充，想在caption上面標注日期，方便尋找。\ntoday=$(date +%F)\ncurl -F chat_id=987654321 \\\n-F document=@/path/to/backup.zip \\\n-F caption=samplefile-${today} \\\nhttps://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/sendDocument",
		"tags": [ "note","💻"]
},

{
		"title": "237. GCP IAM綁GKE的RBAC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/237. GCP IAM綁GKE的RBAC/",
		"content": "WHY\n要弄個小程式定時撈secret的資料出來，\n但叢集是舊的沒有開啟workload Identity，\n也只能掛IAM的service account了。\n只給予Kubernetes Engine Viewer的權限，\n但要撈secret時，會通知缺乏container.secrets.get的權限。\nSolution\n\n檢查Kubernetes Engine Viewer有哪些權限\n\n網頁查詢\n另一種透過指令的方式，參考上圖中的role將值帶入到下面指令。\ngcloud iam roles describe roles/container.viewer\n\n(我很想吐槽，我怎麼知道這個名稱要用哪個role去查啊，還不是要去網頁看)\nref. 查看 IAM 角色授予的權限\n查完的結果是真的缺乏 container.secrets.get的權限。\n那現在的問題是，要怎麼加權限？\n\n釐清認證方式\nGoogle 文件說明\n您可以使用 IAM 和 Kubernetes RBAC 來控制對 GKE 叢集的存取權：\n\nIAM 並非特定於 Kubernetes；它為多種 Google Cloud 產品提供身份管理，並且主要在 Google Cloud 項目級層運行。\n\nKubernetes RBAC 是 Kubernetes 的核心組成部分，可讓您針對叢集內的任何對象或對象類型建立和授予角色（一組權限）。\n\n為了授權操作，GKE 首先會檢查是否存在 RBAC 政策。如果不存在 RBAC 政策，GKE 會檢查 IAM 權限。\n\n在 GKE 中，IAM 和 Kubernetes RBAC 整合在一起，您可以通過任一工具向使用者授權，使其有足夠的權限執行操作。這對於 GKE 叢集的引導過程至關重要，因為默認情況下，Google Cloud 使用者不具有任何 Kubernetes RBAC RoleBindings。\nref. 使用基於角色的存取權控制向叢集中的操作授權\n所以說預設的角色權限就是那些，要針對角色加權限的話只能自己新增了。\n\n新增RBAC權限\n\nClusterRoleBinding 使用 kind: user，綁定IAM的帳號。\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: secret-reader\nrules:\n- apiGroups: [&quot;&quot;]\nresources: [&quot;secrets&quot;]\nverbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: secret-reader-binding\nsubjects:\n- kind: User # 或者 ServiceAccount 根據具體的需求\nname: get-api@project.iam.gserviceaccount.com # 替換成具體的使用者或服務帳號名稱\napiGroup: rbac.authorization.k8s.io\nroleRef:\nkind: ClusterRole\nname: secret-reader\napiGroup: rbac.authorization.k8s.io\n\n驗證方式\n\n綁完後，可使用指令看看能不能正常讀取。\nkc auth can-i get secret iplc-test-com-tls \\\n-o=jsonpath='{.data.tls\\.crt}' \\\n--as get-api@project.iam.gserviceaccount.com\n\n可讀取會顯示 Yes，否則就是 No\nref. Checking API access",
		"tags": [ "note","⎈"]
},

{
		"title": "238. Steam Deck安裝Cheat Engine",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/238. Steam Deck安裝Cheat Engine/",
		"content": "WHY\n有些遊戲用簡易版的搜尋軟體 <a class=\"internal-link\" data-note-icon=\"\" href=\"/206. steamDeck 的 Cheat Engine替代品/\">206. steamDeck 的 Cheat Engine替代品</a>，\n就是搜不到，\n想說是不是還是該用CE來試試看了。\nSolution\n主要是參考這個影片Cheat Tables (.CT) for Steam Deck SteamOS Cheat Engine Guide Setup Tutorial\n主要幾個步驟，備忘一下。\n\n下載軟體\n\n到桌面模式下，安裝 ProtonUP-Qt 以及 Protontricks\n如果沒有外接滑鼠的話，\n右邊的觸控版是滑鼠， RT(R2) 是滑鼠左鍵， LT(L2)是滑鼠右鍵\n\n新增SteamTinkerLaunch\n\n開啟ProtonUP-Qt\n\nAdd version\n\n在Compatibility tool選擇 SteamTinkerLaunch\n然後 install\n完成後重新啟動Steam\n\n新增CE到收藏庫\n\n桌面模式下，開啟Steam。\n到『收藏庫』，左下方有一個新增遊戲的按鈕，\n選擇 『新增非Steam遊戲』。\n找到你剛剛下載的CheatEngine.exe。\n\n安裝CE\n\n新增完成後，會出現下圖，\n\n選擇右邊的內容，\n開啟相容性。\n『強制使用特定Steam Play 相容性工具』打勾。\n選擇『Proton Experimental』\n設定完後，啟動。\n會看到安裝畫面，就安裝吧。\n注意，中途會問你要不要裝其他軟體，選擇Skip All\n\n複製資料夾\n找到CE的資料夾，記得先開啟『顯示隱藏檔』\n\n/home/.steam/steam/steamapps/compatdata\n這邊有兩種方式，\n一種是找到建立日期是剛剛的。\n另一種是打開Protontricks 看CheatEngine的編號是哪一個。\n像我的是3262966842 就再進去\n3262966842/pfx/drive_c/Program FIles\n完整路徑會像這樣\n/home/.steam/steam/steamapps/compatdata/3262966842/pfx/drive_c/Program files\n將Cheat Engine 7.5資料夾整個複製，\n貼到你想要修改的遊戲資料夾上面。\n假設我要修Brotato，編號是 1942280（開Protontricks查）。\n就參考上面的路徑\n/home/.steam/steam/steamapps/compatdata/1942280/pfx/drive_c/Program files\n到這邊將Cheat Engine 7.5資料夾 貼上。\n\n設定Brotato用SteamTinkerLaunch開啟\n\n桌面模式\n內容-&gt;相容性-&gt;強制使用特定Steam Play相容性工具-&gt;Steam Tinker Launch\n\n設定Game Menu\n\n桌面模式\n照上面的設定完後，執行遊戲，會跳出一個視窗，\n選擇下方的Main Menu-&gt; Game Menu\nUse custom command 打勾\nCustom command 選擇剛剛複製過來資料夾裡面的CheatEngine.exe\nFork custom command 打勾\n\n啟用開發者模式\n\n可以回到遊戲模式，\n先按下Steam按鈕，系統-&gt; 啟用開發者模式。\n\n開始修改\n\n執行你的遊戲，此時按下 Steam 按鈕，\n會看到同時有兩個檔案在執行，\n一個是你玩得遊戲，\n另一個則是CheatEngine。\n需要注意的是，如果你有選擇開啟其他視窗的話，\n需要從Steam按鈕去選擇你開啟的其他視窗。\nNote\n\n抓圖的話使用內建的Spectacle\n複製圖片的話，使用ssh\n\nSteamDeck\n如果沒設過密碼，需使用passwd先行設定\nsystemctl enable sshd # 啟用sshd\nsystemctl restart sshd # 重新啟動sshd\nsystemctl status sshd # 查詢狀態\n\nip -4 addr # 查詢ip位置\n\nPC\nscp -r deck@192.168.1.2:/home/deck/Pictures/ .\n\nref.\n\nCheat Tables (.CT) for Steam Deck SteamOS Cheat Engine Guide Setup Tutorial\nFearLess Cheat Engine 下載CT檔案的網站",
		"tags": [ "note"]
},

{
		"title": "239.Cloudflare白名單及擷取國別設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/239. Cloudflare白名單及擷取國別設定/",
		"content": "WHY\n每當要再設定一次時，\n都要找一陣子，才會發現在哪裡，\n於是才發現，我沒筆記啊...\nNote\n白名單設定\n網路安全-&gt;WAF-&gt;自訂規則\n\n新增規則\n\n這邊可選擇要篩選哪個欄位。\n\n再來要允許還是阻擋。\n最後記得『佈署』。\n國別header\n網路-&gt; IP 地理位置\n\n之後就能從header裡面取得這個使用者是哪個國家。\nref. Configuring IP geolocation",
		"tags": [ "note","🌐"]
},

{
		"title": "240. git log 除錯指令",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/240. git log 除錯指令/",
		"content": "WHY\n前陣子，RD同仁找來，說為什麼這樣的CI不會過。\n查了一連串的LOG，發現是fast-forward的問題。\n過程中用了一堆git 指令...\n紀錄吧\nNote\n\n只顯示merge的log\n\ngit log --merges\n\n顯示全部Log\n\ngit log\n\n顯示單一提交的詳細訊息\n\ngit show &lt;commit&gt;\n\n查詢某個特定提交的tag\n\ngit tag --points-at &lt;commit&gt;\n\n列出所有tag及對應的commit值\n\ngit show-ref --tags\n\n以圖形化顯示log\n\ngit log --graph\n\n取得git的tag資料\n\n# 取開頭第一個\ngit tag --points-at &lt;commit&gt; | head -1\n# 取結尾第一個\ngit tag --points-at &lt;commit&gt; | tail -1\n\nLog Merge欄位的意思\n假設顯示的資訊如下\n\ncommit a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t\nMerge: 1a2b3c4 5d6e7f8\nAuthor: John Doe &lt;john.doe@example.com&gt;\nDate: Tue Sep 11 14:30:00 2024 +0800\n\nMerge branch 'feature-branch' into 'develop'\n\ndevelop的 commit值是 1a2b3c4\nfeature-branch的commit值是5d6e7f8\n這兩個是 non-fast-forward合併，因為看得到Parents commit。\n如果是Fast-Forward合併，只會顯示單個提交，不會有Merge標記。\n\npull時不使用rebase，而是使用merge\n\ngit pull --no-rebase",
		"tags": [ "note","💻"]
},

{
		"title": "241. MSSQL紀錄log，使用QUERY_STORE",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/241. MSSQL紀錄log，使用QUERY_STORE/",
		"content": "WHY\n線上出了問題，\n但沒有作Tracer，所以就只能先追Slow Query看看了。\nNote\n本來以為只能寫Trigger，當觸發動作時，記錄當下的Query\n後來問了一下GPT，才發現原來現在的SQL server有一個查詢存放區，\n是專門拿來放SQL指令。\n\n查詢存放區會收集 DML 陳述式，例如 SELECT、INSERT、UPDATE、DELETE、MERGE 和 BULK INSERT。\n\n但這也不是很新的東西，從SQL 2016就有了。\n但從SQL 2022開始，這個設定就會預設打開。\n開啟方式\nALTER DATABASE &lt;database_name&gt;\nSET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE);\n\n然後，就可以查看看所有執行過的SQL了。\nSELECT Txt.query_text_id, Txt.query_sql_text, Pln.plan_id, Qry.*, RtSt.*\nFROM sys.query_store_plan AS Pln\nINNER JOIN sys.query_store_query AS Qry\nON Pln.query_id = Qry.query_id\nINNER JOIN sys.query_store_query_text AS Txt\nON Qry.query_text_id = Txt.query_text_id\nINNER JOIN sys.query_store_runtime_stats RtSt\nON Pln.plan_id = RtSt.plan_id;\n\nsys.query_store_query欄位說明\nsys.query_store_runtime_stats欄位說明\n\n計劃存放區中可為查詢儲存的不重複計劃數目，受限於 max_plans_per_query 組態選項。\n\n查詢 max_plans_per_query的值及其他參數\nselect *\nfrom sys.database_query_store_options\n\nsys.database_query_store_options欄位說明\n\n雖然說他的最大值預設是200，\n但我實際看起來，我的plan_id已經超過200了還在存。\n\nref.\n\n使用查詢存放區監視效能\n管理查詢存放區的最佳做法",
		"tags": [ "note","🗄"]
},

{
		"title": "242.  .Net core 8 使用apt install 權限不足",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/242.  .Net core 8 使用apt install 權限不足/",
		"content": "WHY\n今天早上又被人抓住說要開新服務，\n看這個架構要怎麼調。\n測試時來到Docker build，然後就掛了。\nSOLUTION\n在使用apt update的時候，出現了Permission denied\n在Dockerfile的上層加上\nUSER root\n\n就能執行了，\n但不知道為什麼，我沒辦法復現錯誤狀況，\n使用 docker system prune -a 全部清除了也是一樣。\n但當時確實有碰到這個問題。\n附上完整Dockerfile\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base\n\nUSER root\nWORKDIR /app\n\nRUN apt update &amp;&amp; apt install -y curl python3\nRUN curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-443.0.0-linux-x86_64.tar.gz \\\n&amp;&amp; tar -xf google-cloud-cli-443.0.0-linux-x86_64.tar.gz &amp;&amp; ./google-cloud-sdk/install.sh --command-completion=true --path-update=true --quiet\nCOPY gcs-download.json .\nRUN /app/google-cloud-sdk/bin/gcloud auth activate-service-account abc@abc.com --key-file=gcs-download.json\nEXPOSE 8080\nEXPOSE 8081\n\n其實應該用workload Identity會比較好，但這程式預計要放的位置，\n要改的話變動太大。\nref. # New non-root 'app' user in Linux images\n2024/12/06更新\n還有另一個方法，\n在deploy上面加上下面指令，\n讓pod以root的身分執行。\ncontainers:\n\t....\nsecurityContext:\nallowPrivilegeEscalation: true\n\nsecurityContext:\n\trunAsUser: 0\n\nref. 為 Pod 或容器組態安全上下文",
		"tags": [ "note","🐳"]
},

{
		"title": "243. GCP 全域負載平衡器進階路由設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/243. GCP 全域負載平衡器進階路由設定/",
		"content": "WHY\n新的服務，就順便改一下新的負載平衡器去掛載bucket吧。\n然後滿滿的坑，GCP的說明範例不知道在寫啥。\nSolution\n\n以前都是選擇傳統版，\n設定也頗簡單。\n\n不要看他新版上面寫建議，\n就真的用，至少先弄個傳統的把你要的東西設定好。\n\n上面的設定是將 domain/files/123.jpg，轉到 bucket底下的123.jpg\n這邊提供一個最快的解法。\n假設已經在傳統的負載平衡器上面已經建立好了規則，名稱為 abc-qa。\n使用指令查詢\ngcloud compute url-maps describe abc-qa\n\n將 pathMatchers底下的東西抄出來，改一下，\n\n貼到『進階型主機與路徑規則』上面就好了（不要太相信程式碼指南）。\n真的要參考，請看底下連結。\n\nservice的部分，利用指令查詢你的服務或bucket。\ngcloud compute backend-buckets list --uri\nor\ngcloud compute backend-services list --uri\n\n這指令會得到類似下面的網址。\nhttps://www.googleapis.com/compute/v1/projects/&lt;project_name&gt;/global/backendBuckets/&lt;bucket_name&gt;\n\nservice 可以為 projects/&lt;project_name&gt;/global/backendBuckets/&lt;bucket_name&gt;\n或是\nglobal/backendBuckets/&lt;bucket_name&gt;\nTroubleshooting\n\n出現錯誤 Operation type [update] failed with message &quot;Invalid resource: URL_MAP/513614964978.game-result-video-qa&quot;\n\n錯誤的設定\ndefaultService: global/backendBuckets/abc-qa\nname: matcher1\nrouteRules:\n- matchRules:\n- prefixMatch: /videos\npriority: 100\nrouteAction:\nweightedBackendServices:\n- backendService: global/backendBuckets/abc-qa\nweight: 100\n\n問顧問，說不要用 weightedBackendServices 直接指定service。\ndefaultService: global/backendBuckets/abc-qa\nname: matcher1\nrouteRules:\n- matchRules:\n- prefixMatch: /videos\npriority: 100\nservice: global/backendBuckets/abc-qa\n\nref. GCP - HTTP(S) Load Balancer L7 Backend Bucket Issue\n\n程式碼指南的 $[DEFAULT_SERVICE_URL]是什麼\n\n使用指令查詢\ngcloud compute backend-buckets list --uri\nor\ngcloud compute backend-services list --uri\n\nref. What does &quot;$[DEFAULT_SERVICE_URL]&quot; refer to when setting Google Load Balancer towards a GCP Storage Bucket?\nConclusion\n沒事不要用新版的應用程式負載平衡器。\n官方文件要看的話，請看下面兩個連結。\n\nAdvanced host, path, and route rule\nURL maps overview",
		"tags": [ "note","🌐"]
},

{
		"title": "244. 在GCE使用filebeat傳到GKE的ECK",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/244. 在GCE使用filebeat傳到GKE的ECK/",
		"content": "WHY\nRD說他要搞一台自動錄影的服務，\n本來以為只有一個服務，那就掛在GKE上就好了。\n但是，他有兩個服務A跟B，兩邊的資料要互通。\n真要用GKE硬作也是可以，但如果這個服務本身效能就吃很大，\n那用k8s也是要多開一台機器，那不如就用GCE吧。\n然後，問題就來了，LOG咧！？\nLog出問題時要查，最好也是傳去共同的ECK上面，但ECK在GKE上面。\nSolution\n抓取log的流程也跟一般的filebeat差不多。\nfilebeat會從主機上的 /var/lib/docker/containers/*/*.log 抓取。\n這邊文字檔的log是docker logs的資料，也就是程式的stdout。\n官方簡單說明\nDocker-compose.yaml\n我是用docker compose ，比較好管理。\nservices:\nfilebeat:\nimage: docker.elastic.co/beats/filebeat:8.15.3\nbuild: filebeat\ncontainer_name: filebeat\nuser: root\nrestart: unless-stopped\nenvironment:\n- ELASTICSEARCH_HOSTS=https://10.60.7.192:9200\n- ELASTICSEARCH_USERNAME=elastic\n- ELASTICSEARCH_PASSWORD=PASSWORD\nlabels:\nco.elastic.logs/enabled: &quot;false&quot;\nvolumes:\n- ./filebeat.yml:/usr/share/filebeat/filebeat.yml\n- /var/lib/docker/containers:/var/lib/docker/containers:ro\n- /var/run/docker.sock:/var/run/docker.sock:ro\n- ./ca.crt:/etc/pki/client/ca.crt\n- ./tls.crt:/etc/pki/client/cert.pem\n- ./tls.key:/etc/pki/client/cert.key\nlogging:\ndriver: &quot;json-file&quot;\noptions:\nmax-size: &quot;10m&quot;\nmax-file: &quot;2&quot;\nnetworks:\n- zlm_network\nnetworks:\nzlm_network:\ndriver: bridge\nname: zlm_network\n\n主要會著重於volume的部分。\n其他地方可能有點差異會抓幾個講。\nGeneral\n\nuser: root\n\n不設定的話，在filebeat.yml 的logging 會出現權限不足的錯誤。\n\nnetwork 的設定，讓他跟要抓取log的服務放在一起，故同一個網路名稱。\n\nVolume\n\nfilebeat.yml的設定檔連結，這部份沒什麼問題。\n裡面內容會在下面說。\n抓取log的路徑 /var/lib/docker/containers\n掛載SSL憑證\n為什麼要掛載呢？因為ECK在安裝的時候，預設都走ssl，\n但ECK安裝時在GKE上面，而目前filebeat在GCE，\n所以要拿ECK在GKE上面的憑證來用。\n\n重點來了，你怎麼知道要抓哪個憑證來用，\n此時我們可以先去看一下GKE的ECK裡面的設定檔。\nvolumes:\n- name: elastic-internal-http-certificates\nsecret:\nsecretName: prod-es-http-certs-internal\ndefaultMode: 420\noptional: false\n\n此時，就能去decode secret了\nkubectl get secret prod-es-http-certs-internal -n elastic-system -o=jsonpath='{.data.ca\\.crt}' | base64 -d;echo\n\nkubectl get secret prod-es-http-certs-internal -n elastic-system -o=jsonpath='{.data.tls\\.crt}' | base64 -d;echo\n\nkubectl get secret prod-es-http-certs-internal -n elastic-system -o=jsonpath='{.data.tls\\.key}' | base64 -d;echo\n\n之後將key ，掛載讓filebeat能夠讀取即可。\nref. Run Filebeat on Docker\nFilebeat.yml\nfilebeat.inputs:\n- type: container\nenabled: true\npaths:\n- '/var/lib/docker/containers/*/*.log'\ntags: [&quot;videosokoban-qa&quot;]\n\nprocessors:\n- add_docker_metadata: ~\n- drop_event:\nwhen:\nnot:\nequals:\ncontainer.name: &quot;zlm-server&quot;\noutput.elasticsearch:\nhosts: https://prod-es-http:9200 #https://10.60.7.192:9200\nusername: &quot;elastic&quot;\npassword: &quot;PASSWORD&quot;\nssl:\ncertificate_authorities:\n- &quot;/etc/pki/client/ca.crt&quot;\ncertificate: &quot;/etc/pki/client/cert.pem&quot;\nkey: &quot;/etc/pki/client/cert.key&quot;\nindices:\n- index: &quot;videosokoban-qa-%{+yyyy.MM}&quot;\nwhen.contains:\ntags: &quot;videosokoban-qa&quot;\nlogging:\nmetrics.enabled: false\nlevel: debug\nto_stderr: true\nto_files: true\nfiles:\npath: /var/log/filebeat\nname: filebeat\nkeepfiles: 7\npermissions: 0644\n\nfilebeat.input\n要擷取的log來源，\n已將本機的路徑掛到docker上面的路徑的。\n/var/lib/docker/containers/*/*.log\n這個路徑擁有所有的docker logs\ntype 有分成 container與 docker。\n據說是docker只能用在docker上面，\n但container可以用在任何的容器服務上面。\ntype: docker的設定我沒搞出來，有興趣的可以自己試一試。\nprocessors\n這邊牽扯到的部分比較多\n\nadd_docker_metadata\n增加一些欄位到log裡面，詳細內容可參考 Add Docker metadata\ndrop_event ，判斷當container_name是 zlm-server的時候，\n就不要紀錄，這個值是先塞資料進去到ECK後，再篩選出來的。\n條件判斷可參考conditions。\n\n之前還有用過 drop_fileds ,add_cloud_metadata，\n其他更多請參考Define processors\noutput.elasticsearch\n\nhost &amp; ssl\n要將log丟去的ECK伺服器位置。\n\nhosts這邊，本來是用GKE的內部負載平衡的ip位置，\n但是直接使用ip位置的話，會出現錯誤。\n\ncannot validate certificate for 10.60.7.192 because it doesn't contain any IP SANs\n\n這是因為伺服器的憑證沒有加過IP地址，\n看一下 tls.crt，裡面的這段。\n\n這裡並沒有ip位置，所以沒辦法通過驗證。\n但這個憑證是ECK自動產生的，\n所以換個想法，直接讓服務去呼叫 https://prod-es-http:9200\n此時，就是到主機設定一下/etc/hosts，\n新增\n10.60.7.192 prod-es-http\n\nref. Secure communication with Elasticsearch\n\nindices\n將特定的tag綁定到指定的indices上\n\nref.Configure the Elasticsearch output\nlogging\n除錯時，\n可以先將to_stderr: true 開啟。\n可直接用docker logs filebeat 看 log。\n注意，如果要用 to_files, docker-compose.yaml 記得加上 user: root，\n否則會發生權限不足的情況。\nref. Configure logging\n官方的filebeat.yaml ，請參考 filebeat.reference.yml",
		"tags": ["https", "note","🗒"]
},

{
		"title": "245. gcloud storage取代gsutil",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/245. gcloud storage取代gsutil/",
		"content": "WHY\n剛好想要查一下gsutil的 rsync指令，\n結果官方文件一直跑不出來，\n細看才發現\n\n通常，您應該使用 gcloud storage 命令)而不是 gsutil 命令：\ngsutil 工具是一個舊版 Cloud Storage CLI，僅進行最低程度的維護。\ngsutil 工具不支援使用較新的 Cloud Storage 功能，例如軟刪除和託管式資料夾\ngcloud storage 命令需要較少的手動最佳化即可實現最快的上傳和下載速率。\n\n看來我該改程式了。\nNote\nGsutil\n\nrsync 同步\n\ngsutil -m rsync -r /home/user/docker-compose/VideoSokoban/MP4Final gs://abc\n\nchange\n\ngcloud storage rsync -r /home/user/docker-compose-yaml/VideoSokoban/MP4Final gs://abc\n\nref. gcloud storage rsync\n\nsignurl 下載權限\n\ngsutil signurl -d 30m gcs-download.json gs://backup/downloadbak/24-4-17.bak\n\nchange\n\ngcloud storage sign-url -d=30m --private-key-file=gcs-download.json gs://backup/downloadbak/24-4-17.bak\n\n但第一次執行時，可能會發生錯誤\n\nERROR: (gcloud.storage.sign-url) This command requires the pyOpenSSL library. Please install it and set the environment variable CLOUDSDK_PYTHON_SITEPACKAGES to 1 before re-running this command\n\n執行\nexport CLOUDSDK_PYTHON_SITEPACKAGES=1\n\nref. gcloud storage sign-url\n\ndu 查bucket大小\n\ngcloud storage du gs://abc --summarize --readable-sizes\n\nref. gcloud storage du\n或是在Metrics Explorer ，查詢 Total bytes (v2) 選擇 GCS Bucket。\n這邊也有 Total count (v2) 可以查詢加總。\nfilter可指定bucket name。\n完整 api storage.googleapis.com/storage/v2/total_bytes\n\ncors\n\ngsutil cors set example_cors_file.json gs://example_bucket\n改\ngcloud storage buckets update gs://example_bucket --cors-file=example_cors_file.json\n\nref.\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/142.gcs cors設定/\">142.gcs cors設定</a>\nSet up and view CORS configurations\n\ncp 複製\n\ngsutil cp -n gs://abc/57d51e14-6ea2-409a-9bee-07a832456cf6.jpg .\n\ngcloud storage cp -n  gs://abc/57d51e14-6ea2-409a-9bee-07a832456cf6.jpg .\n\nref. gcloud storage cp",
		"tags": [ "note","☁️"]
},

{
		"title": "246. GCP網路費用粗估計算",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/246. GCP網路費用粗估計算/",
		"content": "WHY\n昨天突然說要叫我算一下影音串流，如果每秒2MB的話，一個月多少。\nSolution\nQ: 現在的題目，每秒有 2Mb的串流，每月會有多少費用。\n先計算每月流量\n2*60*60*24*30=5184000(Mb)\n再轉成MB\n51840000/8=648000\n因Google的費用單位都是 GiB (1GB 約0.93GiB)，\n所以 648 GB = 602GiB\np.s GB是十進制，GiB是二進制\n組成\n網路的費用分成兩塊，\nData transfer(資料傳輸) + data processes(資料處理)\n入站不收 data transfer的費用，但會收 data processes的費用。\n出站，兩邊的費用都會收。\nInbound\nprocesses data的服務 有 Load Balancer 、Cloud NAT 、 Protocol forward\n\n從LB 進入的data processes費用為 602*0.008=4.816\nLB的轉發規則前五條，每小時 0.025 ，於是 0.025*24*30=18\n於是，inbound的 LB 資料處理費用為 22.816 USD\nOutbound\n這邊計算時，務必搞清楚 哪一邊是client 哪一邊是 server。\nGKE Private Cluster 的網路入跟出的IP不一樣，\n所以計算方式也不一樣。\n第一種\n客戶直接連線到server抓取串流\n所以GKE是server，是從Load Balancer出。\nData processes 費用\n跟上面inbound的費用一樣，都是 每GiB 0.008 ，所以是 22.816\nData Transfer 費用\n&gt; 602*0.23 = 138.46\n \n 0.23是台灣到中國的每GB費用，\n 到其他地方的價格不一樣。\n \n ref. 虛擬私人雲端 網際網路資料移轉速率-&gt; 進階級計價模式\n所以總金額約為 138.46+22.816 = 161.276\n第二種\n服務直接將串流打到對方的伺服器上\n所以GKE是client，是吃Cloud NAT的流量。\nData processes 費用\n\n(0.044*720) + (0.005*1*720)+(5184*0.045)=43.056\n\n(0.044＊720 ) : 超過32個VM ，NAT 閘道的每小時價格 0.044 ，一個月有720個小時\n(0.005＊1＊720)：NAT閘道的IP，1個每小時0.005\n(172.8＊0.045)： 每處理 GiB 資料、傳入與傳出資料移轉的價格 0.045 ，172.8G\nData Transfer 費用\n&gt; 602*0.23 = 138.46\n \n 0.23是台灣到中國的每GB費用，\n 到其他地方的價格不一樣。\n所以總金額約為 138.46+43.056 = 181.516\nref. Cloud NAT\n補充\nInter-region data transfer\n如果是跨region的話，也是要算錢的。\n\n參考 Inter-region data transfer\nInter-zone data transfer\n如果是相同region，不同zone的話，每 GiB 0.01\n相同region，相同zone的話則不算錢。",
		"tags": [ "note","☁️"]
},

{
		"title": "247. MSSQL指令賦予權限",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/247. MSSQL指令賦予權限/",
		"content": "WHY\n因為之前RD組長真的幹了從刪庫到跑路的過程，\n重點刪了還沒跑路。\n於是每個RD的資料庫權限直接縮減。\n但由於程式是使用ORM ，會自建資料庫。\n導致RD沒有權限可以連到新建的DB，\n我只能手動賦予權限，但每次新增好煩，\n我又沒window，只能用VM開SSMS出來用。\nSolution\n用指令直接解決\n\ncreate USER bmsrd for login bmsrd\nEXEC sp_addrolemember 'db_datareader','bmsrd'\nEXEC sp_addrolemember 'db_datawriter','bmsrd'\n\n以後直接到 Google的 Cloud SQL Studio 執行指令就好。\n或是使用 Azure Data Studio就好了。",
		"tags": [ "note","🗄"]
},

{
		"title": "250. find指令筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/250. find指令筆記/",
		"content": "WHY\n最近在弄crontab的問題，\n結果才發現find 加上 delete滿滿的細節。\nNote\n\n搜尋時間\n\n這邊要先知道一件事，atime, mtime , ctime 都是用n*24 hour\n作為時間比對的依據，\n判斷文件距離上次修改時間已經過了幾個24個小時時段。\nfind . -mtime -1 # 找到最後修改時間為「少於 1 天前」的檔案。\nfind . -mtime 1 # 找到最後修改時間為「剛好 1 天前」的檔案。\nfind . -mtime +1 # 找到最後修改時間為「超過 1 天前」的檔案。\n\n假設現在時間 星期四 14:55\n-mtime -1 最後修改時間 是 星期三的14:55到星期四的14:55之間的檔案\n-mtime 1 最後修改時間是 星期二14:55到 星期三的14:55的檔案\n-mtime +1 最後修改時間是 星期二14:55之前的檔案\n其他還有這兩個參數可供使用\n-atime 最近訪問時間\n-ctime 最近狀態改動時間\n這幾個時間都可用stat查看\n這張圖表示 -5,5,+5的差別\n\n如果要查詢分鐘的話，\n則是使用 -amin,-mmin,-cmin 用法跟上面差不多。\nref. 徹底搞明白find命令mtime含義和用法\n\n查看說明文件\n\nman find\n\n搜尋類型\n\nfind . -type f # 搜尋檔案\nfind . -type d # 搜尋資料夾\n\n搜尋檔案名稱\n\nfind . -iname &quot;docker&quot; # 不區分大小寫\nfind . -name &quot;*.mp4&quot; # 副檔名為mp4\n\n執行後再其他指令\n\nfind . -iname &quot;docker&quot; -exec rm -f {} +\n\n{} 代表將前面搜尋的結果傳到後面的參數，\n+ 表示將結果一次傳進去給rm，另一種作法是 \\;，針對每個資料夾都執行一次rm的指令\n記得\\前面的空格。\n\n排除特定資料夾\n\nfind . -iname &quot;docker&quot; -mindepth 1 -path /home/user/docker -prune -o -print\n\n-o 在find中，代表OR的邏輯，其中一個為true才執行，\n基本語法為find &lt;path&gt; &lt;condition1&gt; -o &lt;condition2&gt;\n所以 -o -print的用意識，如果不是 /home/user/docker的資料夾就print\nref.\n\n在 Linux 下使用 find 指令查詢目錄與檔案的速查筆記)\nUnix/Linux 的 find 指令使用教學、技巧與範例整理\n\n單純紀錄一下麻煩的要死的cp功能，\n後面還是用glcoud storage rsync處理掉，cp真的太慢了。\n#!/bin/bash\ntotal=$(find /home/user/MP4Final -type f -cmin -1 | wc -l)\nfind /home/user/MP4Final -type f -cmin -5 | while read file; do\n\n# 提取檔案名稱\nfilename=$(basename &quot;$file&quot;)\n\n# 提取最後兩個資料夾名稱\nlast_two_dirs=$(echo &quot;$file&quot; | awk -F/ '{print $(NF-2) &quot;/&quot; $(NF-1)}')\n\n# 目標位置\ntarget_dir=&quot;video/$last_two_dirs&quot;\n\n#echo &quot;source:$file&quot;\n#echo &quot;$target_dir/$filename&quot;\ngcloud storage cp -n &quot;$file&quot; &quot;gs://$target_dir/$filename&quot; --quiet\ndone\necho &quot;total: $total&quot;",
		"tags": ["echo", "echo", "note","🐧"]
},

{
		"title": "251.  GCE上的docker監控",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/251.  GCE上的docker監控/",
		"content": "WHY\n之前因為架構關係，建了幾個在GCE上的docker服務。\n外層主機可以透過GCP的metric監控，\n但裡面的docker就不知道了。\nSolution\n查了一下，\n把cAdvisor裝起來就好了。\n這是一個專門用在監控VM主機上的docker。\ncAdvisor\ndocker-compose.yaml\nservices:\ncadvisor:\ncontainer_name: cadvisor\nimage: gcr.io/cadvisor/cadvisor:latest\npull_policy: if_not_present\nrestart: always\nprivileged: true\nvolumes:\n- /:/rootfs:ro\n- /var/run:/var/run:ro\n- /sys:/sys:ro\n- /var/lib/docker/:/var/lib/docker:ro\n- /dev/disk/:/dev/disk:ro\nports:\n- 8888:8080\nnetworks:\n- zlm_network\nnetworks:\nzlm_network:\nname: zlm_network\ndriver: bridge\n\n記得改用 gcr.io/cadvisor/cadvisor:latest\n如果你查到的文章有人用 google/cadvisor:latest\n這種在建制時會發生錯誤\n\nFailed to create a Container Manager: mountpoint for cpu not found\n\n架好後，可使用下面方法測試\ncurl http://127.0.0.1:8888/metrics\n\nprometheus\n要建一個新的或是拿舊的套用都可以。\n重點是config，\n因我的prometheus掛在GKE上面，\n所以直接讓prometheus去取得GCE的metrics。\n- job_name: 'game-result-stage'\nscrape_interval: 15s\nstatic_configs:\n- targets:\n- '&lt;ip&gt;:8888'\n\nGrafana\n可直接匯入 11600 這個dashboard。\nref.\n\nMonitoring Servers and Docker Containers using Prometheus with Grafana",
		"tags": [ "note","☁️"]
},

{
		"title": "252. k8s中的pod呼叫",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/252. k8s中的pod呼叫/",
		"content": "WHY\n其實如果架構可以，\n應該是不需要用到這個功能。\n用這個也有前提，必須要是statefulset。\nSolution\npod本身的name是隨機的，\n所以不太可能用程式寫死去呼叫他，\n當然，真的要硬幹還是可以。\nstatefulSet，他的pod是從0開始往後，\n從k8s的文件看，呼叫單個pod的方式，\n&lt;podname&gt;.&lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local\nref. Stable Network ID\n但是！！！我在測試時，就莫名其妙一直卡關。\n到最後還直接去查kube-dns的值，看是怎麼解析的。\n測試的statefulset是rabbitMQ，\n\n從上圖可以組成下面的網址\ncurl http://qa-rabbitmq-server-1.qa-rabbitmq.rabbitmq-system.svc.cluster.local:15692/metrics\n\n測試時，一直出現錯誤，找不到服務。\n但用pod IP可以找到資料。\n最後氣到，直接裝了nslookup，反查dns名稱了。\napt-get install dnsutils\n\nor\n\nyum install bind-utils\n\n要查詢之前，最好先找到你的kube-dns 的pod ip，\n不過其實預設的dns應該就是kube-dns的ip了\n但為了萬一，我還是強制指定一下，如果有多個pod，隨便抓一個就好，\nkube-dns的ip 192.168.24.157\n\n利用rabbitMQ的ip 反查domain\n結果..ip註冊的domain是\nqa-rabbitmq-server-1.qa-rabbitmq-nodes.rabbitmq-system.svc.cluster.local\n照格式來講沒錯，但他用的是另一個service name，\n為什麼我到現在還是不清楚。",
		"tags": [ "note","⎈"]
},

{
		"title": "253. docker內服務狀態判斷",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/253. docker內監控服務狀態/",
		"content": "WHY\n上一篇 <a class=\"internal-link is-unresolved\" href=\"/404\">251. GCE上的docker監控</a> ，\n能取得服務本身的CPU、Memory..等，\n但如果要判斷服務有沒有正常，\n還真的不知從何著手。\nSolution\n使用 container_last_seen 配上 absent取絕對值\nabsent(container_last_seen{name=~&quot;filebeat&quot;}) OR vector(0)\n\n為什麼要加上 or vector(0)的原因是\n當promQL判斷服務 filebeat還活著的話，\n不會有任何資料。\n\n當服務沒啟動時，才會顯示 1\n於是加上後面 OR vector(0)\n當沒有查詢結果時，預設會顯示 0 ，方便告警復歸。\nref. What container_last_seen use for",
		"tags": [ "note","🐳"]
},

{
		"title": "256. 取得request的header",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/256. httpbin取得request的header/",
		"content": "WHY\n這幾天在GKE幫人架srs origin+edge +zlm ，\n然後到有沒有來源IP的header可供使用，\n我記得有，但忘記名稱，\n現在只有兩條路 ，一條是自己寫個後端回傳header的服務。\n另一個就是找別人寫好的，還好有找到（我就記得我看過），\n不然真的要自己寫了。\nSolution\n有請， Httpbin ，\n要參考用哪些path的話，可以先到官方文件看一下。\n我就直接掛載成deploy和service了。\napiVersion: v1\nkind: Namespace\nmetadata:\nname: xff\nlabels:\nistio-injection: enabled\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: httpbin\nnamespace: xff\nlabels:\ngroup: srs\napp: httpbin\nspec:\nreplicas: 1\nrevisionHistoryLimit: 5\nprogressDeadlineSeconds: 30\nselector:\nmatchLabels:\ngroup: srs\napp: httpbin\nstrategy:\nrollingUpdate:\nmaxSurge: 25%\nmaxUnavailable: 25%\ntype: RollingUpdate\ntemplate:\nmetadata:\nlabels:\ngroup: srs\napp: httpbin\nspec:\ncontainers:\n- name: srs\nimage: kennethreitz/httpbin\nimagePullPolicy: IfNotPresent # IfNotPresent, Always, Never\nports:\n- name: http-web\ncontainerPort: 80\nprotocol: TCP\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nterminationGracePeriodSeconds: 30\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: httpbin\nnamespace: xff\nspec:\ntype: LoadBalancer\nselector:\ngroup: srs\napp: httpbin\nports:\n- name: http-web\nprotocol: TCP\nport: 80\ntargetPort: 80\n- name: https\nprotocol: TCP\nport: 443\ntargetPort: 80\n\n然後，cloudflare掛ip，開啟proxy。\n用自己的電腦呼叫自己設定的domain。\ncurl -X Get https://video-httpbin.abc.com/get\n\n這樣就看得到headers裡面的 Cf-Connecting-Ip 就是來源IP。",
		"tags": [ "note","⎈"]
},

{
		"title": "257. istio-proxy Readiness probe failed",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/257. istio-proxy Readiness probe failed/",
		"content": "WHY\n最近同事在壓測服務，\n然後發現他的pod常常無故重啟，\n不是restart，而是整個pod重建。\nSolution\n2024/12/13更新\n找的方向錯誤，目前是這個NodePool只要掛上Taint後，\n壓測時，就會導致Istio-proxy的readines 每小時重開一次。\n看event事件，發現這個錯誤。\n看來是istio-proxy本身的Readiness導致。\n\n然後檢查istio-proxy本身的resource 也有設定limit。\n先改個limit設定試試。\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: gameservice\ngroup: gameservice\ntemplate:\nmetadata:\nannotations:\nsidecar.istio.io/proxyCPU: 200m\nsidecar.istio.io/proxyCPULimit: 2000m\nsidecar.istio.io/proxyMemory: 128Mi\nsidecar.istio.io/proxyMemoryLimit: 2Gi\nspec:\ncontainers:\n....\n\n這個可以指定單個pod使用此設定，\n如果要設成全域，參考setting-global-resource-requests-for-sidecar-proxy\n但是，這情況仍然會發生。\n猜測可能是壓策時，大量的請求塞到istio-proxy裡面導致無法負荷。\n另一種是直接 停用readiness了 。\n下面的方法，是改寫不是停用，\n目前還沒找到停用的方法。\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: gameservice\ngroup: gameservice\ntemplate:\nmetadata:\nannotations:\n\t\tsidecar.istio.io/rewriteAppHTTPProbers: &quot;false&quot;\nspec:\ncontainers:\n....\n\nref.\n\nIstio 服務的健康檢查\nResource Annotations\nWorkload Group",
		"tags": [ "note","⛵️"]
},

{
		"title": "258. GCP費用試算",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/258. GCP費用試算/",
		"content": "WHY\n之前聽上面的說，他們有買GCP的費用折扣。\n直到最近我開始申請費用表單，\n才發現申請的金額與GCP上面的費用一致，\n搞不懂怎麼一回事。\nNote\n首先，要先到 Billing裡面的Pricing，查看SKU的折扣。\n這邊很多選項，\n建議是先到Report裡面直接拿SKU ID搜尋，Group by選擇 SKU。\n或是到 Google Cloud Platform SKUs 搜尋 SKU ID。\n\n最後，到pricing貼上 SKU ID\n\n就能查到 Effective discount。\n再來以前三高的金額試算\n\n以網路費用(Network Inter Region Data Transfer Out from Americas to APAC):12587.22G，\n以GCP 計算機計算為 $1006.9，折扣 22% ，故 785.382，與費用明細一致。\n計算機計算時，要分清楚是哪一邊。\n以費用名稱來看是屬於 Inter Region 的資料輸出，\n故選擇下面的 Inter-Region。\n\nGCP網站上，以CPU：C2D 使用時間 36248.56，每小時 0.034231，為1240.82 ，折扣 26% 故金額為 918 。\n與費用明細上使用時間為 35281 hours ，金額為 893。視為一致。\b\n\n以CPU:E2 Custom 計算，使用時間為 30550.16， 每小時 0.02651815，為810.133，折扣 31.33% 故金額為 556.3188，\n與費用明細拆成兩筆，使用時間為11567.12+18983.043，費用為 345.66+210.63 = 556.29，視為一致。\n(E2 的網站費用有分成 Custom與一般的，每小時費用不一樣。)",
		"tags": [ "note","☁️"]
},

{
		"title": "259. Loki上手體驗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/259. Loki快速上手體驗/",
		"content": "WHY\n新的服務又掛上去了，\n本來想繼續掛EFK給人用，\n但看了一下，如果監控的同時可以順便看Log好像也不錯。\n監控是grafana，那log只能是 loki了。\nNote\n先來看一下成果。\n\n1. 部屬 Loki\n我是直接從istio(1.21.1)，將loki.yaml直接拿出來部屬。\n路徑在istio-1.21.1/samples/addons/loki.yaml\ngithub\n原本的沒有namespace，需要的自行增加，\n否則直接部屬即可。\n2. 部屬 Promtail\n再來，部屬Promtail將 pod log丟到loki裡面。\nk8s的話建議使用 DaemonSet的方法建。\n如果想用其他方式，請洽install-promtail\n這邊唯一要改的是Loki的位置，\nConfigmap：promtail-config\n底下的 data.clients.url 改為你的loki ip。\n建立loki svc時的port為3100，\n於是 http://loki.istio-system.svc.cluster.local/loki/api/v1/push\n一開始測試時，可以先到任一pod裡面，呼叫 ready的api，\n正常的話會回復一個ready的字串。\ncurl http://loki.istio-system.svc.cluster.local/ready\n\nref.Loki HTTP API\n--- # Daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\nname: promtail-daemonset\nspec:\nselector:\nmatchLabels:\nname: promtail\ntemplate:\nmetadata:\nlabels:\nname: promtail\nspec:\nserviceAccount: promtail-serviceaccount\ncontainers:\n- name: promtail-container\nimage: grafana/promtail\nargs:\n- -config.file=/etc/promtail/promtail.yaml\nenv:\n- name: 'HOSTNAME' # needed when using kubernetes_sd_configs\nvalueFrom:\nfieldRef:\nfieldPath: 'spec.nodeName'\nvolumeMounts:\n- name: logs\nmountPath: /var/log\n- name: promtail-config\nmountPath: /etc/promtail\n- mountPath: /var/lib/docker/containers\nname: varlibdockercontainers\nreadOnly: true\nvolumes:\n- name: logs\nhostPath:\npath: /var/log\n- name: varlibdockercontainers\nhostPath:\npath: /var/lib/docker/containers\n- name: promtail-config\nconfigMap:\nname: promtail-config\n--- # configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: promtail-config\ndata:\npromtail.yaml: |\nserver:\nhttp_listen_port: 9080\ngrpc_listen_port: 0\n\nclients:\n- url: https://{YOUR_LOKI_ENDPOINT}/loki/api/v1/push\n\npositions:\nfilename: /tmp/positions.yaml\ntarget_config:\nsync_period: 10s\nscrape_configs:\n- job_name: pod-logs\nkubernetes_sd_configs:\n- role: pod\npipeline_stages:\n- docker: {}\nrelabel_configs:\n- source_labels:\n- __meta_kubernetes_pod_node_name\ntarget_label: __host__\n- action: labelmap\nregex: __meta_kubernetes_pod_label_(.+)\n- action: replace\nreplacement: $1\nseparator: /\nsource_labels:\n- __meta_kubernetes_namespace\n- __meta_kubernetes_pod_name\ntarget_label: job\n- action: replace\nsource_labels:\n- __meta_kubernetes_namespace\ntarget_label: namespace\n- action: replace\nsource_labels:\n- __meta_kubernetes_pod_name\ntarget_label: pod\n- action: replace\nsource_labels:\n- __meta_kubernetes_pod_container_name\ntarget_label: container\n- replacement: /var/log/pods/*$1/*.log\nseparator: /\nsource_labels:\n- __meta_kubernetes_pod_uid\n- __meta_kubernetes_pod_container_name\ntarget_label: __path__\n\n--- # Clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: promtail-clusterrole\nrules:\n- apiGroups: [&quot;&quot;]\nresources:\n- nodes\n- services\n- pods\nverbs:\n- get\n- watch\n- list\n\n--- # ServiceAccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: promtail-serviceaccount\n\n--- # Rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: promtail-clusterrolebinding\nsubjects:\n- kind: ServiceAccount\nname: promtail-serviceaccount\nnamespace: default\nroleRef:\nkind: ClusterRole\nname: promtail-clusterrole\napiGroup: rbac.authorization.k8s.io\n\n3. 看Log\n先到Promtail的pod看有沒有錯誤，\n沒有的話到grafana裡面，\n如果你的grafana也是istio裡面的yaml安裝起來的話，\n預設就會有loki的datasource，\n不用再額外設定。\n開啟grafana，開啟選單的explorer。\n\ndatasource選 Loki\n\n一開始可以用label選擇，這邊的app是我建立服務時所用的label。\n也可以用deploy或namespace選擇。\n\n最後按下右上的Run Query。\n\n這樣就表示有資料了。\n此時，也可以按右上的Add dashboard，\n套用到現有的dashboard或是建立一個都可以。",
		"tags": [ "note","👁"]
},

{
		"title": "260. Grafana Variables連動",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/260. Grafana 變數連動/",
		"content": "WHY\n延續上一篇 <a class=\"internal-link\" data-note-icon=\"\" href=\"/259. Loki快速上手體驗/\">259. Loki快速上手體驗</a> 的效果。\n要讓log更好用的話，需要幾個變數，\n原本是想做到當按下Label時，可以直接連動變數，\n可惜還沒有找到相關資料，只好先用下拉選單做連動變數。\nSolution\n連動變數，首先針對的都是有第一個來源，\n第二個才能根據第一個的選擇去作查詢。\n\n第一個是namespace，第二個deployment就是filter namespace的值。\n來源是Prometheus，\n因為如果選Loki的話，沒有辦法用Query Result去做，\n只能用label name或 label value。\n\n圖片中間的Regex的意思是，\n取得 kube_deployment_labels底下 deployment的值。\n\n第三層同樣的方法。\n至於為什麼會到三層，因為實務上有碰到，當多個pod的時候，\n你會需要看特定pod的log。\n最後 dashboard的metrics，Panel選擇Logs。\n{namespace=&quot;$namespace&quot;, pod=~&quot;.*$Deploy.*&quot;, pod=~&quot;$Pod&quot;} |= ``\n\n選擇Loki與Grafana的好處在於，\n當你看到異常高峰的metrics時，\n可以直接看到log，不用再跑去另一個地方查，\n也可以說是 懶人的作法",
		"tags": [ "note","👁"]
},

{
		"title": "README",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "度估記事本\n座右銘\n\n懶人，始終相信科技始終來自於人類的惰性。\n懶是一種方式，卻不是一種態度。\n你如果想要有更多的時間偷懶，就一定要有最高的效率。\n\n最新文章\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/256. httpbin取得request的header/\">256. httpbin取得request的header</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/257. istio-proxy Readiness probe failed/\">257. istio-proxy Readiness probe failed</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/258. GCP費用試算/\">258. GCP費用試算</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/259. Loki快速上手體驗/\">259. Loki快速上手體驗</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/260. Grafana 變數連動/\">260. Grafana 變數連動</a>\n\n用Obsidian發佈到Github\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/193. Obsidian發佈到github.io個人網站/\">193. Obsidian發佈到github.io個人網站</a>\n電影評分表\n做好玩的 電影評分表\n\nTag列表\n💡：Map Of Content\n💻：<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/0.Code MOC/\">0.Code MOC</a>\n🐳：<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/0.Container MOC/\">Container MOC</a>\n🗄：<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗄 Database/0.Database MOC/\">0.Database MOC</a>\n⏱：<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/0.Drone MOC/\">0.Drone MOC</a>\n🗒：<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/0.EFK MOC/\">EFK MOC</a>\n☁︎：<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/0.GCP MOC/\">GCP MOC</a>\n📥：inbox\n⛵️：<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/0.istio MOC/\">istio MOC</a>\n⎈：<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/0.K8s MOC/\">k8s MOC</a>\n🐧：<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/0.Linux MOC/\">Linux MOC</a>\n🍎：Apple\n🌐：<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/0.Network MOC/\">Network MOC</a>\n👁：<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/0.Observability MOC/\">0.Observability MOC</a>\n🎮：Game\n🆒：Side Project\n\nWhat is MOC ?\n當單一筆記的數量越來越多時，就能夠構成一個MOC(Map of Contens)\n詳細請參考下面連結，\n數位筆記太多很凌亂怎麼辦？使用 MOC 架構有系統地管理數百則的數位筆記\n技術文章寫法要點\n文章採用單線結構\n一點接着一點，就是一張卡片接着一個卡片，\n把問題拆成多篇文章，\n讓文章一篇就只有一個要點。\nref. # 科技愛好者週刊（第 288 期）：技術寫作的首要訣竅\n\n舊筆記\n2021年以前文章請至 度估記事本 查詢",
		"tags": [ "note","gardenEntry"]
},

{
		"title": " ⎈ K8s MOC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/0.K8s MOC/",
		"content": "基本概念\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/27.K8s的節點選擇與污染/\">27.K8s的節點選擇與污染</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/41.GKE節點無法自動縮小/\">41.GKE節點無法自動縮小</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/42.k8s PDB（pod中斷預算）/\">42.k8s PDB（pod中斷預算）</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/105. linux系統時間與k8s/\">105. linux系統時間與k8s</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/123.Helm 使用方式/\">123.Helm 使用方式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/172. 自建k8s版本升級/\">172. 自建k8s版本升級</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/192. 自建的k8s拉取private registry/\">192. 自建的k8s拉取private registry</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/194. pod debug方式/\">194. pod debug方式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/196. http健康度偵測/\">196. http健康度偵測</a>\n\nTerraform\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/82. Terraform vs Ansible/\">82. Terraform vs Ansible</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/147. 用chatGPT學Terraform/\">147. 用chatGPT學Terraform</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/148. Terraform的基本概念/\">148. Terraform的基本概念</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/149. terraform 部署VM，啓動docker安裝metadata_startup_script/\">149. terraform 部署VM，啓動docker安裝metadata_startup_script</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/189. 導入Terraform/\">189. 導入Terraform</a>\n\nTools\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/118. kubernetes Dashboard/\">118. kubernetes Dashboard</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/137. kubecolor 直觀的kubectl/\">137. kubecolor 直觀的kubectl</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/139. openlens add pod shell or logs button/\">139. openlens add pod shell or logs button</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/162.查詢已棄用的API/\">162.查詢已棄用的API</a>\n\nyaml\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/2. k8s yaml撰寫 volume 踩坑篇/\">2. k8s yaml撰寫 volume 踩坑篇</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/12. k8s YAML 小細節/\">12. k8s YAML 小細節</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/85. gke-cronjob筆記/\">85. gke-cronjob筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/87.kustomize 共用label(optional）/\">87.kustomize 共用label(optional）</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/94. k8s 細節補充/\">94. k8s 細節補充</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/98. k8s中的command與dockerfile的 CMD/\">98. k8s中的command與dockerfile的 CMD</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/136. 一步步篩選k8s的deploy內容/\">136. 一步步篩選k8s的deploy內容</a>\n\nGKE\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/4. GKE使用GPU/\">4. GKE使用GPU</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/37.SolrCloud on GKE/\">37.SolrCloud on GKE</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/39.solrCloud的初體驗/\">39.solrCloud的初體驗</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/44.srs 影音串流 on GKE/\">44.srs 影音串流 on GKE</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/50.GKE workload Identity 實地演練/\">50.GKE workload Identity 實地演練</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/56. GKE記錄 nginx log/\">56. GKE記錄 nginx log</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/70. GKE pvc還原/\">70. GKE pvc還原</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/71.gitlab on GKE 災害還原筆記/\">71.gitlab on GKE 災害還原筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/101. 批次修改hpa/\">101. 批次修改hpa</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/104. rabbitmq operator安裝/\">104. rabbitmq operator安裝</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/43.自建ECK on GKE/\">43.自建ECK on GKE</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/190. cert-manager安裝/\">190. cert-manager安裝</a>\n\nTroubleshooting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Yaml/74. kubernetes 批次檢查yaml特定字元/\">74. kubernetes 批次檢查yaml特定字元</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/76. k8s error,The node was low on resource../\">76. k8s error,The node was low on resource..</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/144. k8s 的sercret複製/\">144. k8s 的sercret複製</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/151. kubeadm重新產生驗證/\">151. kubeadm重新產生驗證</a>",
		"tags": [ "note","⎈"]
},

{
		"title": "批次修改hpa",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/101. 批次修改hpa/",
		"content": "前言\n這又是因為懶惰而衍生出來的一篇，\n懶得一個一個點進去改HPA的數量，\n就寫sh搞定吧。\n正文\n參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/88.批次修改GCP的label/\">88.批次修改GCP的label</a>，建立一個文字檔 hpa.txt，\n先用指令，將hpa的name取出存到文字檔內，因為是針對特定幾個修改，所以並沒有全拉。\nkubectl get hpa;\n\n文字檔內容如下\nistio-yabo-api-prod-external\nistio-yabo-api-prod-internal\nistio-yabo-frontend-external-demo\nistio-yabo-frontend-internal-demo\nistio-yabo-frontend-prod-external\nistio-yabo-frontend-prod-internal\nistio-yabo-frontpage-external-prod\nistio-yabo-frontpage-internal-prod\nistio-yabo-huanggua-external-beta\nistio-yabo-huanggua-external-prod\nistio-yabo-huanggua-internal-beta\nistio-yabo-huanggua-internal-prod\nistio-yabo-landingpage-external-demo\nistio-yabo-landingpage-external-prod\nistio-yabo-landingpage-internal-demo\nistio-yabo-landingpage-internal-prod\nistio-yabo-pwa-external-prod\nistio-yabo-pwa-internal-prod\n\n主要是 kubectl patch hpa 做修改。\npatch這功能看來頗強大，第一次見到，\n改天可研究看看。\nwhile read p; do\necho &quot;$p&quot;; kubectl patch hpa -p '{&quot;spec&quot;:{&quot;minReplicas&quot;: 2}}' -n istio-system $p; echo &quot;done&quot;;\ndone &lt;hpa.txt\n\nref.\n- Kubenetes: change hpa min-replica\n- 使用 kubectl patch 更新 API 對象",
		"tags": [ "note","⎈"]
},

{
		"title": "rabbitmq operator安裝",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/104. rabbitmq operator安裝/",
		"content": "前言\n接到同事說要裝rabbitmq，\n說之前在dev有裝過，但在k8s上面沒看到，\n用之前留下的yaml，會裝不起來，只好從頭來了。\n正文\n\n本來是想參考他的這篇，結果看到第一段說\nStop! There is a Better Way!\n才知道rabbitmq有出了operator可供安裝。\n安裝rabbitmq operator\n\nkubectl apply -f &quot;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&quot;\n\nref. Installing RabbitMQ Cluster Operator in a Kubernetes Cluster\n\n部署yaml\n\napiVersion: rabbitmq.com/v1beta1\nkind: RabbitmqCluster\nmetadata:\nname: dev-rabbitmq\nspec:\nreplicas: 3\nrabbitmq:\nadditionalConfig: |\ndefault_user=admin\ndefault_pass=abc123\nservice:\ntype: NodePort\n\n因為是自建的k8s，沒有lb，所以就用NodePort了。\n更詳細的參數請參考下面連結。\nref. Using RabbitMQ Cluster Kubernetes Operator\n問題排除\n中間有發生小插曲，k8s內沒有預設的storeageClass\n導致建立pvc時發生錯誤，\n雖然在上面的yaml上面能夠指定，\n但可能是格式沒寫好，導致沒有產生statefuleSet 的pvc。\n最後先指定目前的storageClass為預設的。\nkubectl patch storageclass &lt;your_storageclass_name&gt; -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'\n\nref.\n\ninstall RabbitMQ in kubernetes\nRabbitMQ】五分鐘輕鬆瞭解 RabbitMQ 運作",
		"tags": [ "note","⎈"]
},

{
		"title": "linux系統時間與k8s",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/105. linux系統時間與k8s/",
		"content": "前言\n最近弄之前的人自建的k8s，\n才碰到一堆問題，\n我的linux果然需要加強orz\n正文\n一般的linux內的時間顯示\ntimedatectl\n\nLocal time ：本機時間\nUniversal time ： UTC時間\nRTC time： 硬體時間\nNTP service : 自動效時\nTime Zone: 時區\n當時間不對時，可以設定時間的自動效正\ntimedatectl set-ntp yes\n\n設定完後，等一下（多久不確定，但不會立即），\nUTC的時間應該就會自己效對。\nNTP伺服器（有需要再設定）\n\ntime.google.com\n\n查詢各時區\ntimedatectl list-timezones\n\n設定時區\ntimedatectl set-timezone Asia/Taipei\n\nref. Ubuntu Linux 使用 timedatectl 校正時間、時區教學與範例\nPod的時區設定\npod的時間會跟他所在的node時間一致，\n所以當你的node 主機時間錯時，\npod的時間也會跟着錯。\n但基本上都是UTC，\n如果要指定時區的話，\n可以用volume掛載成特定地區。\n或是直接在build image時，一併更改。\n\npvc 掛載\n時區檔案的位置\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox-sleep\nspec:\ncontainers:\n- name: busybox\nimage: busybox\nargs:\n- sleep\n- &quot;1000000&quot;\nvolumeMounts:\n- name: tz-config\nmountPath: /etc/localtime\nvolumes:\n- name: tz-config\nhostPath:\npath: /usr/share/zoneinfo/Europe/Prague\ntype: File\n\nDockerfile更改\n\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\\n\t\t &amp;&amp; echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone\n\nref.\n\nk8s環境下處理容器時間問題的多種姿勢\ndate and time synchronization among the pods and host in kubernetes",
		"tags": [ "note","⎈"]
},

{
		"title": "kubenetes簡易介面",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/118. kubernetes Dashboard/",
		"content": "碎碎念\n以前GKE用習慣了，現在變成自建的k8s，只能用lens管，\n雖然說不上有哪不足，但總是怪怪的。\n今天查資料看到有這個UI，就架來看看了。\n正文\n本機訪問\n官方文件，安裝下去就好了\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml\n\n執行轉發來連到dashboard\nkubectl proxy\n\nkubectl 會使得 Dashboard 可以通過 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ 訪問。\n登入畫面如下\n\n再來就需要產生一組token來做登入了。\n首先建立一組serviceAccount\ncat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: admin\nnamespace: kube-system\nEOF\n\n綁定serviceAccount 與 角色\ncat &lt;&lt;EOF | kubectl create -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: admin-user\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: cluster-admin\nsubjects:\n- kind: ServiceAccount\nname: admin\nnamespace: kube-system\nEOF\n\nService Account建立完後，Kubernetes Token Controller 就會自動的為其產生一個 secret resource(名稱為 [SERVICE_ACCOUNT_NAME]-token-[RANDOM_STRING])\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk '{print $1}')\n\n這邊取得的token即可在 Dashboard 上登入。\nref. 佈署 &amp; 存取 Kubernetes Dashboard\n外部訪問\n更改service 內的 kubernetes-dashboard.kubenetes-dashboards\n將cluster改成NodePort ，然後看對應的port多少就能登入了。\n記得用https\n\nref. accessing-dashboard",
		"tags": [ "note","⎈"]
},

{
		"title": "Helm 使用方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/123.Helm 使用方式/",
		"content": "睡睡念\n最近要用coturn，看到有人用helm 架在k8s上面，就順便來試試了。\n之前剛開始接觸k8s，碰到helm一堆錯誤，無法執行。\n現在想應該是裏面有些設定要修改的關係，\n現在應該會比較好了吧！？\n正文\n架構\nhelm的架構，可先產生一個範例檔，\n看有哪些東西。\nhelm create ezio\n\n在Helm裏面，將kubernetes的應用程式稱為Chart\n\ntree ezio\n\nezio\n├── Chart.yaml\n├── charts\n├── templates\n│   ├── NOTES.txt\n│   ├── _helpers.tpl\n│   ├── deployment.yaml\n│   ├── hpa.yaml\n│   ├── ingress.yaml\n│   ├── service.yaml\n│   ├── serviceaccount.yaml\n│   └── tests\n│   └── test-connection.yaml\n└── values.yaml\n\nChart.yaml ：包含了chart的內容描述\ncharts(folder) ：此目錄可能包含了其他不同的 chart，也可稱為 subcharts\ntemplates(folder)：此目錄為要部署的所有類型範本\nvalues.yaml ： 參數設定檔，所有的變數都寫在這。\n\ntemplates內寫的參數，通常為\n.Values.replicas：表示values.yaml裏面的replicas參數\n其他非values的取得方式，參考 Built-in Objects\ntemplate有一些可能會用到的函數，\n主要有\n\npipeline ( linux 裏面常見到的 | )\nif/else\nwith（類似vb.net 的with）\nrange(for loop)\n\nCheatsheet\ndry-run\n先查看helm所產生的yaml\nhelm install --dry-run coturn ./coturn\n\n設定參數\nhelm install coturn ./conturn --set certificate.enabled=false\n\n指定values.yaml\nhelm install coturn ./conturn -f myval.yaml\n\n指定namespace\nhelm install coturn ./conturn -n tools\n\n看helm列表\nhelm ls\n\n更新chart\nhelm upgrade coturn ./coturn\n\n移除chart\nhelm uninstall coturn\n\nref.\n- [Helm] Helm v3 使用簡介\n- Helm介紹",
		"tags": [ "note","⎈"]
},

{
		"title": "kubecolor 直觀的kubectl",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/137. kubecolor 直觀的kubectl/",
		"content": "睡睡念\n剛好看到有人的文章說 kubecolor美美的kubectl就來試用了XD\n正文\nmac or linux 安裝\nbrew install hidetatz/tap/kubecolor\n\n直接使用\nkubecolor get pod\n\n但我平常打kubectl打習慣了，突然要改用那個覺得好麻煩，\n改別名吧\n新增別名\nalias kubectl=&quot;kubecolor&quot;\n\n移除別名\nunalias kubectl\n\n但這個只有暫時，要永久的話，需要到.bash_profile設定\nmac 是在 個人目錄底下\nvim ~/.bash_profile\n\n沒有的話，就新建檔案，然後輸入\nalias kubectl=&quot;kubecolor&quot;\n\np.s mac 的環境變數載入順序為\n\n/etc/profile -&gt; /etc/paths -&gt; ~/.bash_profile -&gt; ~/.bash_login -&gt; ~/.profile -&gt; ~/.bashrc\n\nref\n\nkubecolor\nmac 組態環境變數，講的太仔細了，非常棒",
		"tags": [ "note","⎈"]
},

{
		"title": "openlens增加shell和logs 按鈕",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/139. openlens add pod shell or logs button/",
		"content": "睡睡念\nlen在6.X版本後，要登入才能用。\n那個時候也正好跳去openlens，在那之前都可以從pod右上角的按鈕，\n快速進入到pod裏面，或是查pod log，但在6.2.4之後就消失了。\n直到現在6.4.10(寫這篇時的版本)，還是沒有。\n最後得知是因為Lens才把那段移掉。\n正文\n安裝方式\n到 Extensions 直接貼上下面的來源\n@alebcay/openlens-node-pod-menu\n\n來源\n主要是框起來的功能，\n對於比較不熟悉的kubectl指令的人來說，非常實用。\n\n所以當初沒有的時候，反而是我同事無法接受。\n至少我還能用指令的方式查我要的東西。\nkubectl exec -it pod_name bash\nkubectl logs pod_name\n...\n\n今天心血來潮查查看有沒有相關問題，是bug還是已移除。\n才知道了，原來是Lens把它從openLens移除，\n詳細可參考下面連結\nhttps://github.com/lensapp/lens/issues/6819\nOpenLens 6.3.0 - No Logs or Shell buttons\n之後再試試其他的 devtron 或 Monokle",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s取得登入憑證",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/143. k8s取得登入憑證/",
		"content": "睡睡念\n幫同事新建一組drone 自動部署的新專案，\n要發版到自建的k8s上面，\n終於碰到我不想面對的東西了。\nkube_ca ，kube_token ，我哪知道這兩個東西在哪裏阿\n正文\n從 Drone Kubernetes可以知道，一個是使用者，一個是ca認證的key，\n但我還不知道去哪生這兩個東西出來。\nKUBE_TOKEN This plugin has one authentication method and that is to use the token to authorize the user.\nKUBE_CA This should be the base64 encoding of your certificate authority. You can get this string by running the command:\n跑去問了一下chatGPT，得到答案。\n執行指令前，必須先連到你要取得的cluster上。\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep cluster-admin | awk '{print $1}')\n\n我的列出有很多個，就拿第一個看起來也最像的來用了。\nadmin-user-token-tm7g4\n\n這邊就會看到token。\n再來是ca\nkubectl config view --raw --minify --flatten -o jsonpath='{.clusters[].cluster.certificate-authority-data}'\n\n放到drone上面，驗證，收工。\n\nref. 使用者認證",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s 的sercret複製",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/144. k8s 的sercret複製/",
		"content": "睡睡念\n本地的k8s機器，要去google artifact registry pull image ，發現沒權限。\n以前的權限在namespace的 default上面，\ndefault太肥了，實在不想再往那邊塞，\n要麻自己創個SA再把json丟去驗證，\n不然看能不能把舊的拿來用。\n正文\n複製\n參考 Kubernetes: copying a secret from one namespace to another\nkubectl get secret my-tlssecret --namespace=default -o yaml | sed 's/namespace: .*/namespace: gameservice/' | kubectl apply -f -\n\n本來my-tlssecret 在 defaul上 ，要改到gamerservice\n這樣就不用再申請一個sa帳號搞認證了，\n如果要從頭開始的話，\n參考google官方文件\n為 Docker 設定身份驗證\n解密\nkubectl get secret test-grafana -n monitoring -o jsonpath='{.data.admin-user}' |base64 --decode; echo",
		"tags": [ "note","⎈"]
},

{
		"title": "kubeadm重新產生驗證",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/151. kubeadm重新產生驗證/",
		"content": "睡睡念\n事情發生在某一天，\n突然有個人跑來跟我說，他該怎麼連樓上自建的k8s，\n那時在處理其他東西，就說我等等給你。\n再來，突然發現我自己連不上去，一直跳錯誤。\nunable to connect to server: x509: certificate has expired or is not yet valid\n正文\n使用openLen連線時，出現錯誤。當下認爲應該是金鑰有問題，\n但那臺機器應該很久沒動了，當初k8s也不是我架起來的，\n頭痛阿。\n\nproxy_server.go:147 Error while proxying request: x509: certificate has expired or is not yet valid\n\n從這篇K8S自建叢集更換證書知道了一件事情\n\n通過kubeadm自建的叢集，初始化後會生成一年的證書，簽發的CA證書有效期默認是10年，簽發的apiserver證書有效期默認是1年\n\n再問一下其他單位的同事，那個環境是不是已經架一年了？\n結果出來，差不多一年了，那可以判斷是金鑰過期，但還是必須到機器裏面檢查一下。\n用ssh或任意方式連到master主機上，\n視權限變更使用者，\n我在執行kubeadm的時候，全程使用root的帳號\n檢查憑證到期日\nkubeadm certs check-expiration\n\n發現真的過期了，重建吧。\n參考下面連結發現有點問題，可能是因為版本有更新。\nref. 解決kubernetes證書過期問題\nkubeadm certs renew # 這樣有問題，會發生錯誤\n\nkubeadm missing subcommand; &quot;renew&quot; is not meant to be run on its own\n\n查詢官方文件\nkubeadm certs renew all\n\n執行完成後，在檢查一次到期日，是否已經變更。\nkubeadm certs check-expiration\n\n但此時還是沒辦法連線，需更新kube config，\n這邊使用一般使用者帳號\nmv config config.old cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config sudo chmod 644 $HOME/.kube/config\n\np.s 上面的文章說要重啟kube-apiserver,kube-controller,kube-scheduler,etcd 這四個容器，但我沒重啓，因為我這台master主機根本沒找到docker，這部分我也很疑惑。\n但一切都正常了，就先這樣吧。\n\n結尾\n如果要從其他地方連到k8s master，檔案就在/etc/kubernetes/admin.conf裏面了。\n不過官方強烈建議，不要將admin.conf與其他人共享。\n最好是另外開一個使用者，\n這部分我研究下，改天在寫篇文章了吧\n相關資訊：\n\n使用 kubeadm 進行憑證管理\nPKI 證書和要求",
		"tags": [ "note","⎈"]
},

{
		"title": "查詢GKE已棄用的API",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/162.查詢已棄用的API/",
		"content": "前言\n這件事說來突然，\n某一天我就發現在GKE上面出現了警告訊息，\n說我的GKE仍在使用已棄用的API，\n但我搬到1.25版本已經是一個多月前的事情了，\n也沒聽到有人在說服務掛掉，\n百思不得其解。\n正文\n\n再來使用了 kubent ，來尋找到底是哪一個服務卡住，\n但很遺憾的都沒有搜尋到。\n過了幾天後，發現之前拿來測試的QA環境，\n也跳出來了已棄用API警告通知。\n\n發現GKE就有提示是什麼呼叫的了，\n在QA環境很清楚的寫着kubent，\n但本來的那個呢？\n只有 v2.1.0 ，\n於是根據這個關鍵字去找哪個image有這個版本號。\n在kube-state-metric上面找到了這個關鍵字，\n這也符合為什麼出錯了沒人知道，\n因爲沒人看（？）\n但因爲metric來源有至少三套，所以不好發現。\n不過這個前任建置的也有點奇怪，\n我查了官方的kube-state-metrics，\n沒找到有用sts建立過的方式，\n有點神奇。\n\n先拿QA來驗證改改看了，\n看後續會不會再跳出來警告",
		"tags": [ "note","☁️"]
},

{
		"title": "自建k8s版本升級",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/172. 自建k8s版本升級/",
		"content": "睡睡唸\n有人說他要測試OME(OverMediaEngine)，串流直播軟體，說比SRS好一些，但自建的k8s版本不夠，不能上gateway API，所以升級吧。\n正文\n更新前請先確認版本更新後，你的服務不會掛掉。\n更新大版本時，會有API棄用的狀況， <a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/162.查詢已棄用的API/\">162.查詢已棄用的API</a>\n目標\n更新k8s的master一臺與node兩臺的版本\n步驟\n\n查詢目前的版本\n\nkuberctl version\n\n（這是目前版本，升級當時的版本為1.22.?)\nserver的版本為1.25.14\n\nServer Version: version.Info\n\n升級kubeadm\n\n# 查看所有可用的kubeadm的版本\nyum list --showduplicates kubeadm --disableexcludes=kubernetes\n# 然後升級kubeadm的版本到1.23.0\nyum install -y kubeadm-1.23.0-0 --disableexcludes=kubernetes\n# 查看kubeadm版本\nkubeadm version\n\n升級master控制\n\nkubeadm upgrade plan\n\n這邊碰到一個問題，我本來只想升級到1.23.0，\n但上面這張圖，他要求直接到1.23.17，\n不照做行不行，『不行』，升級會出錯。\n所以回到第二步，再度更新kubeadm到1.23.17\n再來一次kubeam upgrade plan\n這次沒出現警告了\n\n執行上面執行完upgrade plan提供升級指令\nkubeadm upgrade apply v1.23.17\n\n完成後會看到\n\nSUCCESS! Your cluster was upgraded to &quot;v1.23.17&quot;. Enjoy!\n\n升級kubectl 與 kubelet\n\n查看目前節點版本\nkubectl get node -o wide\n\n驅逐當下節點的pod（這段與參考來源的指令不一樣，這個比較暴力）\n$master為節點的名稱\nkubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 &quot;$master&quot;\n\n等驅逐完後，確認node為SchedulingDisabled狀態，就可已開始升級了。\nyum install -y kubelet-1.23.17-0 kubectl-1.23.17-0 --disableexcludes=kubernetes\n# 重啓服務\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\n查詢node版本是否已經更新，然後解除封鎖。\nkubectl uncordon $node\n\n升級node的版本\n重複第四步驟，只有更新那一段yum install....需要到目標的node機器上做。\n\n所以指令如下\n$node為要更新的node主機\n# master主機\nkubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 &quot;$node&quot;\n# node主機\nyum install -y kubelet-1.23.17-0 kubectl-1.23.17-0 --disableexcludes=kubernetes\n# 重啓服務\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n# master主機\nkubectl uncordon $node\n\nref.\n\nk8s系列12-kubeadm升級k8s叢集 \nUpgrading kubeadm clusters",
		"tags": [ "note","⎈"]
},

{
		"title": "cert-manager安裝",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/190. cert-manager安裝/",
		"content": "前言\n上次開新專案的延伸，\n設定domain時，對於cloudflare不夠熟，\n還好有個前端同事，熟此門路，因爲他也踩過。\n正文\n安裝cert-manager\n參照官網說明。\n下面語法不一定是最新的\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml\n\n再來就是建立 ClusterIssuer 跟 Certificate\n在此之前，要先到cloudflare產生一組token，\n才有權限對這個domain做驗證。\n我的domain是在cloud domain購買，然後再cloudflare託管。\n為什麼不直接在google cloud託管，我也不知道，之前就這樣了。\n\nUser Profile &gt; API Tokens &gt; API Tokens.\n\n參照官方文件，設定權限，取得token\n\n將token存到 secret\napiVersion: v1\nkind: Secret\nmetadata:\nname: cloudflare-api-token-ezio-com\nnamespace: cert-manager\ntype: Opaque\nstringData:\napi-token: &lt;token&gt;\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\nname: cloudflare-issuer-ezio-com\nspec:\nacme:\nserver: https://acme-v02.api.letsencrypt.org/directory\nprivateKeySecretRef:\nname: cloudflare-issuer-ezio-com\nsolvers:\n- dns01:\ncloudflare:\nemail: ezio@abc.com\napiTokenSecretRef:\nname: cloudflare-api-token-ezio-com\nkey: api-token\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: ezio-com-tls\nnamespace: istio-system\nspec:\nsecretName: ezio-com-tls\nissuerRef:\nname: cloudflare-issuer-ezio-com\nkind: ClusterIssuer\ncommonName: &quot;*.ezio.com&quot;\ndnsNames:\n- &quot;*.ezio.com&quot;\n\n這邊注意一下，檔案之間的關聯性。\n上面寫的範例domain是 *.ezio.com ，請改爲自己實際使用的。\n部署上去後，檢查狀態，有沒有Ready，通常不會超過5分鐘。\nkubectl get certificate -A\n\n如果啓動失敗的話，使用下面指令檢查log\nkubectl describe certificate ezio-com-tls\n\n如果是下面這個失敗訊息，\n\nFound no Zones for domain _acme-challenge.aplusmanagex.com. (neither in the sub-domain nor in the SLD) please make sure your domain-entries in the config are correct and the API key is correctly setup with Zone.read rights.\n\n表示你的token沒有相關權限，這個坑我踩完了orz。\nref.\n\nCloudflare\nkubectl apply",
		"tags": [ "note"]
},

{
		"title": "192. 自建的k8s拉取private registry",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/192. 自建的k8s拉取private registry/",
		"content": "前言\n開發環境上，之前拉得都是另一個專案的registry，\n現在有個全新的專案，理所當然image應該拉去另一個地方的GAR。\n正文\n\n首先到IAM裏面，申請一個ServiceAccount，\n然後給權限，可以只給 Artifact Registry讀取者就好。\n然後產生金鑰，名稱為gar-push-bms.json。\n\n建立screct，名稱取名爲gar-pull\nstackoverflow的說明指出，該email是沒在用的，所以可以隨便打。\n\nkubectl create secret docker-registry gar-pull \\\n--docker-server &quot;https://asia-east1-docker.pkg.dev&quot; \\\n--docker-username _json_key \\\n--docker-email gar-pull@.iam.gserviceaccount.com \\\n--docker-password=&quot;$(cat gar-push-bms.json)&quot;\n\ndeployment的yaml設定\n\nimagePullSecrets:\n- name: gar-pull\n\n記得，如果在不同的namespace，需重新建立新的secret。\nref.\n\nCreating image pull secret for google container registry that doesn't expire?\n為 Docker 設定身份驗證",
		"tags": [ "note","⎈"]
},

{
		"title": "pod debug方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/194. pod debug方式/",
		"content": "前言\n這陣子建了一個新環境，\n然後pod在第一次啓動時，都會重啓，\n但一旦重啓就看不到之前的log。\n正文\n看上一次的log記錄\nkubectl logs {PODNAME} --previous\n\n其他用法\n# Return snapshot logs from pod nginx with only one container\nkubectl logs nginx\n\n# Return snapshot of previous terminated ruby container logs from pod web-1\nkubectl logs -p -c ruby web-1\n\n# Begin streaming the logs of the ruby container in pod web-1\nkubectl logs -f -c ruby web-1\n\n# Display only the most recent 20 lines of output in pod nginx\nkubectl logs --tail=20 nginx\n\n# Show all logs from pod nginx written in the last hour\nkubectl logs --since=1h nginx\n\nref .\n\nK8s如何方便尋找事件發生或是取得已消逝的LOG紀錄\nkubectl logs\n\n連進去pod裏面監控\n有時會碰到這個pod沒有權限可以安裝htop或 top等等，\n這些看系統效能的東西，\n此時可以透過這種方式，直接外掛一個debug的image，\n就能隨便你亂搞了，還不怕會影響到正在執行的pod。\n下面指令的意思是，連去 filebeat的pod。\n如果要指定container的話，增加 --target=filebeat的指令。\nkubectl debug -it \\\n--container=debug-container \\\n--image=alpine \\\nfilebeat-beat-filebeat-bcb666b79-vm6dt\n\nref.\n- Debugging Running Pods on Kubernetes",
		"tags": [ "note","⎈"]
},

{
		"title": "http健康度偵測",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/196. http健康度偵測/",
		"content": "前言\n最近碰到一個怪問題，pod活得好好的，但裏面的服務就掛了。\n因為我還能連進去pod裏面，裝一堆東西，但服務就真的沒辦法了。\n只好，跟開發溝通，不然就弄個健康度偵測吧，\n但要注意，一定要非常確定這個服務隨時重啓沒關係。\n正文\nlivenessProbe:\nhttpGet:\npath: /Info/LivenessCheck\nport: 80\nscheme: HTTP\ninitialDelaySeconds: 120\ntimeoutSeconds: 1\nperiodSeconds: 5\nsuccessThreshold: 1\nfailureThreshold: 2\n\n當http 回應的狀態碼是 200~400 的話，默認為正常。\n不是這個以外的，就認爲失敗。\n上面的參數\n\ninitialDelaySeconds: pod啓動後幾秒才開始執行\ntimeoutSeconds: timeout時間\nperiodSeconds: 間隔幾秒呼叫一次\nsuccessThreshold: 在檢查失敗後, 接下來要多少次連續判定成功才算成功\nfailureThreshold: 失敗次數\n\nref.\n\nKubernetes — 健康檢查\n組態存活、就緒和啟動探針",
		"tags": [ "note","⎈"]
},

{
		"title": "254. StatefulSet呼叫pod的方法",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/254. StatefulSet呼叫pod的方法/",
		"content": "WHY\n延續上篇 <a class=\"internal-link\" data-note-icon=\"\" href=\"/252. k8s中的pod呼叫/\">252. k8s中的pod呼叫</a> ，那篇文章中，\n要直接去call statefulSet的pod，\n透過ip反查，找到DNS，\n但如果IP反查也找不到DNS呢？\nSolution\n在建立srs服務時，\n根據之前的測試方式，提供RD 溝通的網址，\n但上線時，一查 這個網址沒有指定，\nIP也沒有對應的FQDN。\n於是，才發現是sts(StatefulSet的縮寫）的設定問題。\n如果要直接跟sts的pod連線時，需要使用headless service。\n\n無頭 Service 不使用虛擬 IP 地址和代理 組態路由和封包轉發；相反，無頭 Service 通過內部 DNS 記錄報告各個 Pod 的端點 IP 地址，這些 DNS 記錄是由叢集的 DNS 服務所提供的。\n\n要使用Headless service的話，則需設定 clusterIP: none\n但 sts指定的service name要指定是有設定 Headless service的service。\nsts.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: srs-edge\nnamespace: srs-qa\nspec:\nreplicas: 2\nserviceName: srs-edge\nselector:\nmatchLabels:\napp: srs-edge\n\nservice.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: srs-edge\nnamespace: srs-qa\nspec:\nclusterIP: None\nselector:\napp: srs-edge\n\n這樣就能夠使用 &lt;podName&gt;.&lt;serviceName&gt;.&lt;nameSpace&gt;.svc.cluster.local 去呼叫pod了。\n不放心，可以參考上一篇<a class=\"internal-link\" data-note-icon=\"\" href=\"/252. k8s中的pod呼叫/\">252. k8s中的pod呼叫</a>用nslookup驗證FQDN\nref. StatefulSet",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s的節點選擇與污染",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/27.K8s的節點選擇與污染/",
		"content": "前言\n最近碰到的問題，起因是單台的對外連線數過大，導致程式發生問題，無法再連線到外部網路。\n正文\n在 GCP網路對外的方式裡面有提到，對外連線時，如果是私有叢集會使用Cloud NAT，\n本次的情形，是因為再該節點內，某一個deploy疑似將連線數吃滿，最近想通了，應該是節點數量的問題。\nnodeSelector\n所以要將某一個服務，獨立掛在單一的節點池上面。\n最簡單的用法是 nodeSelector，在 spec.template.spec 的下一層。\napp : websocket 這個是label 在建立nodepool時，一併建立的。\n也可在建立nodepool後，在手動新增label。當然也可以直接指定google賦予的label。\n\n查詢label\nkubectl get nodes --show-label\n新增label\nkubectl label node &lt;node name&gt; &lt;label&gt;=&lt;value&gt;\n\nref.\nKubernetes 分派 Pod 到指定節點\ndeploy的範例\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deployabc\nspec:\nreplicas: 10\ntemplate:\nspec:\n.......\nnodeSelector:\napp: websocket\n\nref.\n將 Pod 部署到特定節點池\n將 Pod 分配給節點\n節點污點\n剛上面有提到，deploy可以指定pod到特定的節點，但是當前面的節點滿了後，其他的deploy還是有可能會到特定的節點上。\nk8s可以設定節點為 污點(Taint)，使此污染的節點，只能接受有容忍度的pod。\n污點的設定不能事後更新，所以要在一開始建立時就設定好。請注意(fig.2)的Taint。\n另外建議統一由GKE的節點污點，從控制台建立或使用指令建立。\n否則當node自動新增時，該節點並不會產生 Taits\n設定時，要指定 key 與 value 以及 效果。\n\n效果有以下三種：\n- NoSchedule：不能容忍此污點的 Pod 不會被調度到節點上；現有 Pod 不會從節點中逐出。\n- PreferNoSchedule：Kubernetes 會避免將不能容忍此污點的 Pod 安排到節點上。\n- NoExecute：如果 Pod 已在節點上運行，則會將該 Pod 從節點中逐出；如果尚未在節點上運行，則不會將其安排到節點上。\nref.用節點污點控制調度\n之後在deploy上面建立容忍(toleration)污點， 在 spec.template.spec 的下一層\n其中 key 與 value 為自訂，效果請參考上面選擇。\napiVersion: apps/v1\nkind: pod\nmetadata:\nname: deployabc\nspec:\nreplicas: 10\ntemplate:\nspec:\n\t\tcontainer:\n\t .......\n\t\ttolerations:\n\t\t- key: &quot;visible&quot;\n\t\t operator: &quot;Equal&quot;\n\t\t value: &quot;private&quot;\n\t\t effect: &quot;NoExecute&quot;\n\t\t tolerationSeconds: 3600\n\n補充，如果是舊的節點集區，在GKE上面，可以改使用這個label去選擇。\ncloud.google.com/gke-nodepool: &lt;節點池的名稱&gt;\n\nref.\n污點和容忍度\nk8s進階篇（四）：Affinity and Anti-Affinity、Taints and Tolerations",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s PDB（pod中斷預算）",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/42.k8s PDB（pod中斷預算）/",
		"content": "前言\nK8S真是 TXD博大精深......，\n沒想到再追<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/41.GKE節點無法自動縮小/\">41.GKE節點無法自動縮小</a>的時候，會到這邊來。\n正文\n簡單來說，這個設定是用來避免pod被無預警的刪除。\n當發生自願中斷的時候，讓此pod不會因為pod的數量過少，\n而導致服務中斷。\n這個跟 HPA有點像，但HPA是根據設定，來決定pod要不要增加或減少。\n自願中斷通常是下面三種：\n\n排空（drain）節點進行修復或升級\n從集群中排空節點以縮小集群（瞭解集群自動擴縮）。\n從節點中移除一個 Pod，以允許其他 Pod 使用該節點。\n\n我們一般直接下指令看有哪些pdb在系統內\nkubectl get pdb -A\n\n(fig.1)\n建立pdb，主要有兩個參數 看是要設定 minAvailable (最小可用)或 maxUnavailable(最大不可用)。\n建完後，可以看一下ALLOWED DISRUPTIONS 這個欄位，\n這個欄位表示目前該pod可以被中斷的pod數量。\n所以如果為0的話，您的自願中斷那些操作，\n是不會動作的，會一直waiting，直到這個值不為0。\nref.\n\n干擾(Disruptions)\n為應用程序設置干擾預算(Disruption Budget)\nkubernetes之PDB\nkubernetes之PDB",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s error,The node was low on resource..",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/76. k8s error,The node was low on resource../",
		"content": "前言\n之前發生過的，\nprometheus會這樣、gitlab最近在弄新版的CI/CD也跑這個出來。\n本來想要之後有空再解決，但最近被DDOS攻擊時，\n發現監控的grafana會卡住，因為他的prometheus ram爆了~~\n正文\n\n從字面上看來，其實蠻好理解的，\n\nThe node was low on resource: memory. Container web was using 4236080Ki, which exceeds its request of 0.\n\nnode的資源不夠了，container 使用量達到上限。\n最直覺的方式就是，資源不夠，那就加開資源阿。\n但開資源前要先想一下，如果我開了，那未來資源就一定夠嗎？\n開的資源又不一定是完全給prometheus用，他供應的是全體的pod。\n所以要先做的是，把 resources request 的加進去 pod裡面。\n\u001f\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\napp: webhook\nversion: v1\nspec:\ncontainers:\n- image: prom/prometheus:v2.28.0\nimagePullPolicy: Always\nname: prometheus\nports:\n- containerPort: 80\nprotocol: TCP\nresources:\nrequests:\ncpu: 30m\nmemory: 30Mi\nlimits:\nmemory: &quot;128Mi&quot;\ncpu: &quot;500m&quot;\n\n這邊要特別注意 limit，當limit踩到上限時，這個pod就會被終止\n，並引發記憶體不足(OOM)的錯誤。\n所以 limit 要設嗎？？\n這邊要從掛載的服務去決定，\n如果今天掛載的服務 ram的使用量通常不多，\n那突然達到上限，是不是可以判斷這個服務有問題了，必須要重啟。\n但如果是使用prometheus，記憶體的量本來就會隨着metric的量而增加，\n那就不適合加上去了，不然只會一直看到 記憶體不足的錯誤。\n後面的單位，可設定 Ki, Mi ,Gi...\n補充，\n當node資源不足時，刪除pod的先後順序為，\n\n沒有設定 request 跟 limit的 pod\n只要有pod設定 request 或 limit ，但使用的資源是比在資源上設置的請求還多。\n只要有pod設定 request 或 limit ，但使用的資源是比在資源上設置的請求還少。\n\n結論\n先設定 request ，因node機器的擴展，是根據 request的值，\n來判斷要不要增加node數量，如果沒設定request，\n系統會認爲，沒有擴展的必要。就會導致前面的問題。\nref.\n\nWhy my pods are evicted on Kubernetes?\n節點壓力驅逐\n為容器管理資源\nHow does scale-up work?",
		"tags": [ "note","⎈"]
},

{
		"title": "GKE上架設SolrCloud",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/37.SolrCloud on GKE/",
		"content": "正文\nSolr的架構\nref. 淺談Solr叢集架構\n完整架構\n\nSolrCloud是基於 Solr 與 <a class=\"internal-link is-unresolved\" href=\"/404\">blog.36.分佈式系統-Zookeeper</a>搜尋方案。\n特色功能有\n\n集中式的配置資訊\n自動容錯\n近實時搜尋\n查詢時自動負載平衡\n自動分發\n日誌跟蹤\n\nsolrCloud的架構，可以看看以下幾篇。\nref.\n\nExploring the Apache Solr Operator v0.3.0 on GKE\nSolrCloud分佈式企業搜索引擎架構原理解析\nsolrCloud架構\nSolrCloud集群架構\n\n這邊直接開始安裝吧。\n首先確定你的電腦有helm ，我試著找有沒有單純可以用 kubectl apply的方式安裝，但沒找到。所以，用helm吧。\nref.solr-operator\n\n先增加solr的倉庫\n\n\thelm repo add apache-solr https://solr.apache.org/charts\n\n\thelm repo update\t\n\n安裝 solr-operator，指定安裝在 solr的 namespace，避免istio的istio-proxy安裝進去\n\n\tkubectl create -f https://solr.apache.org/operator/downloads/crds/v0.3.0/all-with-dependencies.yaml\n\n\thelm install solr-operator apache-solr/solr-operator \\\n--version 0.3.0 --namespace solr\n\noperator 安裝完成 (fig.1)\n\n(fig.1)\n3. solrCloud.yaml\napiVersion: solr.apache.org/v1beta1\nkind: SolrCloud\nmetadata:\nname: video\nnamespace: solr\nspec:\ncustomSolrKubeOptions:\npodOptions:\nresources:\nlimits:\nmemory: 3Gi\nrequests:\ncpu: 700m\nmemory: 3Gi\ndataStorage:\npersistent:\npvcTemplate:\nspec:\nresources:\nrequests:\nstorage: 2Gi\nreclaimPolicy: Delete\nreplicas: 3\nsolrImage:\nrepository: solr\ntag: 8.8.2\nsolrJavaMem: -Xms500M -Xmx500M\nupdateStrategy:\nmethod: StatefulSet\nzookeeperRef:\nprovided:\nchroot: /explore\nimage:\npullPolicy: IfNotPresent\nrepository: pravega/zookeeper\ntag: 0.2.9\npersistence:\nreclaimPolicy: Delete\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 2Gi\nreplicas: 3\nzookeeperPodPolicy:\nresources:\nlimits:\nmemory: 500Mi\nrequests:\ncpu: 250m\nmemory: 500Mi\n\n佈署 solrCloud.yaml，這邊的系統資源要求頗高，資源不夠請多開一些cpu 或 memory。\n\n\tkubectl apply -f solrCloud.yaml\n\n由於我是直接佈署在istio上面，所以開啟網頁的話，我是直接新增 virtualService的設定。\n\nkind: VirtualService\nmetadata:\nname: istio-virtualservice-tools\nnamespace: tools\nspec:\nhosts:\n- &quot;*&quot;\ngateways:\n- istio-gateway-tools.istio-system.svc.cluster.local\nhttp:\n- match:\n- uri:\nexact: /\n- uri:\nprefix: /solr\nname: solr\nroute:\n- destination:\nhost: video-solrcloud-common.solr.svc.cluster.local\nport:\nnumber: 80\n\n也可以直接在本機上使用port-forward轉發開啟網頁。如以下指令，開啟 網頁 http://localhost:8080 (fig.2)\n\tkubectl port-forward service/video-solrcloud-common -n solr 8080:80\n\n監控 solrCloud\n本來的solrCloud舊有簡易的監控畫面了，\n這部分是看記憶體的使用量。\n\n但可有發現資料並不是非常得多。\n之前在GKE的叢集上已經有架設prometheus了，\n所以這次也要將資料丟進去prometheus裡面。\n這邊要先export solr的metric，\n官方 github上面有建議的方法 ，但需安裝 prometheus operator。如果沒有安裝，prometheus 在探索metrics時，是找不到這個exporter的。\nref.Deploy Prometheus Exporter for Solr Metrics，\napiVersion: solr.apache.org/v1beta1\nkind: SolrPrometheusExporter\nmetadata:\nlabels:\ncontroller-tools.k8s.io: &quot;1.0&quot;\nname: explore-prom-exporter\nnamespace: solr\nspec:\ncustomKubeOptions:\npodOptions:\nresources:\nrequests:\ncpu: 300m\nmemory: 800Mi\nsolrReference:\ncloud:\nname: &quot;video&quot;\nnumThreads: 6\nimage:\nrepository: solr\ntag: 8.8.2\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: solr-metrics\nlabels:\nrelease: prometheus-stack\nspec:\nselector:\nmatchLabels:\nsolr-prometheus-exporter: explore-prom-exporter\nnamespaceSelector:\nmatchNames:\n- solr\nendpoints:\n- port: solr-metrics\ninterval: 15s\n\n因為我不想裝 prometheus operator，所以複製上面安裝的deploy yaml，自己產生一個solr exporter 的deploy跟 service給 prometheus抓資料。\n或許有人會問，為什麼不利用上面的部屬完後去修改service.yaml把\nprometheus.io/port: &quot;80&quot;\n改成\nprometheus.io/port: &quot;8080&quot;\n就好了。\n因為，改了後他會一直還原成舊的狀態。\n這應該是 solrOperator的關係，會一直覆蓋手動修改的設定，因為我也找不到去修改增加solrOperator的設定，讓80改成8080，所以應該透過上面的 ServiceMonitor 來讓prometheus可以取得資料。\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nnamespace: solr\nname: explore-prom-exporter\nannotations:\ndeployment.kubernetes.io/revision: &quot;1&quot;\n# generation: 1\nlabels:\ncontroller-tools.k8s.io: &quot;1.0&quot;\nsolr-prometheus-exporter: explore-prom-exporter\ntechnology: solr-prometheus-exporter\nspec:\nprogressDeadlineSeconds: 600\nreplicas: 1\nrevisionHistoryLimit: 10\nselector:\nmatchLabels:\nsolr-prometheus-exporter: explore-prom-exporter\ntechnology: solr-prometheus-exporter\nstrategy:\nrollingUpdate:\nmaxSurge: 25%\nmaxUnavailable: 25%\ntype: RollingUpdate\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\ncontroller-tools.k8s.io: &quot;1.0&quot;\nsolr-prometheus-exporter: explore-prom-exporter\ntechnology: solr-prometheus-exporter\nspec:\ncontainers:\n- args:\n- -p\n- &quot;8080&quot;\n- -n\n- &quot;6&quot;\n- -z\n- video-solrcloud-zookeeper-0.video-solrcloud-zookeeper-headless.solr.svc.cluster.local:2181,video-solrcloud-zookeeper-1.video-solrcloud-zookeeper-headless.solr.svc.cluster.local:2181,video-solrcloud-zookeeper-2.video-solrcloud-zookeeper-headless.solr.svc.cluster.local:2181/explore\n- -f\n- /opt/solr/contrib/prometheus-exporter/conf/solr-exporter-config.xml\ncommand:\n- /opt/solr/contrib/prometheus-exporter/bin/solr-exporter\nimage: solr:8.8.2\nimagePullPolicy: IfNotPresent\nlivenessProbe:\nfailureThreshold: 3\nhttpGet:\npath: /metrics\nport: 8080\nscheme: HTTP\ninitialDelaySeconds: 20\nperiodSeconds: 10\nsuccessThreshold: 1\ntimeoutSeconds: 1\nname: solr-prometheus-exporter\nports:\n- containerPort: 8080\nname: solr-metrics\nprotocol: TCP\nresources:\nrequests:\ncpu: 300m\nmemory: 800Mi\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nschedulerName: default-scheduler\nsecurityContext:\nfsGroup: 8080\nterminationGracePeriodSeconds: 10\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\nnamespace: solr\nname: explore-prom-exporter\nannotations:\nprometheus.io/path: /metrics\nprometheus.io/port: &quot;8080&quot;\nprometheus.io/scheme: http\nprometheus.io/scrape: &quot;true&quot;\ncreationTimestamp: &quot;2021-09-03T05:52:23Z&quot;\nlabels:\ncontroller-tools.k8s.io: &quot;1.0&quot;\nservice-type: metrics\nsolr-prometheus-exporter: explore-prom-exporter\nspec:\nports:\n- name: solr-metrics\nport: 80\nprotocol: TCP\ntargetPort: 8080\nselector:\nsolr-prometheus-exporter: explore-prom-exporter\ntechnology: solr-prometheus-exporter\nsessionAffinity: None\ntype: ClusterIP\n\n最後到prometheus看一下有沒有關於solr的資料(fig.4)\n\n(fig.4)\n有資料了，但我們不知道要拿哪些metrics來作爲監控的資料，就先去grafana dashboard找找吧。\nSolr Dashboard\n下載，匯入grafana收工。\nref. Solr Prometheus Exporter\n結尾\n再來就是，實際操作 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/39.solrCloud的初體驗/\">39.solrCloud的初體驗</a>了\n額外參考，雖然此連結已經不適用，但基本的安裝方式是可以參考。\nRunning Solr on Kubernetes",
		"tags": [ "note","⎈"]
},

{
		"title": "solrCloud的初體驗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/39.solrCloud的初體驗/",
		"content": "正文\n傻瓜操作，都是在UI上面執行。\n但solr都是使用api的方式做動作。\n所以下面的操作改成api是也可行的。\n基本篇\n\n新增資料\n\n查詢資料\n\n基本的查詢方式，\n如果要查筆數的話，將rows 改成 0 ，就會列出所有筆數。\n但不會顯示任何資料。\n刪除所有資料\n如果要刪除單筆，只要修改條件即可\n將 : 改成要搜尋的條件，\n(e.g. videoId: 28748)\n\n\t&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt;\n\t&lt;commit/&gt;\n\nref.\nsolr刪除資料的四種方法\nSolr官方文檔\n進階篇\n\n查詢指令\n\nref.\nSolr 查詢參數\nsolr搜尋詳解\nSolr查詢界面\n\n查看collection的架構\n\n\thttp http://&lt;host&gt;:&lt;port&gt;/solr/&lt;collectionName&gt;/schema/fields\n\nref. Schema API\n\n搜尋中文字，欄位結構 text_general vs string\n\n一般我們搜尋中文要額外安裝中文分詞，這樣搜尋關鍵字的時候，才會正確。\n沒裝的話，會有下面的情況。\n例如，搜尋 『中文』 ，但這樣會將欄位裡面除了有『中文』的資料撈出來以外，還會將資料中有『中』的字取出。\n但後來同事有發現另一種解法，建立結構的時候，不能選擇 text_general，必須要選 string。\n如果使用string的話，要注意的是只接受完整的文字搜尋。也就是一串文字中，搜尋的關鍵字要完全符合才會顯示。\n而text_general，可接受部分的文字搜尋，但缺點就是搜尋中文時，會有上面的狀況。\n結構使用string的話，如果改使用\n\t*中文*\n\n這種搜尋方式，就只會搜尋資料中有『中文』關鍵字的資料，字串不會被拆開查詢。\n本來是要使用中文分詞，但後來也不用了。\n\nsolr的外掛套件\n\n本來官方是提供blob store API安裝套件，\n但在 solrCloud 8.4 ，有多出了一個功能 package managr\n可使用簡單且快速的方式安裝plugin。\n但因為上面的第三點，所以也不用研究這塊了。\n留下一些參考網址，留着以後查詢。\n初步看都是在solr 啟動時要加一個指令\n-Denable.packages=true\n\n這個應該是在yaml佈署的時候，就要加上去了。\nyaml佈署時，為容器設置啟動時要執行的命令和參數\n也有在猜測，未來應該是可以直接使用 solr operator安裝的時候一並設定，但目前參數沒有看到，看未來會不會新增。Solr Operator\nref.\n\nPackage Management\nSolr 8.4.0 – Plugin Management\nWorking with Solr Plugins System\n在SolrCloud模式下添加自定義插件",
		"tags": [ "note","⎈"]
},

{
		"title": "GKE使用GPU",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/4. GKE使用GPU/",
		"content": "前言\n接了一個工作，要幫人在GKE上面生一個能夠使用GPU的叢集，並且佈署。\n正文\n要使用Google上面的 GPU，GKE版本要到1.9以上，\n節點池要到1.11.3以上。\n另外，GPU只支援通用N1的機器支援，其他細項 請詳閱 Google 運行 GPU\n可用性建議先用\ngcloud compute accelerator-types list\n看一下你的區域支援哪些顯卡，\n像我這邊的是 asia-east1-a ，就只支援特定顯卡。\n\n到 安裝 NVIDIA GPU 設備驅動程序，這邊基本上都沒問題。\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n這主要會裝在每個節點上的Daemonset，當pod起來後，會自動在pod上面安裝驅動程式。\n所以必須檢查一下，看pod是否有起來。\n\n也可以執行\nkubectl describe node -l cloud.google.com/gke-accelerator | grep nvidia.com/gpu\n看顯卡的使用情況\n當執行下一步驟配置 Pod 以使用 GPU時，這邊就會有問題了。\n他的image(nvidia/cuda:10.0-runtime-ubuntu18.04)會無法啟動，一直在crashLoopbackoff。\n更新\n在12/24有開工單詢問，他們那邊給出了解法。後續應該會改他們的文件。\n\ngoogle的回覆如下\n按照文檔簡單使用“command: [&quot;/bin/bash&quot;]”測試。此時bash執行後立刻退出，故pod被重啟。\n你們可以嘗試使用以下死循環避免這個問題：\ncommand: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]\nargs: [ &quot;while true; do sleep 600; done;&quot; ]\n\n所以，可以改成下面的YAML，就能正常執行了。\napiVersion: v1\nkind: Pod\nmetadata:\nname: gpu-pod\nspec:\ncontainers:\n\nname: digits-container\nimage: nvidia/cuda:10.0-runtime-ubuntu18.04\ncommand: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]\nargs: [ &quot;while true; do sleep 600; done;&quot; ]\nresources:\nlimits:\nnvidia.com/gpu: 1\n\n以下也可以使用\n最後的解法是，不要用他的image。\n完整 YAML\napiVersion: v1\nkind: Pod\nmetadata:\nname: gpu-pod\nspec:\ncontainers:\n- name: digits-container\nimage: nvidia/digits:6.0\nresources:\nlimits:\nnvidia.com/gpu: 1\n佈署完後，執行\nkubectl exec gpu-pod -- nvidia-smi\n這段話的意思是，在gpu-pod裡面執行 nvidia-smi的指令\n正常的話，會顯示如下\n\n下面參考的文件，可以看看，但目前只需要做到上面那段『安裝 NVIDIA GPU 設備驅動程序』的指令即可\nref.\nCrashLoopBackOff message wen running GPU Jobs example and others\nGPU Sharing on GKE DaemonSet\nInstall GPU in GKE(Google Kubernetes Engine)（中）\n查詢各有哪些VM使用GPU的指令\ngcloud compute instances list --format=&quot;table(guestAccelerators.acceleratorCount, name)&quot;",
		"tags": [ "note","⎈"]
},

{
		"title": "GKE節點無法自動縮小，查錯之旅",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/41.GKE節點無法自動縮小/",
		"content": "前言\n果然東西穩定之後，上面的都會想要costDown，\n不過這也算是當初建立的人沒設定好，\n會發生這個情況，後續也查明了，當初在每個節點上只允許放64個pod，\n所以就算放滿，也吃不到那麼多。\n但沒有自動縮小，這其實也是個問題，但要自動縮小，\n還要看這個服務是否合適...。\n反正我之後反而沒在糾結這個狀況了。\n正文\n最近發現一個東西，在GKE上面的節點，cpu分配很奇怪。\n\n(fig.1)\n每個node的節點都只佔了一半的CPU就去開新的節點了。\n本來懷疑是自動擴展有問題，\n後來發現應該是自動縮小有問題。\n這個狀況在新建一個叢集，然後設定節點自動伸縮就會發生了。\n一般來說，節點只要沒有pod在裡面，大約只要十分鐘就會把多餘的節點移除了。\nref. 集群自動擴縮程序的工作原理\n但新建了一個節點池，裡面都空的。他並沒有縮小到最小節點。\n詢問後得知，須先看一下log才能知道為什麼沒有自動縮小。\n首先開啟 GCP的日誌查看器，\na. 資源選擇 Kubernetes Cluster\nb. 日誌類型列表選擇\ncontainer.googleapis.com/cluster-autoscaler-visibility\n\nc. 再選擇要查看的cluster name\n\n(fig.2)\nd. 展開 noDecisionStatus\n\n(fig.3)\n看到錯誤了，就是這個害得pod無法自動縮減。\n這邊的錯誤訊息是\nnode.pod.has.local.storage，\n新建節點無法縮減會出現的錯誤訊息通常是\nnode.kube.system.unmovable\n\n(fig.4)\n碰到 node.pod.has.local.storage的解法是在deploy的yaml上面加上註釋，位置位於 metadata底下\nannotations:\n\tcluster-autoscaler.kubernetes.io/safe-to-evict: &quot;true&quot;\n\n(fig.5)\nref.\n\n集群未縱向縮容\nHow to make sure Kubernetes autoscaler not deleting the nodes which runs specific pod\n\n至於碰到 node.kube.system.unmovable 的解法，\n需建立 PDB ，讓在 namespace 的 kube-system可以被驅逐。PDB是什麼可參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/42.k8s PDB（pod中斷預算）/\">42.k8s PDB（pod中斷預算）</a>。\nkubectl create poddisruptionbudget &lt;pdb name&gt; --namespace=kube-system --selector app=&lt;app name&gt; --max-unavailable 1\n\n要注意，selector的label不一定是app開頭，\n要看各deploy裡面的設定。\n最近碰到在GKE的kube-system裡面pod label，\n反而是叫 k8s-app:kube-dns，selector可以隨意選擇，只要有辦法對到deploy即可\n\n(fig.6)\nHow to set PDBs to enable CA to move kube-system pods?\n但後來有發現一個問題，當解了一個 node.kube.system.unmovable ，後續可能會出現 node.pod.has.local.storage的問題，\n這樣會變成必須針對每個deploy都額外做設定。\n所以最後我選擇了，直接手動驅逐節點內的pod，\n但pdb的設定要先看一下，\nistio在安裝時，ingressgateway會同時設定pdb，\n當pod只有1個的時候，是沒辦法驅逐的，\n所以最好是預設ingressgateway 為 2個pod，這樣才能避免pdb的干擾，而導致無法驅逐。\nkubectl drain [node-name] --force --ignore-daemonsets --delete-local-data\n\ndrain的參數\n--force\n當一些pod不是經 ReplicationController, ReplicaSet, Job, DaemonSet 或者 StatefulSet 管理時\n就需要用--force來強制執行 (例如:kube-proxy)\n--ignore-daemonsets\n無視DaemonSet管理下的Pod\n--delete-local-data\n如果有mount local volumn的pod，會強制殺掉該pod並把資料清除掉\n另外如果跟本身的配置訊息有衝突時，drain就不會執行\nref.\n\nKubernetes中清空一個node的所有pod\n文件-遷移工作負載",
		"tags": [ "note","⎈"]
},

{
		"title": "srs 影音串流 on GKE",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/44.srs 影音串流 on GKE/",
		"content": "前言\n首先介紹一下srs，這個就是一個影音串流，\n本來是乖乖再用EFK on GKE，然後說網路有問題，\n就又被抓去協助了，想裝死還不行 T_T ，\n我的東西還不夠多嗎 (╯‵□′)╯︵┴─┴\n正文\n這次主要是協助除錯，\n所以下面就直接列出碰到的問題點。\n由於我沒玩過直撥，有些概念是後來才知道的。\n也會順便列在下面。\n\nwebrtc推流時，出現 getUserMedia error\n\n需要到chrome上面修改設定。\n\nref.解決 ‘GETUSERMEDIA’ OF UNDEFINED 問題\n\nrtmp 推拉\n\n走TCP的協定\n使用rtmp播放的話，所有的連線都會在伺服器上面留一份。\n\nwebrtc播放\n\n需要到srs 4.0.14以後的版本\n有TCP跟 UDP的協定，\n伺服器只負責對 直撥者跟觀看者做驗證，\nwebrtc，是直撥者跟觀看者直接連線，也就是點對點的連接方式。\nref. 何謂WebRTC\n\nwebrtc觀看方式\n\n也是這一部分的網路架構，導致我不得不跳下去協助處理。\n因為GKE的service 負載平衡目前還不支援混合的網路架構，\n也就是TCP跟 UDP 混合使用。只能使用單一個協定。\n所以沒辦法像傳統機器一樣，直接一個ip，同時開啟TCP跟UDP的網路協定。\n所以會變成 TCP 一個IP，UDP又是一個IP(fig.1)，\n\n此時在SRS上面的 Candidate ，必須要設定 *\n然後再推流的時候，推流網址要更改成\nwebrtc://35.2.3.1/live/ray123?eip=35.3.3.50\n\n35.2.3.1 是 只開TCP的外網位置。\n35.3.3.50 是 只開UDP的外網位置。\nref.\nv4_CN_WebRTC\n[SRS+docker]實現直播服務器 3 基於webRTC協議的srs低延遲直播研究_wltsysterm的博客-程序員秘密\n\n其他參考文件\n\n當然最好的方式，是把裡面的文件大約掃過一下。\n但這資料有點多，我也只挑了幾個重點看。\nref.\n\nv4_CN_K8S\n當SRS遇到K8s：如何實現高可用、回滾與灰度發布？\n使用 Linux nc 測試UDP有沒有通\nSRS全文件\nSRS full.conf",
		"tags": [ "note","⎈"]
},

{
		"title": "GKE workload Identity 實地演練",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/50.GKE workload Identity 實地演練/",
		"content": "前言\n某位大神說，GKE使用 IAM的角色認證安全性不太夠，\n所以希望未來都改用這種，workload Ideneity，\n就開始了採坑之旅，是說這個坑沒想像中的深。\n正文\n查詢cluster是否有 workload Identity\ngcloud container clusters describe yaboxxx-test -z asia-east1-b\n\n建議最好直接加上地區，不然看錯誤訊息也是可以。\n會說在某個地區找不到此cluster\n修改cluster ，啟用 Workload Identity\ngcloud container clusters update istio-test -z asia-east1-b \\    --workload-pool=rd7-project.svc.id.goog\n\n修改 node-pool\ngcloud container node-pools update defaultpool -z asia-east1-b \\    --cluster=istio-test \\    --workload-metadata=GKE_METADATA\n\n如果成功的話，會看到\nGKE 中繼資料伺服器 ： 已啟用\n\n建立ns\nkubectl create ns token-proxy\n\n建立kubernetes 帳號\nkubectl create serviceaccount -n token-proxy proxy-user\n\n申請 Google service accout的 IAM 服務帳號，\n然後綁定 kubernetes service account 與 Google service account 。\ngcloud iam service-accounts add-iam-policy-binding \\    --role roles/iam.workloadIdentityUser \\    --member &quot;serviceAccount: project.svc.id.goog[token-proxy/proxy-user]&quot; \\    token-proxy@rd7-project.iam.gserviceaccount.com\n\n在 kubernet service account上面，增加註釋\nkubectl annotate serviceaccount \\    --namespace token-proxy proxy-user \\    iam.gke.io/gcp-service-account=token-proxy@rd7-project.iam.gserviceaccount.com\n\nref.\n使用 Workload Identity",
		"tags": [ "note","⎈"]
},

{
		"title": "GKE記錄 nginx log",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/56. GKE記錄 nginx log/",
		"content": "前言\n因前陣子被人用DDOS攻擊，\n然後公司有一個單位就跑出來了，說他們要nginx的log，\n發生當下才能作為分析用途。\n正文\nnginx 版本 1.21.4\n更改nginx config ，\n將\naccess_log /dev/stdout main;\nerror_log /dev/stderr;\n\n放在 http 底下的 service內。\n這樣就會把log 訊息，使用標準輸出的方式列印到機器上。\nGKE就能抓到記錄了。\n然後使用 ECK的filebeat ，預設是會去抓各個log的資料。\n但前提是要有標準輸出，所以這邊就不用設定了。\n完整config 如下\n\n#user nobody;\nworker_processes 1;\n\nerror_log /var/log/nginx/error.log warn;\npid /var/run/nginx.pid;\n\nevents {\nworker_connections 1024;\n}\n\nhttp {\ninclude /etc/nginx/mime.types;\ndefault_type application/octet-stream;\n\nlog_format main '$remote_addr - $remote_user [$time_local] , '\n'http-host: &quot;$http_host&quot; , URL: &quot;$request&quot; , request-status : &quot;$status&quot; , '\n'body-byte: $body_bytes_sent ,http-referer: &quot;$http_referer&quot; ,'\n'user-agent: &quot;$http_user_agent&quot; , X-Forwarded-For : &quot;$http_x_forwarded_for&quot; , '\n' request-time: &quot;$request_time&quot; , response_time : &quot;$upstream_response_time&quot; ';\n\nsendfile on;\n#tcp_nopush on;\n\nkeepalive_timeout 65;\n\n#gzip on;\n\nserver {\nlisten 80;\nlisten [::]:80;\nroot /app;\nlisten 443 ssl;\nlisten [::]:443 ssl;\n\nserver_name _;\n\nssl off;\nssl_certificate /etc/nginx/nginxssl.crt;\nssl_certificate_key /etc/nginx/nginxssl.key;\n\nssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;\nssl_protocols TLSv1 TLSv1.1 TLSv1.2;\nssl_prefer_server_ciphers on;\n\naccess_log /dev/stdout main;\nerror_log /dev/stderr;\n\nlocation / {\ntry_files $uri /index.php$is_args$query_string;\n}\n\nlocation ~ \\.php$ {\nfastcgi_pass 127.0.0.1:9000;\nfastcgi_index index.php;\nfastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\ninclude fastcgi_params;\n}\n}\n\n}\n\nref.\n\nNginx配置中的log_format\nLinux – Nginx log 格式\ndocker運行nginx為什麼要使用 daemon off\nHave nginx access_log and error_log log to STDOUT and STDERR of master process",
		"tags": ["user", "tcp_nopush", "gzip", "note","⎈"]
},

{
		"title": "GKE pvc還原",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/70. GKE pvc還原/",
		"content": "前言\n之前的Gitlab建在GKE的公開叢集上，\n雖然有限定IP訪問，一直要改到私有叢集上，\n但一直沒時間動，現在終於有空弄了，首先要解決的是資料還原的問題。\n正文\n首先在GCE上面複製一個磁碟，\n可以先從GKE上面的pod找到當初建立的pvc名稱，\n再根據這個名稱去找。\n\n再來就建立pv 跟 pvc了。\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: test-pv\nnamespace: default\nspec:\n# persistentVolumeReclaimPolicy: Delete\nstorageClassName: &quot;standard&quot;\ncapacity:\nstorage: 300G\naccessModes:\n- ReadWriteOnce\nclaimRef:\nnamespace: default\nname: test-upload\ngcePersistentDisk:\npdName: test-upload\nfsType: ext4\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: test-upload\nnamespace: default\nspec:\nstorageClassName: &quot;standard&quot;\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 300G\n\npv的 gcePersistentDisk ，是剛剛複製的硬碟名稱。\n將裡面的參數改一改，\n要建立的 name,namespace,storage, claimRef ，\n結束。\ndeployment使用正常方式直接掛載即可。\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\napp: video-admin\nversion: v1\nspec:\ncontainers:\n- name: video\n...\nvolumeMounts:\n- mountPath: /upload\nname: source\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nterminationGracePeriodSeconds: 30\nvolumes:\n- name: source\npersistentVolumeClaim:\nclaimName: test-upload\n\n如果是statefuleSet的話，\npv的名稱需要改成 statefulSet的命名規則。\nstatefulSet不會用到pvc。\nVC_TEMPLATE_NAME-STATEFULSET_NAME-REPLICA_INDEX\n\nVC_TEMPLATE_NAME：新 PersistentVolumeClaim 模板的名稱。\nSTATEFULSET_NAME：新 StatefulSet 的名稱。\nREPLICA_INDEX：StatefulSet 副本的索引。此示例使用的是 0 和 1。\n\n注意，刪除pvc時，最好將pv也刪除，避免發生未知情況。\n如碰到下圖情況，表示pv有問題，請刪除重建。\n\nref.\n\n將原有永久性磁盤用作 PersistentVolume\n永久性卷和動態預配",
		"tags": [ "note","⎈"]
},

{
		"title": "gitlab on GKE 災害還原筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/GKE/71.gitlab on GKE 災害還原筆記/",
		"content": "前言\n之前一直沒寫的gitlab yaml，再文章的最下方。\n本片文章都是使用這個yaml做建立及佈署。\n正文\nStep 0.\n之前一直沒寫的gitlab yaml，再文章的最下方。\n本片文章都是使用這個yaml做建立及佈署。\nStep 1.\n先參考上篇 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/70. GKE pvc還原/\">70. GKE pvc還原</a>，建立好PVC。\n主要複製Gitlab的位置為\n- name: config\nmountPath: /etc/gitlab\n- name: gitlab-data\nmountPath: /var/opt/gitlab/git-data\n- name: postgres-data\nmountPath: /var/opt/gitlab/postgresql\n\n由於我之前建立的image為 last版本（應該是快一年前的事），\n這次我用last版本的話，版本超前太多，導致一直重開機。\n後來查我的gitlab版本為 14.0.1，\n但裝上去後一直碰到這個錯誤\n\nrelation &quot;services&quot; does not exist after upgrade to 14.2.0\n\n解決方式：\n後來將版本升級到 14.1.1 後就好了。\nref.\n\nERROR: relation &quot;services&quot; does not exist after upgrade to 14.2.0\n\nStep 2.\n用本來的帳號密碼能登入，沒問題。\n增加新的git remote 位置，\n然後試著推一個版本上去時，又出現錯誤了。這次比較好理解。\n\nremote: HTTP Basic: Access denied\n\n解決方式：\n跑去另一個空目錄，git clone ，會要輸入帳號密碼，輸入後就好了。\nStep 3.\n如果看過之前的文章，可能知道我有在用istio，\n會碰到要用一個ip暴露不同的服務。\n解決方式：\nexternal_url &quot;http://{ip}/gitlab&quot;\nvirtualservice直接改\nspec:\nhosts: []\nhttp:\n- match:\n- uri:\nprefix: /gitlab\nname: gitlab\nroute:\n- destination:\nhost: gitlab-svc.default.svc.cluster.local\nport:\nnumber: 80\n\nref.\n\nConfigure a relative URL for GitLab\n\nGitlab yaml\n現在也有gitlab operator跟 helm 可以做，有興趣的可以參考一下官方文件。\napiVersion: v1\nkind: Service\nmetadata:\nname: gitlab-svc\nspec:\nports:\n- name: &quot;web&quot;\nport: 80\ntargetPort: 80\n- name: &quot;ssh&quot;\nport: 22\ntargetPort: 22\n- name: &quot;ssl&quot;\nport: 443\ntargetPort: 443\nselector:\nio.kompose.service: web\ndrone: gitlab\ntype: LoadBalancer\nloadBalancerIP: 123.123.123.123\nloadBalancerSourceRanges:\n- 192.168.0.0/16\n- 172.16.0.0/12\n- 10.0.0.0/8\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: gitlab\nlabels:\nio.kompose.service: web\nspec:\nreplicas: 1\nselector:\nmatchLabels:\nio.kompose.service: web\ntemplate:\nmetadata:\nlabels:\nio.kompose.service: web\nspec:\ncontainers:\n- env:\n- name: GITLAB_TIMEZONE\nvalue: Taipei\n- name: GITLAB_OMNIBUS_CONFIG\nvalue: |\nexternal_url &quot;http://123.123.123.123/gitlab&quot;\ngitlab_rails['gitlab_default_projects_features_builds'] = false\nimage: gitlab/gitlab-ce:14.1.1-ce.0\nname: web\nports:\n- containerPort: 80\nresources: {}\nvolumeMounts:\n- name: config\nmountPath: /etc/gitlab\n- name: gitlab-data\nmountPath: /var/opt/gitlab/git-data\n- name: postgres-data\nmountPath: /var/opt/gitlab/postgresql\nrestartPolicy: Always\nserviceAccountName: &quot;&quot;\nvolumes:\n- name: config\npersistentVolumeClaim:\nclaimName: gke-drone-web-claim0\n- name: gitlab-data\npersistentVolumeClaim:\nclaimName: gke-drone-repository-data\n- name: postgres-data\npersistentVolumeClaim:\nclaimName: gke-drone-gitlab-postgresql\n\nPVC的建立，請參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/70. GKE pvc還原/\">70. GKE pvc還原</a>，如要建立空的，\n參考下方yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\ncreationTimestamp: null\nlabels:\nio.kompose.service: web-claim0\nname: gke-drone-web-claim0\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 20Gi\nstatus: {}\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nlabels:\ngitlab-data: gitlab\nname: gke-drone-repository-data\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 100Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nlabels:\ngitlab-data: postgresql\nname: gke-drone-gitlab-postgresql\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 100Gi",
		"tags": [ "note","⎈"]
},

{
		"title": "用chatGPT學Terraform",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/147. 用chatGPT學Terraform/",
		"content": "睡睡念\n終於要來試試看terraorm了，\n現在公司好像很少全部k8s，\n大部分還是用虛擬機，這樣變成還是要把terraform學一下。\n正文\nTerraform跟ansible的差異\n\n差異點\nTerraform\nAnsible\n\n組態檔案管理 vs 組態\nTerraform使用組態檔案管理基礎設施，描述其狀態，而不是配置。\nAnsible基於主機的管理，直接配置基礎設施。\n\n可變基礎設施 vs 不可變基礎設施\nTerraform以可變的方式管理基礎設施，可以對基礎設施進行更改。\nAnsible通常將基礎設施視為不可變的，一旦配置，它們就不再更改。\n\n過程式的 vs 聲明式的\nTerraform是過程式的，會循序漸進地執行創建、修改和刪除操作。\nAnsible是聲明式的，描述了期望的最終狀態，而不是執行一系列步驟來實現它。\n\nMaster vs Masterless\nTerraform通常使用單一主節點管理基礎設施狀態。\nAnsible可以在主節點和無主節點（即agentless）模式下運行。\n\nAgent vs Agentless\nTerraform不需要在目標基礎設施上安裝任何代理。\nAnsible需要在目標基礎設施上安裝一個代理。\n\n成熟 vs 前沿\nTerraform是成熟的基礎設施自動化工具，已被廣泛使用。\nAnsible是一個新興的基礎設施自動化工具，並且正在積極發展。\n\n綜合使用多種工具\nTerraform與其他工具集成，如Packer、Docker和AWS CloudFormation等。\nAnsible與其他工具集成，如Jenkins、Docker和Kubernetes等。\n\n安裝\n官網安裝最快了，上面有各個平臺的安裝方式。\n詠唱\n我先問了一下chatGPT怎麼學Terraform，然後給了我很標準的答案。\n\n閱讀Terraform官方文檔，熟悉其基礎結構、功能和概念。\n安裝Terraform，並學習如何使用它來創建基礎設施。\n開始使用Terraform的示例項目，並嘗試使用它們創建簡單的基礎設施。\n學習如何使用Terraform管理不同的基礎設施提供者，如AWS、Azure、Google Cloud等。\n閱讀Terraform的模塊化文檔，並學習如何使用模塊化設計來管理複雜的基礎設施。\n學習如何使用Terraform命令列工具來測試和調試基礎設施代碼。\n研究Terraform的最佳實踐，包括如何管理機密、編寫可重用的代碼和設計可擴展的基礎設施。\n\n我還是直接從實際案例來看好了，\nQ: 幫我建立一個部署deployment的terrform程式碼\nA:\n# 定義Kubernetes provider\nprovider &quot;kubernetes&quot; {\nconfig_context_cluster = &quot;my-cluster&quot;\n}\n\n# 定義deployment\nresource &quot;kubernetes_deployment&quot; &quot;my_deployment&quot; {\nmetadata {\nname = &quot;my-deployment&quot;\nlabels = {\napp = &quot;my-app&quot;\n}\n}\n\nspec {\nreplicas = 3\n\nselector {\nmatch_labels = {\napp = &quot;my-app&quot;\n}\n}\n\ntemplate {\nmetadata {\nlabels = {\napp = &quot;my-app&quot;\n}\n}\n\nspec {\ncontainer {\nimage = &quot;my-image:latest&quot;\nname = &quot;my-container&quot;\n}\n}\n}\n}\n}\n\n看了一下他提供的程式碼，有點問題，然後接着問\nQ：resouce 宣告 kubernetes_deployment my_deployment 是要跟deployment的name一樣嗎\nA:\n\n是的，kubernetes_deployment的metadata區塊中的name屬性會定義Kubernetes deployment的名稱，您需要將其設定為與實際的deployment名稱相同。\n\n然後在後續的對談過程中，發現另一個指令。\nterraform import\n\n再緊接着問，這個的功用是什麼。\nA:\nterraform import是Terraform提供的一個命令，用於將現有的資源引入Terraform管理的狀態中。使用terraform import可以讓Terraform瞭解現有資源的狀態，並允許你使用Terraform管理這些資源，並在需要時進行更改。\n通常情況下，Terraform會從頭開始創建和管理所有資源。然而，在實際環境中，有時候會存在一些已經存在的資源，這些資源可能是手動創建的，或者是由其他工具創建的。這些資源可能包括Kubernetes deployment、AWS EC2實例、GCP虛擬機等等。在這種情況下，使用terraform import命令可以將這些現有資源引入到Terraform管理的狀態中，以便Terraform能夠管理它們。\n使用terraform import命令時，需要指定要引入的資源的類型和ID，以及要對應到Terraform的哪個資源。Terraform將使用提供的資源類型和ID尋找現有資源的狀態，並將其對應到Terraform組態檔案中的相應資源。一旦資源成功被引入，Terraform就可以開始管理它們，並將它們的狀態保存到狀態檔案中，這樣就可以使用terraform plan、terraform apply等命令進行更改。\n如果不知道怎麼用沒關係，\n再問一下，\nQ：該怎麼使用terraform import\nA：\n使用terraform import命令引入現有資源到Terraform管理的狀態中，通常需要以下步驟：\n\n在Terraform組態檔案中定義資源的組態，例如使用kubernetes_deployment定義Kubernetes deployment的組態。\n執行terraform init初始化Terraform，以設置Terraform的工作目錄和相關外掛。\n執行terraform import命令引入現有資源。例如，要引入Kubernetes deployment，可以使用以下命令：\n\nterraform import kubernetes_deployment.systemservice deployment2k7\n\n這裡的kubernetes_deployment.systemservice指的是在Terraform組態檔案中定義的Kubernetes deployment資源，deployment2k7則是現有Kubernetes deployment的名稱或ID。通過這樣的方式，Terraform就會將現有的Kubernetes deployment引入到Terraform管理的狀態中。\n\n檢查Terraform的狀態，使用terraform state list命令可以列出Terraform目前管理的所有資源，可以確認引入的資源是否已經被成功管理。\n更新Terraform組態檔案，通過terraform plan和terraform apply命令對資源進行更改。當Terraform管理現有資源時，它會比較Terraform組態檔案和現有資源的狀態，並生成一個計劃，顯示要將資源帶到Terraform所需的更改。通過執行terraform apply命令，Terraform就可以將這些更改應用到現有的資源中。\n\n結論\n光看沒有用，最重要的還是實作，\n一開始沒碰terraform是不知道寫壞了，會不會影響線上的服務，\n現在我能夠先import，看目前服務建設的程式後，再來修改。\n感想\n實作後完全崩掉，有很多東西還是要從頭開始看一下。\n像是， terraform 的backend ，跟tfstate 這些觀念。\n上面也有提到引入資源，但實際上可能跟你想得不太一樣，\n引入後是變成一個tfstate，而不是tf，\n那tf 跟 tfstate又有什麼不同呢？\n我們還是看下一篇，從頭來過吧。\nref.\n\n為什麼我們使用Terraform 而不使用Puppet, Ansible",
		"tags": [ "note","⎈"]
},

{
		"title": "Terraform的基本概念",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/148. Terraform的基本概念/",
		"content": "睡睡念\n經歷上一篇的失敗，\n還是先從基本觀念開始吧。\n正文\ntf基本組成\n下面指令輸入時，會用tf取代terraform\n宣告\n\nterraform\n宣告terraform以及環境的版本\n\nterraform {\nrequired_version = &quot;~&gt;0.13.5&quot;\nrequired_providers {\nucloud = {\nsource = &quot;ucloud/ucloud&quot;\nversion = &quot;~&gt;1.22.0&quot;\n}\n}\n}\n\nprovider\n呼叫該環境api時需要的參數\nk8s\n\nprovider &quot;kubernetes&quot; {\nconfig_path = &quot;~/.kube/config&quot;\n}\n\nucloud\nprovider &quot;ucloud&quot; {\npublic_key = &quot;JInqRnkSY8eAmxKFRxW9kVANYThg1pcvjD2Aw5f5p&quot;\nprivate_key = &quot;IlJn6GlmanYI1iDVEtrPyt5R9noAGz41B8q5TML7abqD8e4YjVdylwaKWdY61J5TcA&quot;\nproject_id = &quot;org-tgqbvi&quot;\nregion = &quot;cn-bj2&quot;\n}\n\n定義\n\ndata\n取得terraform以外的資訊，不同的proivder提供不同的data sources。\nprovider 列表\n\ndata &quot;ucloud_security_groups&quot; &quot;default&quot; {\ntype = &quot;recommend_web&quot;\n}\n\nresource\n要建立的資源\n\n\tresource &quot;ucloud_eip&quot; &quot;web-eip&quot; {\n\t internet_type = &quot;bgp&quot;\n\t charge_mode = &quot;bandwidth&quot;\n\t charge_type = &quot;dynamic&quot;\n\t name = &quot;web-eip&quot;\n\t}\n\noutput\n輸出\n\n\t\toutput &quot;eip&quot; {\n\t\t value = ucloud_eip.web-eip.public_ip\n\t\t}\n\n狀態檔案(tfstate)\n當terrform要執行變更時，會產生tfstate的檔案，\n上面記載了目前雲上的資源狀態，此檔案不建議編輯，\n當有任何資源變更時，會同步判斷目前雲上的資源狀態是否一致。\n可使用以下指令，匯入目前雲端上的資源狀態\nterraform import kubernetes_deployment.websocketclient default/websocketclient\n\nkubernetes_deployment.websocketclient 為tf的資源名稱\ndefault/websocketclient 是雲上的資源\n此tfstate的檔案是明碼，且多人使用會造成簽入時發生衝突，\n所以要透過backend來解決這個問題\n檔案裏面會看到一些狀態，代表的意思如下\n\n+ 代表新增的資源 (Resource added)\n- 代表刪除的資源 (Resource deleted)\n~ 代表修改的資源 (Resource modified)\n= 代表資源參數的值沒有改變，只是調整了排序 (Resource parameter unchanged, only ordering changed)\n-/+ 同時出現，表示毀壞性變更，先刪除後建立\n\nref. 使用基礎架構即程式碼工具 Terraform\n\n遠端狀態儲存機制(Backend)\nTerraform Remote Backend分為兩種：\n\n標準：支援遠端狀態儲存與狀態鎖\n\n狀態鎖是指，當針對一個tfstate進行變更操作時，可以針對該狀態檔案新增一把全域鎖，確保同一時間只能有一個變更被執行。\n\n增強：在標準的基礎上支援遠端操作(在遠端伺服器上執行plan、apply等操作)\n\n只存在於Terraform Cloud\n\n目前支援的backend list\n\n要共用 backend的話，\n先建立一個 backend.hcl\n裏面輸入\nbucket = &quot;98c2c8-terraform-backend&quot;\nprefix = &quot;terraform/state&quot;\n\n上面的prefix代表的意思為\n\nGCS prefix inside the bucket. Named states for workspaces are stored in an object called &lt;prefix&gt;/&lt;name&gt;.tfstate\n\n在main.tf的required_providers後面填上 backend &quot;gcs&quot; { }，\n格式如下\nterraform {\nrequired_providers {\ngoogle = {\nsource = &quot;hashicorp/google&quot;\nversion = &quot;4.63.1&quot;\n}\n}\nbackend &quot;gcs&quot; {\n\n}\n}\n\n然後在初始化的時候，輸入\nterraform init -backend-config=backend.hcl\n\nref.\n\nBackend組態的動態賦值\ntf-gcs\n\n建立workspace\n相比起多資料夾隔離的方式來說，基於Workspace的隔離更加簡單，只需要保存一份程式碼，在程式碼中不需要為Workspace編寫額外程式碼，用命令列就可以在不同工作區之間來回切換。\n下列指令分別爲，建立、列表、選擇及顯示目前的workspace\nterraform workspace new qa\nterraform workspace list\nterraform workspace select qa\nterraform worspace show\n\nref. 狀態管理\n\n檢查terraform 的計劃\nterraform plan -out=out.tfplan\n\n-out的指定計畫的輸出檔，確保您所檢閱的方案就是所套用的方案。\n之後使用apply執行\nterraform apply out.tfplan\n\n建立完成後，使用show看目前狀態\nterraform show\n\ncount\n\n舉例來說, 兩個VM是一隻狗一隻貓，那就是dog.tf cat.tf\n如果是兩隻狗 就可以寫一個 dog.tf\n裡面用\nresources ec2\nCount = 2\n這樣\n\nconsole\n執行 console ，看程式執行結果\nterraform console\n\ncidrsubnet 後面的第二個參數表示二進位要位移多少，第三個參數表示，要設定位移的數字為多少。\n例如 10 = 1010\n位移4bit，所以是 1010 0000 = 160\n6 = 0110\n位移4bit ，所以是 0110 0000 = 96\ntf console\n&gt; cidrsubnet(&quot;10.1.0.0/24&quot;,4,0)\n&quot;10.1.0.0/28&quot;\n&gt; cidrsubnet(&quot;10.1.0.0/24&quot;,4,10)\n&quot;10.1.0.160/28&quot;\n&gt; cidrsubnet(&quot;10.1.0.0/24&quot;,4,6)\n&quot;10.1.0.96/28&quot;\n&gt; cidrsubnet(&quot;10.1.0.0/24&quot;,4,12)\n&quot;10.1.0.192/28&quot;\n\nref. How cidrsubnet works in Terraform\n\nvariable變數宣告\n先知道變數的繼承順序，\n\n種類\n描述\n範例\n優先\n建議使用\n\n命令列參數\n輸入命令時，指定變數\ntf apply -var=&quot;gcp_region=asia-east&quot;\n1\n特殊情況\n\n環境變數\n使用 TF_VAR_ 環境變數定義\nTF_VAR_gcp_region=asia_east2\n2\n\n變數文件\n使用tfvars變數文件定義\ngcp_region=asia_east2\n3\n開發者\n\n設定文件\n使用variables區塊定義\nvariable &quot;gcp_region&quot; {default = &quot;asia-east1&quot;}\n4\n程式碼\n\ntf的變數要先宣告，\nvariable &quot;gcp_region&quot; {\ndefault = &quot;asia-east1&quot;\n}\n\n如果後續要改值，可以改成\ngcp_region = &quot;asia-east2&quot;\n\n如果要測試，可以執行 terraform console\n再輸入變數名稱驗證。\n變數文件載入方式\n\n檔案名稱為 terraform.tfvars or terraform.tfvars.json\n副檔名為 .auto.tfvars or .auto.tfvars\n\n必須在同一個tf init的資料夾內。\n\n還有其他載入方式，在apply 時載入變數檔，或是環境變數載入。\n詳細參考\n\n[Terraform] Input Variables\nTerraform 變數\n(Youtube)Terraform 從零開始 基礎 | 9-Variables（變數）的種類與使用順序\n\n常用指令\n\ntf console 可以經由這邊去驗證變數\n\ntf fmt 格式化main.tf\n\ntf validate 驗證terraform",
		"tags": [ "note","⎈"]
},

{
		"title": "terraform 部署VM，啓動docker安裝metadata_startup_script",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/149. terraform 部署VM，啓動docker安裝metadata_startup_script/",
		"content": "睡睡念\n由於前人弄向Let's encrypt 申請的ssl憑證，偶爾會出問題，\n步驟繁瑣，常常改了這個，忘記要重啓那個，那就重新做一套吧。\n為了我想偷懶..(^__^)\n但在那之前，先生個機器來測試看看。\n正文\n第一次用terraform部署vm機器，\n這邊要注意的是 backend &quot;gcs&quot;，\n我直接將tfstate傳到gcs上面了，可以參考上一篇<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/148. Terraform的基本概念/\">148. Terraform的基本概念</a>。\n底下的metadata_startup_script，\n指令很長，導致我不想放在同一行，\n跑去問了下chatGPT，能使用\n&lt;&lt;-EOT\nEOT\n\n這種方式將程式碼包在一起，就比較方便閱讀了\nterraform {\nrequired_providers {\ngoogle = {\nsource = &quot;hashicorp/google&quot;\nversion = &quot;4.63.1&quot;\n}\n}\nbackend &quot;gcs&quot; {\n\n}\n}\n\nprovider &quot;google&quot; {\nproject = &quot;abc&quot;\n}\n\nresource &quot;google_compute_instance&quot; &quot;certificate-vm&quot; {\nname = &quot;ca-service&quot;\nmachine_type = &quot;f1-micro&quot;\nzone = &quot;asia-east1-b&quot;\ntags = [&quot;allow-admin-ip&quot;]\n\nboot_disk {\ninitialize_params {\nimage = &quot;debian-cloud/debian-11&quot;\n}\n}\n\n# Install Docker\nmetadata_startup_script = &lt;&lt;-EOT\nsudo apt-get update; sudo apt-get install ca-certificates curl gnupg;\nsudo install -m 0755 -d /etc/apt/keyrings;\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg;\nsudo chmod a+r /etc/apt/keyrings/docker.gpg;\necho &quot;deb [arch=&quot;$(dpkg --print-architecture)&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n&quot;$(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;)&quot; stable&quot; | \\\nsudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null ;\nsudo apt-get update;\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin;\nEOT\n\nnetwork_interface {\n# subnetwork = google_compute_subnetwork.default.id\nsubnetwork = &quot;default&quot;\naccess_config {\n# Include this section to give the VM an external IP address\n}\n}\n}\n\ntroubleshooting\n如果執行了startup-script，但沒跑，該怎麼除錯？\n到vm裏面執行下面指令，就可以查log了\nsudo journalctl -u google-startup-scripts.service\n\nref. Install Docker Engine on Debian",
		"tags": [ "note","⎈"]
},

{
		"title": "Terrafrom 建立private cluster",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/183. Terrafrom 建立private cluster/",
		"content": "前言\n要重建一個新的project，然後把舊的服務搬過去。\n初步估計就一堆東西，正好拿來練一下terraform了\n正文\n\n服務列表\n\nCreate Project\n\nCreate Cluster\n\nCreate VPC\n\nCreate CLoudSQL\n\nCreate Memorystore\n\nCreate CloudStorage\n\nCopy file to CloudStorage\n\n後端修改config&amp;publish\n\nCreaet RabbitMQ\n\nCreate Nginx ingress\n\n前端服務部署&amp;測試\n\n後端服務部署&amp;測試\n\nNetwork policy\n\nObserbility service\n\nEFK logs\n\n前後端 CI/CD 修改 &amp; IAM權限\n\nProject沒辦法用terraform，因爲那不是我操作的。\n也不知道那個能不能用terraform 。\n這篇會建立一個bucket、vpc 、private cluster以及cloud NAT。\n下面會用tf 來稱呼terraform\n\n建立bucket\n因爲狀態關係，就乾脆用gcs統一狀態了，可參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/148. Terraform的基本概念/\">148. Terraform的基本概念</a>\nnew\\gcs\\main.tf\n\nterraform {\nrequired_providers {\ngoogle = {\nsource = &quot;hashicorp/google&quot;\nversion = &quot;4.63.1&quot;\n}\n}\n}\n\nprovider &quot;google&quot; {\nproject = &quot;ezio-sms-o&quot;\n}\n\nresource &quot;random_id&quot; &quot;bucket_prefix&quot; {\nbyte_length = 3\n}\n\nresource &quot;google_storage_bucket&quot; &quot;tfstate_bucket&quot; {\nname = &quot;${random_id.bucket_prefix.hex}-terraform-devops&quot;\nforce_destroy = false\nlocation = &quot;asia-east1&quot;\nstorage_class = &quot;Nearline&quot;\n\n//物件版本管理\nversioning {\nenabled = false\n}\n}\n\noutput &quot;bucket_name&quot;{\ndescription = &quot;get bucket name&quot;\nvalue = google_storage_bucket.tfstate_bucket.name\n}\n\n記下bucket name與預計要用的前綴，然後寫在backend.hcl，\n等等下面會用到。\nnew\\backend.hcl\nbucket = &quot;cb0392-terraform-devops&quot;\nprefix = &quot;terraform/state&quot;\n\n建vpc 與cloudNAT\nnew\\gke\\main.tf\n\nterraform {\nrequired_providers {\ngoogle = {\nsource = &quot;hashicorp/google&quot;\nversion = &quot;5.4.0&quot;\n}\n}\nbackend &quot;gcs&quot; {\n\n}\n}\n\nprovider &quot;google&quot; {\nproject = &quot;ezio-sms-o&quot;\nzone = &quot;asia-east1-b&quot;\n}\n\n# VPC\nresource &quot;google_compute_network&quot; &quot;basic&quot; {\nname = &quot;basic&quot;\nauto_create_subnetworks = false\n# delete_default_routes_on_create = false\nrouting_mode = &quot;REGIONAL&quot;\n}\n\nresource &quot;google_compute_subnetwork&quot; &quot;basic&quot; {\nname = &quot;basic-subnet&quot;\nip_cidr_range = var.subnet_cidr\nregion = var.gcp_region\nnetwork = google_compute_network.basic.id\nprivate_ip_google_access = true\n\nsecondary_ip_range {\nrange_name = &quot;gke-pod-range&quot;\nip_cidr_range = var.pod_cidr\n}\nsecondary_ip_range {\nrange_name = &quot;gke-service-range&quot;\nip_cidr_range = var.service_cidr\n}\n}\n\n# Cloud NAT\nresource &quot;google_compute_router&quot; &quot;default_cloudnat&quot; {\nname = &quot;default-nat&quot;\nregion = var.gcp_region\nnetwork = google_compute_network.basic.id\n}\n\nresource &quot;google_compute_router_nat&quot; &quot;default_cloudnat&quot; {\nname = &quot;nat&quot;\nrouter = google_compute_router.default_cloudnat.name\nregion = var.gcp_region\n\nsource_subnetwork_ip_ranges_to_nat = &quot;LIST_OF_SUBNETWORKS&quot;\nnat_ip_allocate_option = &quot;MANUAL_ONLY&quot;\n\nsubnetwork {\nname = google_compute_subnetwork.basic.id\nsource_ip_ranges_to_nat = [&quot;ALL_IP_RANGES&quot;]\n}\n\nnat_ips = [google_compute_address.nat.self_link]\n}\n\nresource &quot;google_compute_address&quot; &quot;nat&quot; {\nname = &quot;nat&quot;\naddress_type = &quot;EXTERNAL&quot;\nnetwork_tier = &quot;PREMIUM&quot;\nregion = var.gcp_region\n\n}\n\n變數的設定，也請參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/148. Terraform的基本概念/\">148. Terraform的基本概念</a>\nnew\\gke\\terraform.tf\nvariable &quot;project_id&quot; {\ndefault = &quot;ezio-sms-o&quot;\n}\nvariable &quot;gcp_region&quot; {\ndefault = &quot;asia-east1&quot;\n}\nvariable &quot;gcp_zone&quot; {\ntype = list(string)\ndefault = [&quot;asia-east1-b&quot;]\n}\nvariable &quot;gke_name&quot; {\ndefault = &quot;fixed&quot;\n}\n\nvariable &quot;subnet_cidr&quot; {\ndefault = &quot;10.10.0.0/16&quot;\n}\n\nvariable &quot;pod_cidr&quot; {\ndefault = &quot;192.168.8.0/21&quot;\n}\n\nvariable &quot;service_cidr&quot; {\ndefault = &quot;192.168.16.0/21&quot;\n}\n\nvariable &quot;master_cidr&quot; {\ndefault = &quot;10.0.2.0/28&quot;\n}\n\nref.\n\n透過 Terraform 建立私有 GKE Cluster\n# 使用基礎架構即程式碼工具 Terraform\ngoogle_compute_network\n\n建立GKE\n\n這部分算是查最久的，用到了terraform的module概念。\n可以先看這篇 [Terraform] 入門學習筆記，瞭解一下什麼是module。\n然後再來就是看官方文件了。\nnew\\gke\\main.tf\n沒錯跟上面vpc的檔案一樣，因爲是同一份文件。\n# GKE\nmodule &quot;gke&quot; {\nsource = &quot;terraform-google-modules/kubernetes-engine/google//modules/private-cluster&quot;\nversion = &quot;29.0.0&quot;\nproject_id = var.project_id\nname = var.gke_name\n# 區域性\nregional = false\nregion = var.gcp_region\nzones = var.gcp_zone\nnetwork = google_compute_network.basic.name\nsubnetwork = google_compute_subnetwork.basic.name\nip_range_pods = google_compute_subnetwork.basic.secondary_ip_range[0].range_name\nip_range_services = google_compute_subnetwork.basic.secondary_ip_range[1].range_name\nhttp_load_balancing = true\nnetwork_policy = false\nhorizontal_pod_autoscaling = true\nfilestore_csi_driver = false\n# private cluster\nenable_private_endpoint = false\nenable_private_nodes = true\nmaster_global_access_enabled = false\nmaster_ipv4_cidr_block = &quot;10.0.0.0/28&quot;\n# gke version\nrelease_channel = &quot;UNSPECIFIED&quot;\nremove_default_node_pool = true\n# log record\nlogging_enabled_components = [&quot;SYSTEM_COMPONENTS&quot;]\ncreate_service_account = false\n\nnode_pools = [\n{\nautoscaling = false\nnode_count = 2\nname = &quot;default-node-pool&quot;\nmachine_type = &quot;e2-custom-4-12288&quot;\nlocal_ssd_count = 0\nspot = false\ndisk_size_gb = 100\ndisk_type = &quot;pd-standard&quot;\nimage_type = &quot;COS_CONTAINERD&quot;\nenable_gcfs = false\nenable_gvnic = false\nlogging_variant = &quot;DEFAULT&quot;\nauto_repair = true\nauto_upgrade = false\npreemptible = false\ninitial_node_count = 80\n},\n]\n\nnode_pools_oauth_scopes = {\nall = [\n\t\t&quot;https://www.googleapis.com/auth/logging.write&quot;,\n\t\t&quot;https://www.googleapis.com/auth/monitoring&quot;,\n\t\t&quot;https://www.googleapis.com/auth/devstorage.read_only&quot;,\n\t\t&quot;https://www.googleapis.com/auth/service.management.readonly&quot;,\n\t\t&quot;https://www.googleapis.com/auth/servicecontrol&quot;,\n\t\t&quot;https://www.googleapis.com/auth/trace.append&quot;\n]\n}\n\nnode_pools_labels = {\nall = {}\n\ndefault-node-pool = {\ndefault-node-pool = true\n}\n}\n\nnode_pools_metadata = {\nall = {}\n\ndefault-node-pool = {\nnode-pool-metadata-custom-value = &quot;fixed-node-pool&quot;\n}\n}\n\nnode_pools_taints = {\nall = []\n\ndefault-node-pool = [\n{\nkey = &quot;default-node-pool&quot;\nvalue = true\neffect = &quot;PREFER_NO_SCHEDULE&quot;\n},\n]\n}\n\nnode_pools_tags = {\nall = []\n\ndefault-node-pool = [\n&quot;default-node-pool&quot;,\n]\n}\n}\n\n簡單說一下module的用法，\n當第一次用module時，需要初始化，加上指定gcs儲存tfstat\ntf init -backend-config=../backend.hcl\n\n之後要使用submodule的話，不用再初始化，只要get\ntf get\n\n要建立private cluster的話，需要使用submodule，\n可從kubernetes-engine 到 submodule的 private-cluster，\n有些參數不知道是建立GKE的哪個設定，\n可以多利用tf validate 驗證，\n但有時是部署時才會發生錯誤。\n我後來是直接手動把cluster砍掉，再tf apply比較快。\n本來也有想過不要用module，直接實作。\n但看了官方文件 Using GKE with Terraform，\n\nAdditionally, you may consider using Google's kubernetes-engine module, which implements many of these practices for you\n就試試看了。\n\n注意，submodule與主要module的source不一樣，有些參數在主module是沒有的。\nTroubleshooting\n部署GKE時，發現我沒辦法從Google Artifact Registry 下載image。\n最後發現是 沒有設定 存取權範圍，Access scopes in GKE。\n\n這個需要在 node_pools_oauth_scopes 裏面補上允許的api網址。",
		"tags": [ "note","⎈"]
},

{
		"title": "導入Terraform",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/189. 導入Terraform/",
		"content": "前言\n前面有稍微提到幾篇是terraform的使用方式，\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/148. Terraform的基本概念/\">148. Terraform的基本概念</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/149. terraform 部署VM，啓動docker安裝metadata_startup_script/\">149. terraform 部署VM，啓動docker安裝metadata_startup_script</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/Terraform/183. Terrafrom 建立private cluster/\">183. Terrafrom 建立private cluster</a>\n再來把剩下碰到的補一補了。\n正文\n\n建立完了private cluster，後續要建立SQL 跟 redis的話，該怎麼建？\n\n另外開一個資料夾，重新terraform init，但要注意，\n你的backend.hcl是怎麼寫的。\n舉例來說，\nprivate cluster的bucket.hcl是這樣\nbucket = &quot;terraform-devops&quot;\nprefix = &quot;terraform/state&quot;\n\n如果，另一個資料夾裏面的bucket.hcl也相同，\n那在產生計劃時，就會把你剛剛建好的private cluster給刪除，\n因爲他檢查的是tfstate跟你目前計劃有哪些差別。\n所以，需要更改prefix\nbucket = &quot;terraform-devops&quot;\nprefix = &quot;terraform/other&quot;\n\n這樣在bucket裏面，就會存成兩個不同的tfstate\n\nterrafrom使用迴圈，減少重複的程式碼\n\n建立兩個bucket，並開啓allUser read\nvariable &quot;bucket_names&quot; {\ntype = list(string)\ndefault = [&quot;A-legacy&quot;, &quot;B-legacy&quot;]\n}\nresource &quot;google_storage_bucket&quot; &quot;frontend_bucket&quot; {\nuniform_bucket_level_access = true\nfor_each = { for name in toset(var.bucket_names) : name =&gt; name }\nname = each.value\nforce_destroy = false\nlocation = &quot;asia-east1&quot;\nstorage_class = &quot;STANDARD&quot;\n# public_access_prevention = &quot;inherited&quot;\n\n//物件版本管理\nversioning {\nenabled = false\n}\n}\nresource &quot;google_storage_bucket_iam_binding&quot; &quot;binding&quot; {\nfor_each = google_storage_bucket.frontend_bucket\n\nbucket = each.value.name\nrole = &quot;roles/storage.legacyObjectReader&quot;\nmembers = [\n&quot;allUsers&quot;,\n]\n}\n\nref.\n\nThe for_each Meta-Argument\n禁止公開訪問\nIAM policy for Cloud Storage Bucket\n\n讀取之前的設定\n\n在建立private cluster的時候，就已經建好了vpc ，\n相同的網路，我想在建立mssql 時使用，\n使用data呼叫這個VPC。\n這個範例是拿來建立vpc裏面的私人服務連線的位置。\ndata &quot;google_compute_network&quot; &quot;private-network&quot; {\nname = &quot;basic&quot;\n}\n\nresource &quot;google_compute_global_address&quot; &quot;private_ip_address&quot; {\ndepends_on = [ google_project_service.project ]\nname = &quot;private-ip-address&quot;\npurpose = &quot;VPC_PEERING&quot;\naddress_type = &quot;INTERNAL&quot;\nprefix_length = 16\nnetwork = data.google_compute_network.private-network.id\n}\nresource &quot;google_service_networking_connection&quot; &quot;private_vpc_connection&quot; {\n\nnetwork = data.google_compute_network.private-network.id\nservice = &quot;servicenetworking.googleapis.com&quot;\nreserved_peering_ranges = [google_compute_global_address.private_ip_address.name]\n}\n\n設定.gitignore\n\n# Local .terraform directories\n**/.terraform/*\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\ncrash.*.log\n\n# Exclude all .tfvars files, which are likely to contain sensitive data, such as\n# password, private keys, and other secrets. These should not be part of version\n# control as they are data points which are potentially sensitive and subject\n# to change depending on the environment.\n# *.tfvars\n# *.tfvars.json\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n# Ignore CLI configuration files\n.terraformrc\nterraform.rc\n\nref. gitignore\n結尾\n基礎概念介紹\n\nTerraform基礎概念——Provider\nGCP-使用 Terraform 的最佳實踐\n大家都在用 Terraform 實作 IaC 為什麼不將程式寫得更簡潔易讀呢？\nLocals",
		"tags": [ "note","⎈"]
},

{
		"title": "Terraform vs Ansible",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Terraform/82. Terraform vs Ansible/",
		"content": "前言\n本來想直接用表格闡述兩邊不同的地方，\n但發現我不知道從何下手。\n只好先用條列代替了，\n這兩套我都沒用過，因為...我直接從GKE開始XD。\n正文\nTerraform\n\n聲明性代碼：程式碼的結果就是最終狀態\n協作(Orchestration)：確保環境持續處於『理想狀態』，當服務不能用時，會砍掉重建。\n\nAnsible\n\n程式性代碼：每一次的狀態都是獨立的\n配置管理工具(Configuration Management)： 是一套流程和程序，執行時不會重置系統，會在機器上修復問題或升級服務。\n\n結論\n目前看起來，\nAnsible主要偏向自動化管理工具但同時具備一定程度上的環境　建立。\nTerraform 就是單純的 環境建立工具\n如果走向k8s服務的話，學TerraForm就好了。\n用來建立cluster、loadbalancer...。\n由於 k8s的服務都已經容器化了，使用Ansible也沒什麼意義，\n重開服務又消失了。\nref.\n\nTerraform vs Ansible: What's the difference and which one you should use?\nAnsible vs Terraform vs Puppet差異比較：有什麼區別？選擇哪個？\n架構程式碼的實施工具\n[DevOps系列] 為什麼我們使用Terraform 而不使用Puppet, Ansible\nTerraform vs. Ansible : Key Differences and Comparison of Tools，感覺這篇寫的最好，但結論就...",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s YAML 小細節",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/12. k8s YAML 小細節/",
		"content": "前言\n最近寫yaml，有時常會忘記這個的功能是什麼，就記一下吧\n正文\n\nService 的Port功能\nPort exposes the Kubernetes service on the specified port within the cluster. Other pods within the cluster can communicate with this server on the specified port.\n\nTargetPort is the port on which the service will send requests to, that your pod will be listening on. Your application in the container will need to be listening on this port also.\nNodePort exposes a service externally to the cluster by means of the target nodes IP address and the NodePort. NodePort is the default setting if the port field is not specified.\n\n探針檢查\n\nlivenessProbe:\ninitialDelaySeconds: 10\nperiodSeconds: 10\nsuccessThreshold: 1\nfailureThreshold: 3\ntcpSocket:\nport: 80\nreadinessProbe:\ninitialDelaySeconds: 30\nperiodSeconds: 10\nsuccessThreshold: 1\nfailureThreshold: 3\ntcpSocket:\nport: 80\n\npull image 認證\n如果要佈署的image是屬於 私密的，\n需要加上\n\nimagePullSecrets:\n- name: regcred\n\n要先注入Secred\nkubectl create secret generic regcred --from-file=./username.txt\n\nService 服務類型\n\nClusterIP\n\n一個集群內的服務，集群內的其它應用都可以訪問該服務。集群外部無法訪問它。 這也是默認的 ServiceType。\n可以使用proxy 的本機 port轉移，來訪問服務。像 (Fig.1)\n[[12.fig-1.jpg]]\n\nNodePort\n\n在所有節點（虛擬機）上開放一個特定端口，任何發送到該端口的流量都被轉發到對應服務。\n在 1 的service port，就是指這個部分。\n需使用K8s的叢集主機加上port，去訪問服務。 (Fig.2)\n[[12.fig-2.jpg]]\n\nLoadBalancer\n\nLoadBalancer 服務是暴露服務到 internet 的標準方式。在 GKE 上，這種方式會啟動一個 Network Load Balancer，它將給你一個單獨的 IP 地址，轉發所有流量到你的服務。\n一個外部ip，直接訪問即可，有時需注意防火牆有沒有擋住。\n\nref.\nKubernetes的三種外部訪問方式：NodePort、LoadBalancer 和 Ingress\nKubernetes 服務\n\nkube dns\n\nPod DNS record\n[[12.fig-3.jpg]]\n172-17-0-6.default.pod.cluster.local\n\n172-17-0-6: Pod IP\ndefault: Namespace Name\n\nService DNS record\n[[12.fig-4.jpg]]\nnginx.default.svc.cluster.local\n\nnginx: Service Name\ndefault: Namespace Name\n\nService Discovery\n[[12.fig-5.jpg]]\nweb-0.nginx.default.svc.cluster.local\n\nweb-0: Pod Name\nnginx: Service Name\ndefault: Namespace Name\n\nAuto Scaling（自動擴展）\n在Auto Scaling 裡面，有一個設定是根據cpu的使用量大小去增加Pod，\n假設\ntargetCPUUtilizationPercentage:10\n在deploy 裡的request CPU 設定 200m\n表示，當cpu的使用率在 20m(200m * 10% = 20m )時，會自動擴展\n\n如果要使用其他的指標，api 需改用 autoscaling/v2beta2\n這邊還沒測試過，有需要的可至相關網頁查詢\nref.\nKubernetes 那些事 — Auto Scaling\nKubernetes--k8s--進階--全面瞭解HPA--部署HPA實現高可用和成本控制\nKubernetes 1.8: Now with 100% Daily Value of Custom Metrics",
		"tags": [ "note","⎈"]
},

{
		"title": "一步步篩選k8s的deploy內容",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/136. 一步步篩選k8s的deploy內容/",
		"content": "睡睡念\n要抓一下，目前ingress裏面的所有白名單資訊，\n但一個一個開起來看又很麻煩，\n就寫code filter吧\n正文\n\n先取得json，先指定一個服務，不然會太亂\n\nkubectl get ingress websocketclient -o json\n\n然後開始看json 格式開始拆解，這次目的是要取得\nnginx.ingress.kubernetes.io/whitelist-source-range的值，\n這邊會碰到一個問題，因為有特殊符號 . 跟 / ，所以要用 \\ ，將它視爲一個整體。\n\nkubectl get ingress -n default -o=jsonpath='{range .items[*]}{&quot;\\n&quot;}{.metadata.name}{&quot;\\t&quot;}{.metadata.annotations.nginx\\.ingress\\.kubernetes\\.io\\/whitelist-source-range}{end}'\n\nref.\n\nJSONPath Support\nExtract information of kubernetes annotations with jsonpath",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s yaml撰寫 volume 踩坑篇",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/2. k8s yaml撰寫 volume 踩坑篇/",
		"content": "前言\n昨天接到一個任務，要把一個純html，放到nginx上面。\n然後就開始踩坑之旅了\n正文\n這邊寫的都是GKE 的佈署方式，\n關於GKE 文章實在太多了，我也不知道何年何月何日才有那個技術存量可以把它寫成文章。\n目前使用的是 kustomize的方式佈署網站，\n目前也在架設drone，日後走的是自動佈署，\n但是第一次佈署還是要自己來的。\n基本的流程是，將網頁包成一個 docker image，\n(不論是php 或是 html， Go 則是本身有服務可以掛載，比較簡單點。)\n再將此image設定成 initContainers ，\n將此image的資料夾，掛載成volume，讓nginx可以直接掛載此資料夾。\n這次踩的坑有兩個，\n\n網站程式，是別人傳給我的，所以檔案權限，everyone都是 無法存取。\n所以要把裡面的所有檔案改成 755 rwxr-xr-x ，nginx圖片才能夠讀取。\n我懶的一個一個改，所以下指令一次把整個資料夾內容都改掉，當初安全點是設定777\n\nchmod -R 755 resource\n\n(ref. https://www.jianshu.com/p/6c52260b1ff3)\n\n在寫yaml的時候，沒有搞清楚 volume的用途\n導致一直出現rsync的錯誤。\n\ninitContainers:\n- command:\n- sh\n- -c\n- |\nrsync -avrh --delete /source/* /app\nimage: gcr.io/project/busybox-web:v2.7\nimagePullPolicy: Always\nname: source\nvolumeMounts:\n- mountPath: /app\nname: source\nvolumes:\n- emptyDir: {}\nname: source\n\nvolumeMounts: 這段的意思是 定義一個叫 soure的新磁區，掛載在 busybox-web容器裡的 /app 的path上面。\nvolumes: 的意思是定義磁碟區，emptyDir是伴隨著pod的生命，當pod消失資料也會跟著消失\n\nnginx.conf 改完後，要重啟pod ，因為nginx.conf 是寫在 k8s的 configMapGenerator，\n所以更改裡面的設定後，不會重啟pod ，裡面的服務就不會重新抓取conf。",
		"tags": [ "note","⎈"]
},

{
		"title": "kubernetes 批次檢查yaml特定字元",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/74. kubernetes 批次檢查yaml特定字元/",
		"content": "前言\n因為dockershim的問題，\n這次要搬cluster的node了。\n但我根本忘記有哪個yaml有寫到 nodeSelector了，\n一個一個看又好懶...怎麼辦？\n正文\n想起來之前在檢查 docker.sock的時候，有用指令檢查過這個yaml (1)裡面，\n有沒有 /docker.sock\nkubectl get pods --all-namespaces \\\n-o=jsonpath='{range .items[*]}{&quot;\\n&quot;}{.metadata.namespace}{&quot;:\\t&quot;}{.metadata.name}{&quot;:\\t&quot;}{range .spec.volumes[*]}{.hostPath.path}{&quot;, &quot;}{end}{end}' \\\n| sort \\\n| grep '/var/run/docker.sock'\n\n這段最後面兩個就不用看了，sort 跟 grep 是 linux的指令。\n主要針對 -o jsonpath 來看，詳細指令　 參考(2)\n當要取得複數資料時，用 range 來取資料。\n然後...參考 pod的yaml看要取得哪些欄位來用吧。\n下面範例為取得 全部的 nodeSelectord\nkubectl get pods -A　 \\\n-o=jsonpath='{range .items[*]}{&quot;\\n&quot;}{.metadata.namespace}{&quot;:\\t&quot;}{.metadata.name}{&quot;:\\t&quot;}{.spec.nodeSelector}{end}'\n\n再排序一下，就可以很快的知道有哪些yaml有用到 nodeSelector了。\nref.\n1. 從 dockershim 遷移遙測和安全代理\n2. JSONpath 幫助",
		"tags": [ "note","⎈"]
},

{
		"title": "gke-cronjob筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/85. gke-cronjob筆記/",
		"content": "前言\n之前同事是直接自己土炮用golang寫timer，\n不過如果碰到執行時間過長，重複執行的話，\n就要判斷一堆狀態，決定要不要做，\n那就改用k8s的cronJob了吧。\n正文\n文件看一看(1) (2)，好像就差不多了。\n下面是測試，每兩分鐘執行一次，\n但如果執行時間過長，是不是會在執行。\n這邊關掉的是併發以及 暫停後續執行，\n另外注意，gcp的cronjob是每10秒檢查一次狀態。\napiVersion: batch/v1beta1 # k8s after version 1.21 ,apiVersion: batch/v1\nkind: CronJob\nmetadata:\nname: hello\nspec:\nschedule: &quot;*/2 * * * *&quot; # min hour day Mon week\nconcurrencyPolicy: Forbid # 並發政策\nstartingDeadlineSeconds: 60 # 截止時限\nsuspend: true # 暫停後續執行\nsuccessfulJobsHistoryLimit: 3 # 歷史限制: 保存成功的數量\nfailedJobsHistoryLimit: 1 # 歷史限制： 保存失敗的數量\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: hello\nimage: busybox\nargs:\n- /bin/sh\n- -c\n- echo &quot;start&quot;; date; echo &quot;Hello, World! Sleep 150s&quot;; sleep 150s; date;\nrestartPolicy: OnFailure\n\nref.\n1. GCP-CronJob\n2. k8s-CronJob",
		"tags": [ "note","⎈"]
},

{
		"title": "kustomize 共用label(optional）",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/87.kustomize 共用label(optional）/",
		"content": "前言\n要把別人寫的yaml拆開，然後整併到自己的image，\n才發現service的 base 怎麼沒寫selector ，\n但佈署時卻有mapping到。\n正文\n原因在kustomize的 共用屬性\ncommonLabels:\napp: token-proxy\n\n這個會自動在各個resource上面增加label，\n如果這邊拔掉的話，\n需要在Deployment以及service上面增加label。\n但在一些文章也有人討論到這個問題，\n如果我不要讓他強制增加的話，\n該怎麼做？\nkustomize有新的參數可供設定(1)。\ncommonLabels:\n- path: metadata/labels\ncreate: true\n\n- path: spec/selector\ncreate: true\nversion: v1\nkind: Service\n\n- path: spec/selector/matchLabels\ncreate: true\nkind: Deployment\n\n另外，還有 下列新增的可供參考。\n\nannotations\nimages\nlabels\nname reference\nnamespace\nprefix/suffix\nvariable reference\n\nref.\n1 .transformerconfigs",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s 細節補充",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/94. k8s 細節補充/",
		"content": "前言\n最近幫其他組的同事看問題時，\n發現有些東西是我沒看過的，\n就整合再一起，寫起來當備忘拉。\n正文\n\ndeployment 的 lifecycle\npostStart 在container創建後，立即發送postStart事件。\npreStop 在 container結束前，立即發送preStop事件。\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: lifecycle-demo\nspec:\ncontainers:\n- name: lifecycle-demo-container\nimage: nginx\nlifecycle:\npostStart:\nexec:\ncommand: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;]\npreStop:\nexec:\ncommand: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;nginx -s quit; while killall -0 nginx; do sleep 1; done&quot;]\n\nref. 為容器的生命週期事件設置處理函數\n\npod的終止\n通常情況下，container會發送TERM信號到每個container中，\n當超出了正常終止期限，則會向其他的processor發送KILL信號\nref. Pod 的終止\n\n探針\nlivenessProbe ：判斷container是否正常\nreadinessProbe：判斷服務是否正常\n\n查詢kubectl resouce 簡寫\n\nkubectl api-resources\n\n類似下列資源都可用縮寫\nconfigmap = cm\ndeployment=deploy\npod = po\npersistentvolumeclaims = pvc\nref .【從題目中學習k8s】-【Day8】K8s常用指令 (Cheat Sheet)&amp;解題技巧\n\nk8s的 command ，bash -c 意思\n\ncommand: [&quot;/bin/bash&quot;,&quot;-c&quot;]\nargs: [ &quot;service cron start; while true; do sleep 30; done;&quot; ]\n\n很常看到 bash -c ，這個表示從後面的字串中 讀取命令來使用。\nps. bash 跟 sh 不一樣，sh通常是指dash。\n可用下列指令，查看目前使用的shell\necho $SHELL\nor\nls -l /bin/sh\n\nref.\n- The bash shell\n- 【Bash】什麼是 #!/bin/bash, #!/bin/sh，為什麼要加在 script 前面 (問題解決：sh, bash 的不同)\n\n使用指令建立基本的 yaml\n\nkubectl run nginx-kusc00101 --image=nginx --restart=Never --dry-run=client -o yaml&gt; q1.pod\n\n有基本指令後再去裡面修修改改。\n如果要直接取得現有的yaml\nkubectl get deploy yiyuan-landingpage-qa -n istio-yiyuan -o yaml &gt; yiyuan.yaml",
		"tags": [ "note","⎈"]
},

{
		"title": "k8s中的command與dockerfile的 CMD",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⎈ k8s/Yaml/98. k8s中的command與dockerfile的 CMD/",
		"content": "前言\n忘記是什麼原因，\n讓我跑去查這兩個的差異\n正文\ndockerfile中的指令\nCMD 、 ENTRYPOINT\nk8s 中的yaml 中也有\ncommand 、args\n先來分辨 CMD 與 ENTRYPOINT的差別\nCMD\n先上個Dockerfile\nFROM golang:1.16-alpine AS build-env\n\nCMD [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;echo $HOME&quot;]\n\n這個build完後，直接執行會顯示 /root，如果用\ndocker run golang:1.16 /bin/sh -c &quot;echo $SHELL&quot;\n\n，會直接執行剛剛的指令。\n原因是，CMD 跟 外面接受的指令會互相覆蓋。\n\nENTRYPOINT\nDockerfile\nFROM golang:1.16-alpine AS build-env\n\nENTRYPOINT [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;echo $HOME&quot;]\n\n同樣也直接執行會顯示 /root ，\n如果用\ndocker run golang:1.17 /bin/sh -c &quot;echo $SHELL&quot;\n\n還是顯示 /root\n原因是，使用entrypoint的話，指令不會互相覆蓋。\n反而是被當成參數加在後面。\nref. Docker CMD 與 ENTRYPOINT 說明詳解\n兩者互動表\n\nNo ENTRYPOINT\nENTRYPOINT exec_entry p1_entry\nENTRYPOINT [“exec_entry”, “p1_entry”]\n\nNo CMD\nerror, not allowed\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry\n\nCMD [“exec_cmd”, “p1_cmd”]\nexec_cmd p1_cmd\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry exec_cmd p1_cmd\n\nCMD exec_cmd p1_cmd\n/bin/sh -c exec_cmd p1_cmd\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry /bin/sh -c exec_cmd p1_cmd\n\nref.\n\nUnderstand how CMD and ENTRYPOINT interact\nDocker CMD 與 ENTRYPOINT 說明詳解\n\nK8S yaml command\n佈署個cronJob來測試\n這次掛載的映像檔是使用 dockerfile『command』的映像檔\n跟上面測試方法一樣，先不加command看結果是什麼\n加了後結果又會是什麼。\nspec:\ncontainers:\n- command:\n- /bin/sh\n- -c\n- echo $PATH\n\n11:57 那個沒有加command，所以會看到顯示root\n12:03 加了上面的command，指令覆蓋，所以顯示$PATH的內容\np.s 如果不用command改用args，指令覆蓋。\nK8S yaml args\n這次掛載的映像檔是使用 dockerfile『ENTRYPOINT』的映像檔\nspec:\ncontainers:\n- args:\n- /bin/sh\n- -c\n- echo $PATH\n\n13:44 不加任何參數，顯示 /root\n13:45 加了args，還是顯示/root，指令無覆蓋\nargs的主要用途是，當dockerfile 為 ENTRYPOINT時，\n使用args會將其視爲參數傳入到image內。\nps. 如果改使用 command ，則覆蓋\nref. k8s command, args, entrypoint, cmd 區別\n結論\nDocker\nENTRYPOINT 主要是將整個container當做可執行檔來用。\nCMD 則比較類似一台VM\nK8S\ncommand ： 指令都是覆蓋\nargs ： 外加的參數，可與command一起使用，\nspec:\ncontainers:\n- args:\n- -a\ncommand:\n- /bin/sh\n- -c\n- uname",
		"tags": [ "note","⎈"]
},

{
		"title": " ⏱ Drone MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/0.Drone MOC/",
		"content": "建置\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/72. drone on GKE 建立/\">72. drone on GKE 建立</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/10.drone 使用 騰訊雲 K8S及 容器鏡像服務/\">10.drone 使用 騰訊雲 K8S及 容器鏡像服務</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/77. drone build Open Source Edition/\">77. drone build Open Source Edition</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/78. drone k8s-runner 進階設定/\">78. drone k8s-runner 進階設定</a>\n\n編寫\n\n<a class=\"internal-link is-unresolved\" href=\"/404\">3. 撰寫drone.yml 筆記篇</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/109. drone jsonnet 各種怪招/\">109. drone jsonnet 各種怪招</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">119. Drone plugin-Drone-docker 修改原始檔</a>",
		"tags": [ "note","⏱"]
},

{
		"title": "drone 使用 騰訊雲 K8S及 容器鏡像服務",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/10.drone 使用 騰訊雲 K8S及 容器鏡像服務/",
		"content": "前言\n科技始終來自於人性，現在要讓自動佈署可以支援到騰訊雲\n正文\n1.發佈image到騰訊雲的容器鏡像服務\n直接先給 drone.yml\n- name: push2TCR\nimage: plugins/docker\nsettings:\nusername:\nfrom_secret: TCR_user\npassword:\nfrom_secret: TCR_PW\nbuild_args:\n- website=qa\n- location=tke\nrepo: tcr.tencentcloudcr.com/project/abc\nregistry: tcr.tencentcloudcr.com\ntags:\n- latest\n- ${DRONE_COMMIT}\n\n這邊使用的docker image 請參考 Docker\n要注意的地方在，如果直接把username跟 password 打在上面的話，\n會發生驗證失敗。\n需要將帳號密碼放到drone裡面的 Secrets，\n一開始也可以使用 command 直接看有沒有成功登入。\n- name: push2TCR\nimage: plugins/docker\ncommands:\n- docker login tcr.tencentcloudcr.com --username 123456789 --password token\n\n另外，當初在設定的時候，有限定ip才能連線，這部分記得要打開\n，如果沒有就不用在意了。\n\n2.佈署到騰訊雲\nkind: pipeline\nname: TKEPipeline(QA)\n\nplatform:\nos: linux\narch: amd64\n\nsteps:\n\n- name: deploy2TKE-QA\nimage: danielgormly/drone-plugin-kube:0.0.2\nsettings:\ntemplate: tke.yml\nserver: https://loud.com\nca:\nfrom_secret: TKE_CA\ntoken:\nfrom_secret: TKE_TOKEN\nnamespace: demo\n\nserver 跟 ca token，需看騰訊雲集群上的設定\n\n這個image不像GKE一樣有參數可以丟進去，只有固定的幾個參數\nref. danielgormly/drone-plugin-kube\n所幸這個有放在github上，還能夠根據自己的需求更改內容，增加參數。\n重新編譯後，如果不是上傳到公開的Container Registry ，而是傳到私有的CR就必須加上\nimage_pull_secrets:\n- TKE_LOGIN\n\n這個與image 同樣的層級，不然會導致pull image失敗\n\nref.How to pull private images with 1.0，\n至於TKE_LOGIN的內容（這個也必須加在drone的Secrets內），\n通常是在 $HOME/.docker/config.json 內，會長得像\n{\n\t&quot;auths&quot;: {\n\t\t&quot;https://index.docker.io/v1/&quot;: {\n\t\t\t&quot;auth&quot;: &quot;YW11cmRhY2E6c3VwZXJzZWNyZXRwYXNzd29yZA==&quot;\n\t\t}\n\t}\n}\n\n但是，我的只有\n{\n\t&quot;auths&quot;: {\n\t\t&quot;https://index.docker.io/v1/&quot;: {}\n\t}\n}\n\n所以下面那個就必須自己產生了，\n產生的方式如下\necho -n 'username:password' | base64\n把username與 password 輸入，轉成base64，再貼上去至程式碼內，並儲存至drone的Secrets\nref.Docker login auth token\n至於修改這個image，要加入參數的話，\n目前看應該是 增加 main.go 跟 plugin 的 Kubeconfig 參數，然後重新編譯(build.sh)即可\np.s. 在騰訊雲的pod(Alpine Linux)內安裝軟體，需替換鏡像源\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories\n\nref.\n支援k8s的各種指令Sh4d1/drone-kubernetes\n可直接在drone上下kubectl指令sinlead/drone-kubectl\nalpine源配置",
		"tags": [ "note","⏱"]
},

{
		"title": "drone jsonnet 各種怪招",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/109. drone jsonnet 各種怪招/",
		"content": "睡睡念\n本文章內容包含許多寫drone jsonnet會碰到的寫法。\n正文\n\n在pipeline下comand的時候，有時會碰到多行指令湊在一起的情況，\n在drone.yaml是直接這樣寫\n\n但如果要用jsonnet的話，要用\\n換行，\n\n轉換後就會變成下圖\n\n在執行command的時候，因為要先用&quot; 將指令包起來，如果裏面還要有&quot; 的話，必須在前面加上 \\\n\n執行drone jsonnet --stream --format後\n\n這是因爲 linux shell裏面，單引號跟雙引號的差別，詳細可看<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/133. shell script字串處理/\">shell script 雙引號與單引號</a>\n\n如果碰到很長的一串指令，例如if else之類的，改使用 @'' 將程式包起來。\n\n@'\nif [ -z $${TAG_FROM} ]\n\nthen\n\necho &quot;錯誤：無來源TAG。&quot;;\n\nexit 1\n\nfi'\n\n如果字串過長，想要換行的話， 可在雙引號內 使用 兩個\\\n\n用curl時，有些參數要用&quot; ，所以在單引號使用 &quot; &quot; 固定參數，要先用 @ '' 包起來\n\n@'OAUTH2_TOKEN=$(curl -XGET $${TOTP_AUTH_URL} \\\\\n-H &quot;accept: application/json&quot; \\\\\n-H &quot;TOTP-PIN-CODE: $${pin}&quot; \\\\\n-H &quot;TOTP-USER: $${user}&quot;)',\n\nref.\n\njsonnet教學\njsonnet 官方說明\nStandard Library",
		"tags": [ "note","⏱"]
},

{
		"title": "Drone  plugin-Drone-docker 修改原始檔",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/119. Drone  plugin-Drone-docker 修改原始檔/",
		"content": "碎碎念\n之前發現一個後端執行drone會發生的問題，\n偶爾發生，但一段時間總會發生一次。\n多個step同時build，會造成image錯亂。\n好麻煩阿....\n正文\n先來看幾張圖，應該就能瞭解我再說什麼了。\n\n首先看到第一張圖，他在執行docker build的時候，會用git commit的值，當作image的tag名稱。\n然後看第二張圖，直到build完image後，才會再根據你的tag去將image改成你要的tags\n所以我們來想一個情境，當A、B同時執行時，他們的git commit都一樣，\nA已經build完了，正要執行最後一個步驟，將git commit tag改成正常的image tag 。\n此時B也剛build完，所以他也產生了一個一模一樣的 commit tag的image，那A執行最後一個步驟時，他改得到底是誰的image？\n\n如果一個接一個step跑的話，時間大概需要 4分半，\n但同步的話，只要1分半，時間快了三倍。\n\n所以還是要想辦法解掉，本來有想幾個解法，\n\n一次把程式都build一遍，放在同一個image內，\n執行時，再根據參數看要執行哪個檔案。\n\n但是他們有一種情境是只要更新其中一個功能就好，那此時全部build就不太適合。\n\n在drone step時，塞tag進去，但他只會在最後一個步驟，才會改image的tag ，這招也不行\n\n改plugin/gcr 了，在第一步驟時就把image的tag改掉，不要使用git commit，那就解掉這個問題了。\n\n動手\n首先找到這個image的原始碼 drone-plugins/drone-docker，然後開始改吧。\n各環境main.go的package import docker 要先改掉，不改掉我不確定他會不會抓本機的package，我golang初學沒多久。\n修改前\nimport (\n\t&quot;os&quot;\n\t&quot;runtime&quot;\n\n\t&quot;github.com/joho/godotenv&quot;\n\t&quot;github.com/sirupsen/logrus&quot;\n\t&quot;github.com/urfave/cli&quot;\n\n\tdocker &quot;github.com/drone-plugins/drone-docker&quot;\n)\n\n修改後\nimport (\n\t&quot;os&quot;\n\t&quot;runtime&quot;\n\n\t&quot;github.com/joho/godotenv&quot;\n\t&quot;github.com/sirupsen/logrus&quot;\n\t&quot;github.com/urfave/cli&quot;\n\n\tdocker &quot;drone-docker&quot;\n)\n\n然後我在docker.go上面直接加了兩行，指定了tag，這不適合所有人，\n因為你們的使用情境可能跟我不太一樣，我這邊是只要push到git後，drone會自動產生tag，所以如果不指定的話，我還是有tag的值可以取得。\ntag := build.Repo + &quot;:&quot; + build.Tags[0]\n\nargs := []string{\n&quot;build&quot;,\n&quot;--rm=true&quot;,\n&quot;-f&quot;, build.Dockerfile,\n&quot;-t&quot;, build.Name,\n&quot;-t&quot;, tag,\n}\n\n另外，如果要知道plugin/gcr裏面有什麼環境變數可以拿來用的話，\n可以在drone的step上面，執行export -p\n- name: publish-dev\nimage: plugins/gcr\npull: if-not-exists\nvolumes:\n- name: docker\npath: /var/run/docker.sock\ncommands:\n- export -p\n\n執行結果：\n\n改完後，執行下面的指令先產生go 的程式。\n這個go的指令，我是從.drone.yaml抄出來的。\n執行前記得先設定環境變數\nexport GOOS=linux\nexport GOARCH=amd64\nexport CGO_ENABLED=0\nexport GO111MODULE=on\n\nGo build\ngo build -v -ldflags &quot;-X main.version=${DRONE_COMMIT_SHA:0:8}&quot; -a -tags netgo -o release/linux/amd64/drone-docker ./cmd/drone-docker\n\ngo build -v -ldflags &quot;-X main.version=${DRONE_COMMIT_SHA:0:8}&quot; -a -tags netgo -o release/linux/amd64/drone-gcr ./cmd/drone-gcr\n\nDocker build\n將go的執行檔打包進去docker image內，\n先打包到本機的docker:linux-amd64，\n因為docker-gcr的dockerfile會去抓docker:linux-amd64 當作image的基底再產生檔案\ndocker build \\\n--label org.label-schema.build-date=$(date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;) \\\n--label org.label-schema.vcs-ref=$(git rev-parse --short HEAD) \\\n--file docker/docker/Dockerfile.linux.amd64 --tag plugins/docker:linux-amd64 .\n\ndocker build \\\n--label org.label-schema.build-date=$(date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;) \\\n--label org.label-schema.vcs-ref=$(git rev-parse --short HEAD) \\\n--file docker/gcr/Dockerfile.linux.amd64 --tag asia-east1-docker.pkg.dev/rouge-sms/docker-public/gcr:0.0.13 .\n\n完成\n\n上圖可以看到，在docker build時，會同時下兩個tag，\n一個還是維持git commit的值，但另一個就是拿repository加版本號當作tag。\nref.\n\nDRONE_STEP_NAME not available as substitution in environment\n\n查資料時剛好有碰到這個問題，drone_step_name一直抓不到，原因再文章內。\n\n修復在多模組java項目中，平行打包鏡像時，鏡像因為同名稱被覆蓋的問題\n\n昨天才看到有人發了個PR修復同名稱的問題，感覺跟我的頗像，但實際看了，覺得不太適合。",
		"tags": [ "note","⏱"]
},

{
		"title": "撰寫drone.yml 筆記篇",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/3. 撰寫drone.yml 筆記篇(未)/",
		"content": "前言\n換工作後，開始接觸k8s、GCP、GKE，再來是Drone。\nk8s中又延伸了kustomize ，ymal的寫法、佈署方式。\nGCP中的cloud armor、GCE ，然後GKE的架構\n真的是....滿滿的坑阿。以上講到的，還處於懞懂中，\n筆記不知道何年何月何日可寫\n正文\n簡單說一下環境，在GKE上面佈署 drone 跟 gitlab，\n當gitlab有條件觸發的話(例如在push 在master上)，會同時送出一個webhook給drone，\n此時drone就會去gitlab pull程式碼下來，做你想要的動作（例如，打包成image，把image送到gcr上面，再佈署到GKE上）。\n先來份yaml，建議可以先看一下yaml是什麼東西，這樣寫起來比較不會撞牆。\n(ref. https://zh.wikipedia.org/wiki/YAML)\nkind: pipeline\nname: Pipeline(branch)\n\nsteps:\n- name: push2GCR(前哨)\nimage: plugins/gcr\nsettings:\nrepo: gcr.io/rd7-project/landingpage\ntags:\n- latest\n- ${DRONE_COMMIT}\n# dockerfile: configs/beta/Dockerfile\njson_key:\nfrom_secret: GOOGLE_CREDENTIALS\nbuild_args:\n- website=beta\n\n- name: deploy2GKE(前哨)\nimage: nytimes/drone-gke\nenvironment:\nTOKEN:\nfrom_secret: GOOGLE_CREDENTIALS\nsettings:\nproject: rd7-project\n# 改拉參數傳入kube.yml內\n# template: configs/beta/.kube.yml\nvars:\ndeployName: yaboxxx-landing-page-beta\nenv: beta\nzone: asia-east1-a\ncluster: xbb-common\ntrigger:\n# branch:\n# - master\nref:\ninclude:\n- refs/heads/master\nevent:\n- push\n\n分成三小塊來看，\n如果以 --- 區分的話，那\nkind: pipeline\nname: Pipeline(branch)\n是必須的。\n再來是 steps ，\n根據步驟游上往下執行，如果要并行的話，則使用depend_on ，\n這部分留到下次講。\n\ntrigger ，這是觸發條件，表示在gitlab上面做了哪些動作，會觸發這個step（步驟）\n這邊的觸發事件，常用到應該是 Branch、Reference 以及 Event\n(ref. https://docs.drone.io/pipeline/docker/syntax/trigger/)\nBranch 跟 Event 比較好理解（但碰到Tag的事件又是另一回事了） 。\nReference 這個其實只要在你的git裡面下指令\ntree .git/refs/\n大概就會知道他在講些什麼，git的底層都是靠refs去用出來的\n(\nref.\nhttp://iissnan.com/progit/html/zh-tw/ch9_3.html\nhttps://titangene.github.io/article/git-branch-ref.html\n)\n但要注意的是，drone 1.X，以後的版本不支援event的tag 與 branch 共存。\n可以參考這篇\n(\nref.\nhttps://github.com/drone/drone/issues/2536\nhttps://rabbit52.com/2019/11/drone-from-0-8-to-1-0/\n)\n\nsteps:\n這部分則開始由上往下開始執行，\n像上面的程式先做的是將程式打包後，上傳到GCR上面。\n（最上面還有一段clone，會先做（這是drone預設的），將gitlab的程式複製到機器內，這樣才能做後面的上傳，\n這一塊可以 參考 https://docs.drone.io/pipeline/docker/syntax/cloning/ ）\n上傳到GCR使用的 plugins/gcr ，這個image，裡面的參數請參考說明頁面\n由於這邊需要上傳到GCR，故要先到Google 的 服務賬戶申請權限 ，除了需要push以外還必須要有 storage.buckets.get\n這邊會取得一個json檔案，將內容塞入drone內的Sercets。",
		"tags": [ "note","⏱"]
},

{
		"title": "drone on GKE 建立",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/72. drone on GKE 建立/",
		"content": "前言\n正文\ngitlab的安裝方式，參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/71.gitlab on GKE 災害還原筆記/\">71.gitlab on GKE 災害還原筆記</a>\n再來是安裝drone的方式，\n這邊之前沒寫文件，重試的時候搞了一陣子。\n要先知道 runner分成哪幾種，根據你的環境選擇你要的方式。\n在drone畫面，基本上都差不多。\n有差異的點是再建置時，就直接看圖吧\nDrone Runner分成五種，適用於不同的情境\n\nDocker runner :是drone最常見的用法，pineline中的每一個step都起一個獨立的container運行特定任務，本文即是採用docker runner。\n\n在建置時，會在該runner裡面執行docker，所以當下只會看到runner裡面的cpu跟ram使用量上升。\n\nKubernetes runner:跟docker runner 的功能相似，只是改成Kubernetes 版本，如果你的服務要搭建在k8s上，就用這個。\n\n選擇kubernetes的版本，再建置時會在當下的叢集建立一個pod，\n然後就會在裡面執行要跑得步驟。\n\nExec runner:用於你的專案不適合跑在容器內的狀況EX:MacOs專案。\n\nSSH runner:直接以ssh 連線到遠端以default shell 執行pipeline流程，需要使用openssh 7.9 以上版本。\n\nDigital Ocean runner :用於當你的pipeline需要權限可以操作整台虛擬機的情況，且不適合跑在容器內。\n\n角色\n功能\n\n用戶\nGitlab\n\nDrone Server\nDrone 主服務，提供Web界面\n\nDrone Runner\n我理解為實現各種操作的適配器，例如ssh、docker、k8s操作\n\nDrone Agent\n操作主機 Docker API 的代理程序\n\nDocker Server\n主機的 Doker 程序\n\n發佈到GKE，參數說明\n發佈到GCR，參數說明\n\n再來就是要佈署 drone server跟 runner了。\nStep 1.\n首先要到Gitlab，建立oAuth的id 跟token。\n切記是在user Setting的 Application，\n因為Gitlab是我建的，所以我有另一個Admin的區域，\n不是在Admin裡面。\n\n網頁不要關， Application ID 跟 Secret等等會用到。\n\nStep 2. drone Server\n\n將一些參數改成自己的。\n\nyaml欄位\n對應的欄位\n\nDRONE_GITLAB_CLIENT_ID\nApplication ID\n\nDRONE_GITLAB_CLIENT_SECRET\nSecret\n\nDRONE_SERVER_HOST\nCallback URL 的domain\n\nDRONE_RPC_SECRET\n自行建立，runner的secret需一致\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: gke-drone-server\nlabels:\napp: drone\ndrone: drone-server\nio.kompose.service: drone-server\nspec:\nselector:\nmatchLabels:\napp: drone\ndrone: drone-server\nio.kompose.service: drone-server\ntemplate:\nmetadata:\nlabels:\napp: drone\ndrone: drone-server\nio.kompose.service: drone-server\nspec:\ncontainers:\n- env:\n- name: DRONE_GITLAB_CLIENT_ID # OAuth 的 Application ID\nvalue: 5b2f90188137bab7b1c6959721feb6e55f1137919e5b4b5ce005bd5e15af536a\n- name: DRONE_GITLAB_CLIENT_SECRET # OAuth 的 Secret\nvalue: 4cc261149603b3df864677f5af37b55613f6f4d20dd65fbb3e4f77f0cbca9e15\n- name: DRONE_GITLAB_SERVER # Gitlab Server\nvalue: http://35.234.26.234\n- name: DRONE_GIT_ALWAYS_AUTH # Drone clone 時，是否每次都驗證\nvalue: &quot;false&quot;\n- name: DRONE_LOGS_COLOR # Log 啟用顏色辨識\nvalue: &quot;true&quot;\n- name: DRONE_LOGS_DEBUG # 選擇是否開啟 debug 模式\nvalue: &quot;false&quot;\n- name: DRONE_LOGS_PRETY\nvalue: &quot;false&quot;\n# - name: DRONE_LOGS_TRACE #開啟log追蹤\n# value: &quot;true&quot;\n- name: DRONE_RUNNER_CAPACITY # 表示一次可執行 n 個 job\nvalue: &quot;3&quot;\n- name: DRONE_SERVER_HOST # Drone URL,不能加HTTP，解析會在加上HTTP\nvalue: 14.199.130.26\n- name: DRONE_SERVER_PROTO # http 或者 https 連線設定\nvalue: http\n- name: DRONE_TLS_AUTOCERT # 自動生成 ssl 證書，並接受 https 連線，末認為false\nvalue: &quot;false&quot;\n- name: DRONE_REGISTRATION_CLOSED # 啟用的話，管理員需在用戶登入前創建用戶\nvalue: &quot;false&quot;\n- name: DRONE_USER_CREATE # 建立admin權限的使用者 cli登入用\nvalue: username:ezio,admin:true\n- name: DRONE_USER_FILTER # 可操作 Drone 的用戶清單\nvalue: root,ezio\n- name: DRONE_RPC_SECRET\nvalue: 824de51510f08498fa770c7798e3cc9e\nimage: drone/drone:2\nimagePullPolicy: &quot;Always&quot;\nname: drone-server\nports:\n- containerPort: 80\nresources: {}\nvolumeMounts:\n- mountPath: /data\nname: drone-server-claim0\nrestartPolicy: Always\nserviceAccountName: &quot;&quot;\nvolumes:\n- name: drone-server-claim0\npersistentVolumeClaim:\nclaimName: gke-drone-server-claim0\n\n如果要其他git的安裝方式請參考 1. 的官方文件\nref.\n\nDrone Gitlab\n\nStep 3-a. drone Runner - Docker Runner\n\n注意 DRONE_RPC_SECRET需與server一致。\n這邊會用到 docker.sock，也是因為這個的關係，\n才導致這次重新測試。\nk8s即將廢棄 Dockershim，改用containerd，\n詳細內容就不多說了，有興趣的可參考 (1) 、 (2)\n雖然我實測出來的結果，\n在containerd的node是可以用的。\n有在想是不是因為核心還是 /var/run/docker.sock 個關係，\n但隨著後續研究 docker.sock (3)，發覺應該是不能用的才對...\n但為什麼能用還是搞不懂。但(4)的官方論壇有說，他不擔心這個問題，\n實際上還真的沒問題...\n補充，\n最近把node的 containerd 版本升到 1.20 後，Docker Runner就真的不能用了。應該是早期還在用舊版的containerd，所以才能跑。\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: agent\nspec:\nselector:\nmatchLabels:\ndrone: drone-agent\nreplicas: 1\ntemplate:\nspec:\ncontainers:\n- env:\n- name: DRONE_LOGS_DEBUG\nvalue: &quot;false&quot;\n# - name: DRONE_LOGS_TRACE #開啟log追蹤\n# value: &quot;true&quot;\n- name: DRONE_RPC_SECRET\nvalue: 824de51510f08498fa770c7798e3cc9e\n- name: DRONE_RPC_HOST\nvalue: 14.199.130.26\n- name: DRONE_RPC_PROTO\nvalue: &quot;http&quot;\n- name: DRONE_RUNNER_CAPACITY\nvalue: &quot;3&quot;\nimage: drone/drone-runner-docker:1\nimagePullPolicy: &quot;&quot;\nname: drone-agent\nresources: {}\nvolumeMounts:\n- mountPath: /var/run/docker.sock\nname: docker-socket\nrestartPolicy: Always\nserviceAccountName: &quot;&quot;\nvolumes:\n- name: docker-socket\nhostPath:\npath: /var/run/docker.sock\n\nref.\n1. 檢查棄用 Dockershim 對你的影響\n2. 棄用 Dockershim 的常見問題\n3. [Docker] 掛載 /var/run/docker.sock 的用意？\n4. Drone Disscus - Docker plugin &amp; dockershim deprecation // Kubernetes 1.20+\nStep 3-b. drone Runner - kubernete Runner\n\n注意 DRONE_RPC_SECRET需與server一致。\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nnamespace: default\nname: drone\nrules:\n- apiGroups:\n- &quot;&quot;\nresources:\n- secrets\nverbs:\n- create\n- delete\n- apiGroups:\n- &quot;&quot;\nresources:\n- pods\n- pods/log\nverbs:\n- get\n- create\n- delete\n- list\n- watch\n- update\n\n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: drone\nnamespace: default\nsubjects:\n- kind: ServiceAccount\nname: default\nnamespace: default\nroleRef:\nkind: Role\nname: drone\napiGroup: rbac.authorization.k8s.io\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: gke-drone-agent\n\nlabels:\napp: drone\ndrone: drone-agent\nio.kompose.service: drone-agent\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: drone\ndrone: drone-agent\nio.kompose.service: drone-agent\ntemplate:\nmetadata:\nannotations:\nsidecar.istio.io/inject: &quot;true&quot;\nlabels:\napp: drone\ndrone: drone-agent\nio.kompose.service: drone-agent\nspec:\ncontainers:\n- name: runner\nimage: drone/drone-runner-kube:latest\nports:\n- containerPort: 3000\nenv:\n- name: DRONE_RPC_SECRET\nvalue: 824de51510f08498fa770c7798e3cc9e\n- name: DRONE_RPC_HOST\nvalue: 14.199.130.26\n- name: DRONE_RPC_PROTO\nvalue: &quot;http&quot;\n- name: DRONE_RUNNER_CAPACITY\nvalue: &quot;3&quot;\n\nref.\n- KUBERNETES Runner\n問題排除\n\n首先碰到的問題是，drone沒有反應，先檢查webhook\n到專案底下的webhook選項，查看錯誤的訊息\n\n因為GKE的防火牆沒開的關係，導致webhook資料丟不過去。\n\n2. drone無法連線到gitlab\n\n本來以為是防火牆的關係，但直接連進去pod裡面，呼叫這個網址是正常的。\n後來看到 (2) 的討論，我跟他們狀況一樣，應該也是istio造成的。\n原以為是 drone server或 runner，有安裝istio的關係，\n還特別寫了\ntemplate:\nmetadata:\nannotations:\nsidecar.istio.io/inject: &quot;false&quot;\n\n來阻止 istio的注入(3)。\n後來想了又想，才驚覺，我要取消的應該是整個namespace的istio注入，\n因為當新的 pipeline建立時，就已經注入了istio的sidecar，\n\n所以要先讓這個namespace通通不能自動注入，\n指定其他的deploy會自動注入。\n自動注入的規則，請參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/73. istio sidecar 注入規則/\">73. istio sidecar 注入規則</a>\n\n切換namespace，無法佈署\n\nref.\n1. Gitlab Webhooks\n2. Drone CI pipeline first step clone failed: Connection refused\n3. 安裝 Sidecar\n4. istio -Resource Annotations",
		"tags": ["開啟log追蹤", "開啟log追蹤", "note","⏱"]
},

{
		"title": "drone build Open Source Edition",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/77. drone build Open Source Edition/",
		"content": "前言\n在某一天，drone就跳出來了 『Your license limit is exceeded。』\n當下真是嚇死我了，當天又是維護日，\n一堆程式要上版，自動佈署掛掉就完了，要通通手動上。\n還好，沒事，只是跳個通知在那邊，查了一些文件，\n都沒說到期了會怎樣，就這樣過了兩個月，\n也都沒事發生，本來想繼續撐下去的，\n但有其他東西要用，就順便一起動了。\n正文\n\n官方文件有寫到關於權限的部分，\n\nYour license limit is exceeded ，這個是因為你build 超過了5000次，然後你又是直接抓官方版本的image才會出現這個通知。（OS：一開始測試當然直接抓官方的image來用阿...）\n基本上只要是年收入沒到 1 million（美金）的公司，都能免費使用，但要自己打包image。\n\n其他細項，可自行參考官方網站\nBuild image\n我們可以先從官方的github看看，他們是怎麼編譯的。\ndrone.yml\n\n先執行， scripts/build.sh\n再根據 docker/Dockerfile.server.linux.amd64 的dockerfile去打包程式。\n\n下面這段，直接存成dockerfile去編譯吧，\n版本號自行更改。\n\n補充說明，\ndocker 在編譯的時候，\n每一行指令 (前面有加 RUN 或COPY..的這種算一行），\n都算是一個新的image，可以看到sha256的數值都不一樣，\n所以用 cd drone 切 路徑的話，要再同一行把要執行的command，\n都執行完。\nFROM golang AS Builder\n\nENV DRONE_VERSION v2.9.1\nWORKDIR $GOPATH/src\nRUN git clone https://github.com/drone/drone --branch ${DRONE_VERSION} --single-branch\nRUN cd drone &amp;&amp; pwd &amp;&amp; go mod download &amp;&amp; \\\ngo build -ldflags &quot;-extldflags \\&quot;-static\\&quot;&quot; -tags=&quot;nolimit&quot; github.com/drone/drone/cmd/drone-server\n\nFROM alpine:3.13 AS Certs\n\nRUN apk add -U --no-cache ca-certificates\n\nFROM alpine:3.13\nEXPOSE 80 443\nVOLUME /data\n\nRUN [ ! -e /etc/nsswitch.conf ] &amp;&amp; echo 'hosts: files dns' &gt; /etc/nsswitch.conf\n\nENV GODEBUG netdns=go\nENV XDG_CACHE_HOME /data\nENV DRONE_DATABASE_DRIVER sqlite3\nENV DRONE_DATABASE_DATASOURCE /data/database.sqlite\nENV DRONE_RUNNER_OS=linux\nENV DRONE_RUNNER_ARCH=amd64\nENV DRONE_SERVER_PORT=:80\nENV DRONE_SERVER_HOST=localhost\nENV DRONE_DATADOG_ENABLED=true\nENV DRONE_DATADOG_ENDPOINT=https://stats.drone.ci/api/v1/series\n\nCOPY --from=Certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nCOPY --from=Builder go/src/drone/drone-server /bin/drone-server\n\nENTRYPOINT [&quot;/bin/drone-server&quot;]\n\n本來想用go get 的方式，直接把程式build出來，\n但一直卡在go 的版號限制，看了一些文章都說要改裡面的程式。\n但這東西寫完，基本上就不會再動了。\n只好用git clone把程式先抓下來，再build了。\nref.\n\n使用容器方式編譯無功能限制的 Drone CI\ngo get命令——一鍵獲取代碼、編譯並安裝\ngo 命令",
		"tags": [ "note","⏱"]
},

{
		"title": "drone k8s-runner 進階設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⏱ Drone/78. drone k8s-runner 進階設定/",
		"content": "前言\n再上篇 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⏱ Drone/72. drone on GKE 建立/\">72. drone on GKE 建立</a>，有談到k8s runner，\n那時候都在default 的 namespace上面運作，\n所以沒什麼問題，\n但一到了正式環境，要切專門的namespace，\n就陣亡拉)))))\n正文\n這篇會講到兩個東西，\n\nRBAC(Role-based Access Control)\ndrone k8s-runner policy\n\nStep 1.\n先從Cloud RBAC開始說起，\n基本概念如下圖\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nnamespace: default\nname: drone\nrules:\n- apiGroups:\n- &quot;&quot;\nresources:\n- secrets\nverbs:\n- create\n- delete\n- apiGroups:\n- &quot;&quot;\nresources:\n- pods\n- pods/log\nverbs:\n- get\n- create\n- delete\n- list\n- watch\n- update\n\n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: drone\nnamespace: default\nsubjects:\n- kind: ServiceAccount\nname: default\nnamespace: default\nroleRef:\nkind: Role\nname: drone\napiGroup: rbac.authorization.k8s.io\n\nrole ： 設定權限能做什麼事\nroleBinding ： 要將哪個角色綁定權限到sa帳號\n這邊簡單說明roleBinding （因rolebinding的roleRef 無法指定ns)，\n\n將 role, namespace 在 default 的 drone 跟\nserviceAccount的 namespace在 default 的 default 綁在一起，\n\n更細項的說明，請參考 (1)~(4)\n當服務沒有指定是哪一個 『serviceAccountName』，就會使用預設的default 。\n所以，如果將namespace通通改成drone的話，會發生什麼事？\n會先在drone的畫面看到 skip\n\n然後查看 k8s-runner的 log，會看到沒有權限，被拒絕執行。\n\nStep 2.\n再來要解決權限問題，\n因為k8s-runner，預設都是使用 default(namespace)的default再建立pod。\n所以，現在有兩種做法（這邊會講第二種做法）。\n\nrole的權限，給他能夠在不同的ns也能佈署的權限（這部分應該是建立clusterRole，沒試過）。\nk8s-runner在pipeline時，要求在指定的namespace做動作。\n\n建立一個 policy(5) 的 yml，\n底下的關鍵欄位，有在碰k8s的應該都很熟，\nmatch這塊要再測試看看，這邊就先註解掉。\n（文件上是說 ，可以用來定義多個policy用，\n當有符合時，就執行下面的policy）\n這邊設定規則是，\n\n佈署時，佈署到drone的namespace ，\n資源請求是 512Mi,節點選擇的label 是 disktype: ssd\n\nkind: policy\nname: drone\n\n# match:\n# repo:\n# - &quot;octocat/*&quot;\n# - &quot;octocat/hello-world&quot;\n\nmetadata:\nnamespace: drone\n\nresources:\nrequest:\nmemory: 512MiB\n# limit:\n# cpu: 4000\n# memory: 1GiB\n\nnode_selector:\ndisktype: ssd\n\n再來要在runner裡面掛載這個檔案，直接用 configMap將檔案載入。\n建立configmap\nkubectl create configmap -n drone drone-policy --from-file=policy.yml --dry-run=client -o yaml | kubectl apply -f -\n\nk8s-runner內掛載configmap\n要特別注意env ，\nDRONE_POLICY_FILE(6)，需要指定你的yml位置。\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: drone-runner-k8s\nnamespace: drone\nlabels:\ndrone: runner-k8s\nspec:\nreplicas: 1\nselector:\nmatchLabels:\ndrone: runner-k8s\ntemplate:\nmetadata:\nlabels:\ndrone: runner-k8s\nspec:\ncontainers:\n- name: runner\nimage: drone/drone-runner-kube:latest\nports:\n- containerPort: 3000\nvolumeMounts:\n- mountPath: /policy.yml\nname: policy\nsubPath: policy.yml\nenv:\n- name: DRONE_POLICY_FILE\nvalue: /policy.yml\nvolumes:\n- configMap:\ndefaultMode: 420\nname: drone-policy\nname: policy\n\n附註，如果node_selector，選擇的節點是ssd的硬碟的話，\n時間會減少 35%。 但每個月會多13塊美金。\n再來下篇應該就是寫將drone完整導入istio的做法了\nref.\n\n使用 RBAC 鑑權\nKubernetes權限管理之RBAC\nDay-28 瞭解 Namespace 與 Rbac\n[Kubernetes] 如何取得合法可用的權限，讓 pod 與 API server 溝通\nPolicies\nDRONE_POLICY_FILE\n配置 Pod 的 Service Account",
		"tags": [ "note","⏱"]
},

{
		"title": "GCP MOC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/0.GCP MOC/",
		"content": "雲架構中心\nref. ### 雲架構中心\nPrice\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/GCP費用/\">GCP費用</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/GCP CPU用途/\">GCP CPU用途</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/180. GCP 監控費用/\">180. GCP 監控費用</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/126. GCP 硬碟費用比較/\">126. GCP 硬碟費用比較</a>\nGCP\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/88.批次修改GCP的label/\">88.批次修改GCP的label</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/5. Google Cloud Function限制/\">5. Google Cloud Function限制</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/95.cloud build的問題/\">95.cloud build的問題</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/97. Anthos 初體驗/\">97. Anthos 初體驗</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/120. 指令備份cloudSQL及下載/\">120. 指令備份cloudSQL及下載</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/121.cloudCDN設定/\">121.cloudCDN設定</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/128. 解決 the gcp auth plugin is deprecated in v1.22+/\">128. 解決 the gcp auth plugin is deprecated in v1.22+</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/161. GKE利用pubsub通知訊息到discord/\">161. GKE利用pubsub通知訊息到discord</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/164.gcloud install by bash/\">164.gcloud install by bash</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/181. cloudstorage無法使用cloudflare轉址/\">181. cloudstorage無法使用cloudflare轉址</a>\nGCE\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/19. GCE硬碟增加容量大小/\">GCE硬碟增加容量大小</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/46.列出GCE的所有VM/\">46.列出GCE的所有VM</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/46.GCE機器列表內的CPU個數/\">46.GCE機器列表內的CPU個數</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/79. gcloud compute instance 筆記/\">GCE指令</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/130. Google IAP ssh失敗/\">130. Google IAP ssh失敗</a>\nGCS\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/8.GCS使用方式/\">GCS使用方式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/109.GCS臨時下載權限/\">109.GCS臨時下載權限</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/127. fuse device not found, try 'modprobe fuse' first/\">127. fuse device not found, try 'modprobe fuse' first</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/129. nginx 掛載gcs/\">129. nginx 掛載gcs</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/135. gsutil整批複製/\">135. gsutil整批複製</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/140. gcs 掛載domain/\">140. gcs 掛載domain</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/141. gcs自動刪除檔案/\">141. gcs自動刪除檔案</a>\nNetwrok\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/9.GCP網路對外的方式/\">GCP網路對外的方式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/13.騰訊雲連接GOOGLE雲/\">13.騰訊雲連接GOOGLE雲</a>\nTroubleshooting\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/17. Gcloud AttributeError module importlib has no attribute util/\">17. Gcloud AttributeError module importlib has no attribute util</a>",
		"tags": [ "note","☁️"]
},

{
		"title": "GCS臨時下載權限",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/109.GCS臨時下載權限/",
		"content": "碎碎念\n今天維護，後端同仁要cloudSQL的備份檔，\n然後，因為他們都沒權限，只好丟去網路空間，給他們抓。\n我就在想，沒道理阿，應該有個方法能夠開特定的檔案臨時給人用。\n正文\n由於GCS的權限是 統一儲存分區級存取權，\n所以不能細分檔案權限給別人用。\n所以，開V4簽名，就能給別人訪問了。\ngsutil signurl -d 1m gcs-download.json gs://ai-assist-data/&quot;Cloud_SQL_Export_warehouse_2022-10-12 (06:03:23)&quot;.bak\n\n1m 表示，只開放一分鐘的時間\ngcs-download.json是SA，從IAM那邊找到一個有權限下載的帳號產生的金鑰。\ngs 的網址，可以到詳細資料內取得 gsutil URI\n\n這個執行後，會得到一串\n\n把後面的https複製，就能拿來用了。\n錯誤處理\n如果在執行signurl的時候發生以下錯誤，\nCommandException: The signurl command requires the pyopenssl library (try pip install pyopenssl or easy_install pyopenssl)\n\n執行\npip3 install pyopenssl\n\n其他方式\n將底下的檔案跑遞迴(-R)，通通設成public-read\ngsutil -m acl set -R -a public-read gs://bucket",
		"tags": [ "note","☁︎"]
},

{
		"title": "指令備份cloudSQL及下載",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/120. 指令備份cloudSQL及下載/",
		"content": "前言\n有人不習慣ui操作，\n加上他們是兩三個月用一次，\n那就生個指令給他們用吧。\n做成自動化應該也可以，\n只是要看一下權限要用誰的。\n正文\n主要的兩行指令，請將視角往下移\n\ntest-db01: cloudSQL的instance名稱\ngs://data/desk-22-12-14.bak ：要存放cloudStorage的位置\nDesk : 資料庫名稱\n\ngcloud storage cp\n\ngcloud beta sql export bak test-db01 gs://data/desk-22-12-14.bak \\\n--database=Desk\n\ngcloud storage cp gs://data/desk-22-12-14.bak /Users/daimom/Downloads\n\nTroubleshooting\n\n執行 gcloud storage cp ，出現error invalid choice storage\n\n更新 gcloud components\ngcloud components update\n\nref. gcloud beta functions command says &quot;Invalid choice functions&quot;\n\n權限問題\ncloudSQL 備份權限\nCloud SQL Editor 角色和 storage.objectAdmin IAM 角色\ncloudStorage下載權限\n這部分就需要到cloudStorage的bucket設定了\nref.\n使用 BAK 檔案匯出和匯入\n從儲存桶下載對象",
		"tags": [ "note","☁︎"]
},

{
		"title": "cloudCDN設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/121.cloudCDN設定/",
		"content": "碎碎念\n之前測完後就忘了寫，\n但今天有人問我才想起來，\n我到底跑去哪設定的！？\n現在趕緊補一下\n正文\n如果已經在GCP上面建立好了負載平衡，\n那就到後端設定，看要針對哪個服務去開啓cdn就好了。\n上面有個逾時的設定，看要設定多少，\n當初同事是有說他測30秒就會斷線一次，\n所以我有把這邊拉長。\n\n這邊設定完後，在cloudCDN上面就會多一個設定出來。\n\n也能用指令去查，啓用cdn時的欄位\nkubectl get BackendConfig\n\nref. cloudCDN",
		"tags": [ "note","☁︎"]
},

{
		"title": "fuse device not found try modprobe fuse first",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/127. fuse device not found, try &#39;modprobe fuse&#39; first/",
		"content": "睡睡念\n以前用GKE掛載gcs的時候發生錯誤，fuse device not found, try 'modprobe fuse' first。解決方法。\n現在GKE有了新的掛載GCS方法了，Announcing Cloud Storage FUSE and GKE CSI driver for AI/ML workloads。\n目前我沒采用此方式，請他們改用api的方式上傳檔案了，\n要訪問的話，用Load balancing 指向GCS bucket。\n這文章用方法應該不太適用，\n而且此方法有缺點，當你檔案數量太多時，會掛載不起來。\n正文\n\napt install -y gnupg lsb-release\nexport GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`\nexport GCSFUSE_REPO=gcsfuse-buster\necho &quot;deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main&quot; | tee /etc/apt/sources.list.d/gcsfuse.list\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\napt-get update\napt-get install -y gcsfuse\nexport GOOGLE_APPLICATION_CREDENTIALS=/home/gcs-download.json\ngcsfuse sms_backend_backup ./gcs-mount\n\n需要用特權模式\n在yaml檔裏面，加上\nsecurityContext:\nprivileged: true",
		"tags": [ "note","☁️"]
},

{
		"title": "解決 the gcp auth plugin is deprecated in v1.22+",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/128. 解決 the gcp auth plugin is deprecated in v1.22+/",
		"content": "睡睡念\n每次下kubectl的指令總會跳這個錯誤出來，\n不處理也沒什麼事，但就是都會出現。\n今天...就解決吧\n正文\n警告訊息通常是這樣，\n但你deploy的服務都還是會跑。\n\nWARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.26+; use gcloud instead.\nTo learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke\n\n這是由於docker在安裝的時候，會順便將kubectl安裝進去，\nwhere kubectl\n\n可以得到 kubectl的執行檔位置\n\n/usr/local/bin/kubectl\n\n查詢kubectl的softlink 位置，會查到kubectl的命令是跟docker連結的。\n附註, ll 跟 ls -la 同意\nls -la /usr/local/bin\n\n先將本來的softlink刪除，\nrm /usr/local/bin/kubectl\n\n安裝gcloud版本的kubectl\ngcloud components install kubectl\n\n裝完後如果還不認識指令，\n需要額外處理。\n先檢查一下gcloud有沒有安裝好kubectl，\n如果path有路徑就代表正常，不然需要重新製作softlink\ngcloud info | grep -i kubectl\n\n製作softlink\nln /usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin/kubectl /usr/local/bin/kubectl\n\nref.\n- [實作筆記] 錯誤處理 the gcp auth plugin is deprecated\n- How to set path to kubectl when installed using gcloud components install?",
		"tags": [ "note","☁︎"]
},

{
		"title": "nginx 掛載gcs",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/129. nginx 掛載gcs/",
		"content": "睡睡念\n今天要上QA測試了，然後我突然發現我之前測完沒留下筆記。\n現在都快忘光了，還好最後有找回記憶。\n來補一下了。\n正文\n今天要做的是把GCS變成一個圖片上傳空間，\n可以用nginx來訪問。\n\n前言\n某個案子，要整個翻掉重做，這次要讓VM的機器跟GKE的pod能夠同時存取一個儲存空間，\n預計使用Google Cloud Storage，踩了一個早上的坑。\n正文\n預計做兩個項目\n\n在VM上面掛載gcs的資料夾\n\n在GKE 上面的pod同時掛載 gcs的資料夾\n\n在VM上面掛載gcs的資料夾\n安裝方式，目前我的vm是 RedHat 所以用下面的方式，CentOS也是用同樣的方式\n\nConfigure the gcsfuse repo:\n\nsudo tee /etc/yum.repos.d/gcsfuse.repo &gt; /dev/null &lt;&lt;EOF\n[gcsfuse]\nname=gcsfuse (packages.cloud.google.com)\nbaseurl=https://packages.cloud.google.com/yum/repos/gcsfuse-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\nhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n\nInstall gcsfuse:\n\nsudo yum install gcsfuse\n\n完成，其他安裝方式，請參考GoogleCloudPlatform/gcsfuse/install.md\n登入使用，\n這邊先使用 gcloud auth login 做測試\n先建立資料夾 mkdir upload\n掛載 gcsfuse my-bucket upload\n卸除掛載 fusermount -u upload\nDebug用\ngcsfuse --foreground --debug_gcs --debug_http --debug_fuse --debug_invariants --key-file=/home/user/Downloads/my-key.json mybucket /upload\n\n永久加入GOOGLE_APPLICATION_CREDENTIALS變數\n開啟 /etc/profiles\n新增 export GOOGLE_APPLICATION_CREDENTIALS=&quot;/home/user/Downloads/my-key.json&quot;\n修改檔案後要想馬上生效還要執行#\nsource /etc/profile不然只能在下次重進此使用者時生效。\n\nref.Linux系統環境變數和別名設定（永久生效和臨時生效）\n附註，json檔案的取得在 IAM角色內的服務賬戶\nref.創建和管理服務帳號密鑰。\n新增完GCS，記得把權限給服務賬戶。記得選擇完整權限（Fig. 1)\n自動掛載\n有權限了已後，再來就是自動掛載了。\n到/etc/fstab上，輸入\nmy_bucket /home/ezio/upload gcsfuse key_file=key/key.json,rw,user,allow_other,uid=1008,gid=1009 0 0\n\n重開機測試看看，收工。\nref.\nHow to use mount command in fstab file\ngcsfuse automount on a non root user\n查詢目前登入使用者的uid 跟 gid\nid $(whoami)\n\n如果要看全部使用者的話，\ncat /etc/passwd\n\nref.Linux 的帳號與群組\n\n在GKE 上面的pod同時掛載 gcs的資料夾\n基本的方式，\n先自己產生一個 image\n\nFROM golang:1.14-alpine AS build-env\nENV GO111MODULE on\n\n# WORKDIR /工作名錄名稱 當前的工作目錄名稱，若是不存在則會新建該目錄，\n# 需要注意的是copy跟run的指令都是以WORKDIR為當前目錄下去跑的，\n# 運用的時候需要注意相對位置。\nWORKDIR $GOPATH/src\n\nRUN go get -u github.com/googlecloudplatform/gcsfuse\n\nCOPY key.json .\n\nFROM alpine:3.6\nRUN apk add --no-cache ca-certificates fuse &amp;&amp; rm -rf /tmp/*\nCOPY --from=build-env /go/bin/gcsfuse /usr/local/bin\nCOPY --from=build-env /go/src/key.json /\nWORKDIR /\n\n佈署 Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: gcs-fuse\nversion: v1\nnamespace: debug\nname: gcsfuse-test\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: gcs-fuse\nversion: v1\ntemplate:\nmetadata:\nlabels:\napp: gcs-fuse\nversion: v1\nspec:\ncontainers:\n- name: gcsfuse-test\nimage: gcr.io/your-project/gcsfuse:v1.2\n# image: golang:1.14-alpine\n# command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]\n# args: [ &quot;while true; do sleep 600; done;&quot; ]\nsecurityContext:\nprivileged: true\ncommand:\n- &quot;/bin/sh&quot;\n- &quot;-c&quot;\n- &quot;while true; do mkdir /upload ; gcsfuse --key-file=key.json your-bucket /folder; sleep 600; done; &quot;\n\n如果跟現有的程式整合的話，\n由於要執行多行指令，需使用shellScript的方式執行指令。\n可能會遇到下列情況\n&quot;exec: &quot;/init.sh&quot;: permission denied\n在Dockerfile上修改權限\nRUN chmod +x /init.sh\n\nref.\ngetting permission denied in docker run\nexec user process caused &quot;exec format error&quot;\n有人說在sh的頂端加上\n#!/bin/bash\n但我試了沒用，在猜想可能是alpine linux沒有bash導致\n所以改用下面這個\nENTRYPOINT [&quot;sh&quot;,&quot;/run.sh&quot;]\n\nref. standard_init_linux.go:178: exec user process caused “exec format error”\n執行sh時卡住，debug方式\n用if else檢查\nif mkdir /upload; then\necho &quot;mkdir directory! Success&quot; 1&gt;&amp;2\ngcsfuse --key-file=key.json yellow-video /upload\nelse\necho &quot;Could not mkdir directory!&quot; 1&gt;&amp;2\nexit 1\nfi\n\n另外，最後找出來原因是因為先執行go的程式，導致後續卡住，所以先建立資料夾後，再執行go\nref.\nShell Script 遇到錯誤時自動退出離開\nref.\n身份驗證入門\nLinux系統環境變數和別名設定（永久生效和臨時生效）\n\n這篇有講到如何在deploy掛載GCS\n通常就是要安裝一些東西。\n會使用到gcsfuse這個指令，\n所以我選擇直接建一個image，\n然後掛載的時候直接用那個image就好。\ngcs-download.json 是 要掛載GCS用的權限json檔\nFROM nginx:latest\nWORKDIR /app\n\nCOPY gcs-download.json .\n#Start CloudStorage\nRUN apt update &amp;&amp; apt install -y gnupg lsb-release\nRUN echo &quot;deb https://packages.cloud.google.com/apt gcsfuse-$(lsb_release -c -s) main&quot; | tee /etc/apt/sources.list.d/gcsfuse.list\nRUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\nRUN apt-get update &amp;&amp; apt-get install -y gcsfuse\nENV GOOGLE_APPLICATION_CREDENTIALS=/app/gcs-download.json\n\nWORKDIR /usr/share/nginx/html/files\n\n然後部署的yaml，\n這邊要注意的地方是\n1.\nnginx.conf 要加上 user root ，\n這是因爲gcs掛載資料夾是用root的角色，所以如果nginx不用root啓動，\n會出現403的權限問題。\nref. 四種解決Nginx出現403 forbidden 報錯的方法\n2.\ndaemon off; 這部分指的是要讓nginx能夠在前景執行，不然pod會一直重開。\nref. nginx -g &quot;daemon off;&quot; 你學廢了嗎？\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: systemfile-nginx-config\nnamespace: default\ndata:\nnginx.conf: |\nuser root;\nworker_processes auto;\n\nerror_log /var/log/nginx/error.log notice;\npid /var/run/nginx.pid;\n\nevents {\nworker_connections 1024;\n}\n\nhttp {\ninclude /etc/nginx/mime.types;\ndefault_type application/octet-stream;\n\nlog_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '\n'$status $body_bytes_sent &quot;$http_referer&quot; '\n'&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';\n\naccess_log /var/log/nginx/access.log main;\n\nsendfile on;\n#tcp_nopush on;\n\nkeepalive_timeout 65;\n\n#gzip on;\n\ninclude /etc/nginx/conf.d/*.conf;\n}\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: sms-systemfile-config\nnamespace: default\ndata:\ndefault.conf: |\nserver {\nlisten 80 default_server;\nserver_name _;\nserver_tokens off;\n\nindex index.html;\n\nlocation /files {\nalias /usr/share/nginx/html/files;\nautoindex on;\n}\n\nlocation / {\nroot /usr/share/nginx/html;\nindex index.html;\n}\n\nerror_page 500 502 503 504 /50x.html;\nlocation = /50x.html {\nroot /usr/share/nginx/html;\n}\n}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: systemmanageservice-systemfile\nnamespace: default\nlabels:\ngroup: systemmanageservice\napp: systemfile\nspec:\nreplicas: 1\nrevisionHistoryLimit: 5\nprogressDeadlineSeconds: 15\nselector:\nmatchLabels:\ngroup: systemmanageservice\napp: systemfile\nstrategy:\nrollingUpdate:\nmaxSurge: 25%\nmaxUnavailable: 25%\ntype: RollingUpdate\ntemplate:\nmetadata:\nlabels:\ngroup: systemmanageservice\napp: systemfile\nspec:\ncontainers:\n- name: systemmanageservice-systemfile\nimage: gcs-nginx:0.0.2\ncommand:\n- /bin/bash\n- '-c'\n- &gt;-\ngcsfuse systemfile-qa /usr/share/nginx/html/files &amp;&amp; nginx -g\n&quot;daemon off;&quot;;\nimagePullPolicy: Always # IfNotPresent, Always, Never\nsecurityContext:\nprivileged: true\nports:\n- name: http\ncontainerPort: 80\nprotocol: TCP\nresources: {}\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\nvolumeMounts:\n- name: config-volume\nmountPath: /etc/nginx/conf.d/default.conf\nsubPath: default.conf\nreadOnly: true\n- name: nginx-config-volume\nmountPath: /etc/nginx/nginx.conf\nsubPath: nginx.conf\nreadOnly: true\nvolumes:\n- name: config-volume\nconfigMap:\nname: sms-systemfile-config\nitems:\n- key: default.conf\npath: default.conf\ndefaultMode: 420\n- name: nginx-config-volume\nconfigMap:\nname: systemfile-nginx-config\nitems:\n- key: nginx.conf\npath: nginx.conf\ndefaultMode: 420\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nterminationGracePeriodSeconds: 30\nschedulerName: default-scheduler\nsecurityContext: {}",
		"tags": ["Start", "tcp_nopush", "gzip", "note","☁︎"]
},

{
		"title": "騰訊雲連接GOOGLE雲",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/13.騰訊雲連接GOOGLE雲/",
		"content": "前言\n又在搞新的東西了，這次要把騰訊雲（TKE）跟Google雲（GCP），做VPN對連，\n這次一樣又弄了幾天....最後還是找了GOOGLE工程師協助。\n正文\n下面的步驟，基本上都是TKE跟GCP，互相切換設定。\n\n騰訊雲設定網關IP\n私有網路-&gt; VPN連接-&gt;VPN網關\n\n!<a class=\"internal-link is-unresolved\" href=\"/404\">13.fig-1.jpg</a>\n內容就照想輸入的打，輸入完成後會看到公網IP，為了方便溝通，就稱呼他為TKE的閘道IP。\n\nGCP設定VPN閘道\n這邊的選項比較繁瑣，因為騰訊雲不支援IKEv2，同時也不支援BGP的協定，\n所以Google要使用傳統VPN。\n高可用性的VPN有些要求才能使用，請參考轉為高可用性 VPN\n\n從Fig.3 有連結可以切到 傳統VPN設定，\n再來終於要開始設定GCP閘道IP了。\n建立IP，這邊建立的就是GCP的閘道ip，這組就是要與 TKE的閘道IP 對聯的IP。\n\n其他的選項，\n\n名稱： 自填\n網路： VPN 要連結的 Compute Engine 網路\n區域： 連結至 Compute Engine 網路的閘道所在地區\n\nGCP的通道設定\n\n名稱： 隨意\n遠端對等互連IP： TKE的閘道IP\nIKE版本：這邊選 IKEv1\nIKE預先共用金鑰： 可以自行輸入 或是 由系統產生，但要注意騰訊雲不支援 特殊符號（. / 之類的），\n所以要注意自行產生的金鑰\n導向選項： 這邊選擇 『依據政策』\n遠端網路ip範圍：輸入TKE的私有網路範圍，可以從『騰訊雲的私有網路-&gt;私有網路』 看到 IPv4 CIDR (Fig.6)。\n\nGCP的防火牆設定\n可能需要配置防火牆規則，不確定。\nref. 配置防火牆規則\n4. 騰訊雲的對端網關設定\n私有網路-&gt;VPN連接-&gt;對端網關\n\n名稱： 隨意\n公網IP: 這邊就是輸入 GCP的閘道ip\n標籤： 可以不打\n\n騰訊雲的VPN通道設定\n\n通道名稱：隨意\n地域： 連結至網路的區域\nVPN網關類型： 私有網路。 雲聯網指的是 騰訊雲內部各區域的私有網路互聯(ref.雲聯網產品概述)\n私有網路： 請看(Fig.6)\nVPN網關： 請看(Fig.1)\n對端網關： 請看(Fig.7)\n預共享金鑰： 請看(Fig.5) 的預先共享金鑰\n\nSPD策略\n\n本端網段： 請看(Fig.6)\n對端網段： 請看(Fig.4)的網段\n\nIKE、IPsec配置\n需與下圖一摸一樣。\n\nref. 支持的 IKE 加密方式，此文內(IKEv1 加密方式的第二階段，指的就是 IPsec配置）\nIKE 配置\n\n版本： IKEv1\n加密算法： AES-128\n認證算法： SHA1\n協商模式： main\nDH group: DH2\nIKE SA Lifetime： 36600\n\nIPsec 配置\n\n加密算法：AES-128\n認證算法：SHA1\nPFS： DH-GROUP2\nIPsec sa Lifetime：10800\n\n結尾（不，還有路由要設定）\n\n這邊會先看到VPN的通道狀態是 已聯通， 但如果實際去Ping機器，會發現還是不通。\n\n騰訊雲路由表設定\n私有網路-&gt;路由表-&gt;默認路由表\n\n新增路由策略(Fig.13)\n\n新增路由(Fig.14)\n\n新增路由：這邊是從騰訊雲要跳去GCP的網段，所以這邊要輸入的是（Fig.9)的對端網段\n下一跳類型：選擇VPN網關\n\n結論\n可能有人會想，為什麼騰訊雲要設定路由，\n而GCP不用設定路由。\n因為在設定(Fig.5)的時候，就已經幫你設定好了。\n但如果走的是 依據路徑，那這塊就必須要自行設定了。\n如果想要看路由表，可以到\nGCP-&gt;VPC網路-&gt;路徑\n\nref.\n使用靜態路由創建傳統 VPN\n網絡和隧道路由\n【VPN錦囊】 騰訊雲對接華為雲配置指南\n通過VPN連接到VPC\nMTR：Linux 網路診斷工具使用教學",
		"tags": [ "note","☁︎"]
},

{
		"title": "Google IAP ssh失敗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/130. Google IAP ssh失敗/",
		"content": "睡睡念\n為什麼會發生，俺不知道。\n但好像是好之前硬碟空間不夠，我砍了資料夾的關係...\n正文\n先看個錯誤訊息\n\n權限授權失敗，一般使用ssh連線的話，大部分都是server的ssh key出問題\n可以先參考 [GCP]遇到permission denied (publickey)怎麼辦？\n所以我才回想起，之前我好像去砍過資料夾。\n所以先弄ssh能夠直連GCE的機器，再來細看裏面的狀況。\nSSH解法\n\n建立ssh key\n\nssh-keygen -C &quot;ezio&quot;\n\n這邊使用ezio當作名稱\n\n複製 public key\n將 ~/.ssh/id_rsa.pub的內容複製\n有 .pub的是公鑰 ，沒有的是 私鑰\n\n貼上公鑰\n到GCE的VM執行個體內，將剛剛取得的公鑰貼到 『安全殼層金鑰』\n\n直接用ssh連線\n\n\tssh ezio@32.12.32.12\n\n一般到這邊就結束了，但今天的主題是用IAP無法登入。\n更詳細的ssh 用法，請參考下列網址\nref.\n- ### [Git] 多個SSH Key與帳號的設定(Mac)\n- [教學] 產生SSH Key並且透過KEY進行免密碼登入\nIAP解法\n能連進去了，終於確定裏面沒有我之前的資料夾。\n所以應該是這樣導致無法登入？\n然後查了一下目前的使用者帳號\ncat /etc/passwd | awk -F: '{print $1}'\n\n有我mail的使用者帳號，\n然後刪除！！！\nuserdel -r daimom\n\n再來試試，用IAP登入，看能不能在VM上面重建一個使用者\ngcloud compute ssh --tunnel-through-iap --project sms --zone asia-east1-b test-service01\n\n還是同樣的權限失敗。\n最後，在GCE的設定畫面，將本來的安全殼層金鑰刪掉，強迫重建，\n終於能連了",
		"tags": [ "note","☁︎"]
},

{
		"title": "gsutil整批複製",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/135. gsutil整批複製/",
		"content": "睡睡念\n幫後端改圖片上傳的架構，直接把GCE(Google Compute Engine)的機器改成用GCS（GoogleCloudStorage），所以要把GCE的圖搬到GCS。\n正文\n\n首先要用到的是 <a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/135. gsutil整批複製/\">關閉ssh連線後，仍可執行程式</a> ，不然用gcloud 連線時間過久是會被切斷連線的。\n\n情境 1 整批資料夾\n將nfs資料夾裏面的所有檔案/資料夾複製到bucket裏面，\n-m 是多工，-r 是複製底下資料夾的檔案\ngsutil -m rsync -r nfs gs://systemfile-stage/\n\nref. gsutil rsync\n情境2 ，複製特定資料夾\n需要先建立一個文字檔，將資料夾名稱寫入\nrsync.txt\n\n20220201\n20220301\n20220310\n\nwhile read p;\ndo\nfolder=$(echo &quot;$p&quot;);\ngsutil rsync -r nfs/$p gs://systemfile-stage/$p/\ndone &lt; rsync.txt\n\n情境3，複製特定圖片\n這邊只能根據你的來源隨機應變了，\n我的文件格式為\nkeepfile.csv\n\n&quot;files/20230217/104abc69-e612-4b6c-abe4-52b91b79562f.jpg&quot;,&quot;17/2/2023 19:27:46.79&quot;\n&quot;files/20230217/7a30d9eb-5e7c-4eb8-93c9-a72e5bd74f78.jpg&quot;,&quot;17/2/2023 19:28:47.56&quot;\n&quot;files/20230217/ea81d738-ff0c-416a-b6d9-3520fda8f0ed.jpg&quot;,&quot;17/2/2023 19:32:47.44&quot;\n&quot;files/20230217/7a7e1c75-4bec-4831-9a58-fa9650a82df1.jpg&quot;,&quot;17/2/2023 19:33:24.94&quot;\n&quot;files/20230217/cf495ecb-71a9-4d46-95f4-b051f260523a.jpg&quot;,&quot;17/2/2023 19:33:49.4&quot;\n&quot;files/20230217/eb05163b-bc0f-4e3b-96d5-82b0bb4f3a8d.jpg&quot;,&quot;17/2/2023 19:34:41.463&quot;\n\n所以，要改讀csv的格式，然後將路徑與檔案名稱拆開，\n最後才複製圖片過去。但這個過成真的久，\n如果可以最好是用rsync複製資料夾吧。\n$file的變數，拆解出來有&quot; ，所以又用了<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/133. shell script字串處理/\">133. shell script字串處理</a>的方式，將&quot;刪除。\nwhile IFS=&quot;,&quot; read -r path date\ndo\nfolder=$(echo &quot;$path&quot; | awk -F '/' '{print $2}');\nfile=$(echo &quot;$path&quot; | awk -F '/' '{print $3}');\ngsutil cp -n nfs/$folder/${file%*\\&quot;} gs://systemfile-stage/$folder/\ndone &lt; keepfile-sort.csv\n\nref.\n\nHow to Parse a CSV File in Bash\ngsutil cp",
		"tags": [ "note","☁︎"]
},

{
		"title": "gcs 掛載domain",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/140. gcs 掛載domain/",
		"content": "睡睡念\n這篇文章要完成得部分有\n\n建立LB，連結到GCS\n設定cloud Armor白名單\nCertificate Manager 增加 ssl憑證\n\n正文\n建立負載平衡\n建立負載平衡 -&gt; HTTP(S) 負載平衡\n\n不能用區域的，因為區域的不支援後端值區\n\n前端設定\n這邊建議最好就直接建立一個ip，不要用臨時的。\n如果通訊協定要用https，需要設定憑證，所以這邊先選擇http就好\n\n後端設定\n建立值區\n\n第一個箭頭，選擇你要建立的值區\n第二個箭頭，看要不要建CDN\n第三個箭頭，是設定白名單(cloud Armor)用的，這邊先不設定，等等第二部分會說\n\n主機與路徑規則\n預設會直接進入bucket，這邊要注意，記得要將bucket 設定為公開。\n\n設定cloud Armor\n上一部份有說到設定白名單(cloud Armor)\n網路安全性-&gt; Cloud Armor\n這邊要增加的設定是 Edge 安全性政策，不是後端安全性政策，\n因為只有edge安全性政策支援後端儲存分區\nref. 安全政策的類型。\n\n新增的時候，選擇 Edge安全性政策，如果沒看到這個選項，\n你可能已經進到政策裏面的規則了。\n\n建立完成後，可以從目標去新增要新增的對象，或是到負載平衡那邊做修改。\ncloud armor 目標新增\n\n選擇『負載平衡器後端值區』，再選擇你的bucket\n\n負載平衡器修改\n後端設定 -&gt; 修改\n\n箭頭處選擇剛剛新增的 cloud armor規則\n\nCertificate Manager 新增憑證\nssl憑證，在GCP上面是可以直接幫你管理的。\n雖然不支援 Wildcard SSL Certificate\n\n憑證分成自己管理與google代管，\n自己管理，時間到就要自己上傳新的憑證。\ngoogle管理，不用管，時間到會自己展延，前提是你必須要有該domain。\n選擇google代管的憑證，輸入要管理的domain\n\n儲存後，回到畫面應該會看到正在佈建中(PROVISIONING)，\n這個需要一點時間，最久可能要到60 分\n\n然後到dns伺服器上面綁定domain跟ip了，\n我是使用cloudflare，就不截圖了。\nref. 使用 Google 管理的 SSL 證書\n設定好了以後，回到負載平衡的前端設定\n第一個箭頭，可以選擇跟上面一樣的ip（但你必須先幫他建立）\n第二個箭頭，選擇剛剛新增的ssl憑證，記得一定要新增，\n不然我卡了三個多小時還沒部署完。\n\nref. 部署概覽\n綜合設定\n假設有一個情境是希望domain後面一定要有files才能看圖的話，\n需要用到『進階型主機與路徑規則(第一個箭頭)』\n第二個箭頭設定domain\n第三個箭頭新增新的路徑規則\n\n路徑前置字串重新編寫，將/files轉成 / 訪問bucket\n\n另外要注意，由於每個規則都有一條預設的條件，\n『任何不相符合的項目』，這個可以選擇要不要導轉到其他網站。",
		"tags": [ "note","☁︎"]
},

{
		"title": "gcs自動刪除檔案",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/141. gcs自動刪除檔案/",
		"content": "睡睡念\n本篇的目標，測試用的bucket，每隔一段時間就刪除檔案。\n或是轉到更便宜的bucket。\n先說個概念，一個bucket裏面，他的檔案類型可以是不一樣的。\n正文\n上面那句話不懂的話，直接看圖吧。\n\n由於每個儲存類型的價格不一樣，量大的話還是能省不少錢。\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/126. GCP 硬碟費用比較/\">126. GCP 硬碟費用比較</a>\n到bucket裏面選擇 『生命週期』-&gt;新增規則\n\n動作看要選擇哪一種\n\n選擇條件\n\n要注意的地方是，設定完不是馬上生效，\n\n對儲存桶生命週期組態的更改可能需要長達 24 小時才能生效，在此期間，對象生命週期管理可能仍會根據舊組態執行操作。\n\n設定完後，再來就是等待他執行了。\n另外，如果有多個規則的話，\n目前GCP的優先順序為\n\n如果多個規則同時滿足單個對象的條件，則 Cloud Storage 將基於以下考慮，僅執行與其中一個規則關聯的操作：\n\nDelete 操作優先於任何 SetStorageClass 操作。\n將對象切換至靜態儲存價格最低的儲存類別的 SetStorageClass 操作優先。\n\np.s 如果你也跑去chtGPT提問的話，可能也會碰到我這個答案，但目前看起來GCS沒有標記這個東西了。\n\nref. 對象生命週期管理",
		"tags": [ "note","☁︎"]
},

{
		"title": "gcs cors設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/142.gcs cors設定/",
		"content": "睡睡念\n架構從本來的 NFS全部轉到GCS上了，\n所以cors也要跟着設定\n正文\n將下面檔案，存成json\n[\n{\n&quot;origin&quot;: [&quot;https://tester.abc.com&quot;],\n&quot;method&quot;: [&quot;GET&quot;,&quot;PUT&quot;,&quot;POST&quot;,&quot;HEAD&quot;,&quot;DELETE&quot;,&quot;PATCH&quot;],\n&quot;responseHeader&quot;: [&quot;Content-Type&quot;, &quot;DNT&quot;,&quot;X-CustomHeader&quot;,&quot;X-LANG,Keep-Alive&quot;,&quot;User-Agent&quot;,&quot;X-Requested-With&quot;,&quot;If-Modified-Since,Cache-Control&quot;,&quot;X-Api-Key,X-Device-Id&quot;,&quot;Access-Control-Allow-Origin&quot;],\n&quot;maxAgeSeconds&quot;: 3600\n}\n]\n\n執行，收工。\ngsutil cors set example_cors_file.json gs://example_bucket\n\n但要注意，這個不會馬上生效，感覺要等超過10分鐘以上。\n~~補充，origin不能用 * 的萬用符號，之前測了沒有效果 ，\n未來會不會改不清楚。 ~~\n測試時，可以用* ，直接用 origin:*\n查看bucket有沒有設定cors則用\ngsutil cors get gs://example_bucket\nor\ngcloud storage buckets describe gs://example_bucket --format=&quot;default(cors_config)&quot;\n\n如果要用gcloud/api，請看下面連結\nref. 設定和查看 CORS 組態\nCors的工作原理\n當瀏覽器向 Cloud Storage 發出簡單請求時，會發生以下過程：\n\n瀏覽器將 Origin 標頭新增到請求中。Origin 標頭包含相應資源（該資源在尋求共享 Cloud Storage 儲存桶的資源）的來源，例如 Origin:https://www.example.appspot.com。\n\nCloud Storage 將請求的 HTTP 方法以及 Origin 標頭的值與目標儲存桶的 CORS 組態中的方法和來源資訊進行比較，以查看是否存在匹配項。如果存在匹配項，Cloud Storage 將在響應中包含 Access-Control-Allow-Origin 標頭。Access-Control-Allow-Origin 標頭包含初始請求的 Origin 標頭的值。\n\n瀏覽器接收響應並檢查 Access-Control-Allow-Origin 值是否與原始請求中指定的網域匹配。如果它們匹配，則請求成功。如果它們不匹配，或者響應中不存在 Access-Control-Allow-Origin 標頭，則請求失敗。",
		"tags": [ "note","☁︎"]
},

{
		"title": "GKE Pub/Sub通知訊息到DC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/161. GKE利用pubsub通知訊息到discord/",
		"content": "前言\n本來以爲不需要，但GKE有時就會幫你升級master的版本，\n神不知鬼不覺，就算用靜態版本也是。\n雖然是不會影響到線上服務，\n但還是最好知道一下是什麼時候升級的。\n正文\n\nPub/Sub建立一個主題\n\nGKE的叢集設定開啓通知，並選擇剛剛建立的主題\n\n建立Cloud Function\n\n環境選『第二代』，函式名稱自己命名。\n觸發條件，『允許未經驗證的叫用』，再選擇 ADD TRIGER裏面的『Pub/Sub trigger』\n\n選擇剛剛建立的Pub/Sub主題\n\n在連線的地方，可選擇僅允許內部流量即可。\n\n建立程式碼\n\n選擇Go 1.20 ，進入點改成 sendDiscord\n原始碼到pubsub notify複製 function.go的程式。\n\n測試\n\n觸發GKE的升級後，就會發送訊息到Discord上面。\n\n相關資訊\nPub/Sub價格\n在每個日曆月，結算帳號的 Message Delivery Basic SKU 的前 10 GiB 吞吐量免費\nref. Pub/Sub 價格\nPub/Sub架構\n\nref. 什麼是 Pub/Sub？\nThe request was not authenticated.\n\nThe request was not authenticated. Either allow unauthenticated invocations or set the proper Authorization header. Read more at https://cloud.google.com/run/docs/securing/authenticating Additional troubleshooting documentation can be found at: https://cloud.google.com/run/docs/troubleshooting#unauthorized-client\n\n如果沒在 第三步驟，選擇『允許未經驗證的叫用』，事後需改到cloud run修改安全性。\n\n從cloud function的連結到cloud run\n\n到安全性，將驗證改成『允許未經驗證的叫用』。\n如果你仍然想要用需要驗證，\n可能需要研究下，目前只允許內部流量調用。\nMessagePublishedData\npub/sub 事件的組成部分\n{\n&quot;subscription&quot;: &quot;projects/test-project/subscriptions/my-subscription&quot;,\n&quot;message&quot;: {\n&quot;attributes&quot;: {\n&quot;attr1&quot;:&quot;attr1-value&quot;\n},\n&quot;data&quot;: &quot;dGVzdCBtZXNzYWdlIDM=&quot;,\n&quot;messageId&quot;: &quot;message-id&quot;,\n&quot;publishTime&quot;:&quot;2021-02-05T04:06:14.109Z&quot;,\n&quot;orderingKey&quot;: &quot;ordering-key&quot;\n}\n}",
		"tags": [ "note","☁️"]
},

{
		"title": "使用bash安裝gcloud cli",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/164.gcloud install by bash/",
		"content": "前言\n因爲『懶』，所以要來搞個一個sh就能安裝完所有東西。\n但在install gcloud就踩坑了。\n正文\n官方文件，這邊提到正常的安裝方式，\n./google-cloud-sdk/install.sh\n\n但這個是建立在人要在電腦前面輸入Y，於是改成\n./google-cloud-sdk/install.sh --command-completion=true --path-update=true --quiet\n\n這樣才會在.bashrc 增加執行的參數。\n一開始沒有想到是沒有增加 .bashrc的關係，\n只有一直出現找不到gcloud的錯誤，\n最後確認<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/163.Shell的區別及查詢/\">163.Shell的區別及查詢</a>的關係後，\n才釐清是.bashrc沒加，\n於是利用\n./google-cloud-sdk/install.sh --help\n\n查詢有哪些參數可供使用。\n完整shell script\n完整install.sh如下，需使用root執行，不然在安裝gcloud會出錯，\nupdate-cert.sh是另一個shell script，是用gsutil拿來抓gcs的資料用。\ncurl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-443.0.0-linux-x86_64.tar.gz\ntar -xf google-cloud-cli-443.0.0-linux-x86_64.tar.gz\n./google-cloud-sdk/install.sh --command-completion=true --path-update=true --quiet\nsource ~/.bashrc\n\ngcloud auth activate-service-account view-bucket@project.iam.gserviceaccount.com --key-file=view-bucket.json\n\nSCRIPTPATH=$(dirname &quot;$(readlink -f $0)&quot;)\nchmod u+x update-cert.sh\n\necho &quot;30 1 * * 1 root /bin/bash ${SCRIPTPATH}/update-cert.sh&quot; &gt;&gt; /etc/crontab\n\nref.\n\ngcloud command not found - while installing Google Cloud SDK\ninstall Google Cloud CLI\n[Shell Script] 執行sh檔案提示Permission denied\n第十五章、例行性工作排程(crontab)",
		"tags": [ "note","☁️"]
},

{
		"title": "Gcloud AttributeError module importlib has no attribute util",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/17. Gcloud AttributeError module importlib has no attribute util/",
		"content": "前言\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/16.dustise sleep 測試工具簡介/\">16.dustise sleep 測試工具簡介</a> 上一篇，有講到httpie這個套件，簡單使用，我就直接在mac上裝了。\n然後，gcloud,kubectl就掛了...\n正文\n錯誤碼是這樣，\nAttributeError: module 'importlib' has no attribute 'util'\n\n後來一查，發現是kubectl 跟 python 3.9 的問題AttributeError: module 'importlib' has no attribute 'util' #970\n本來是想直接升級kubectl就好在 macOS 上使用 Homebrew 安裝，\nbrew upgrade kubectl\n\n雖然升級完成了，\n但查詢版本\nkubectl version --client\n出來的版本仍是舊的。\n後來才想到，我的kubectl 好像是跟gcloud 一起裝的，試了一下，連gcloud也是同樣的錯誤。\n那就來升級gcloud吧。但只要輸入 gcloud components update 也是出現同樣的錯誤，那要怎麼升級...\n所以，根據這篇 先在terminal上面輸入，\nexport CLOUDSDK_PYTHON=python2 # 先指定使用其他python版本\ngcloud components update # 更新gcloud\n\n完成。",
		"tags": ["970", "note","☁︎"]
},

{
		"title": "Debian install",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/181. cloudstorage無法使用cloudflare轉址/",
		"content": "前言\n線上的圖片是使用GCP的Load balancer連到Google cloud storage，\n之前測試沒問題，但後來 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/186. istio的Authorization policy(白名單)/#cloudflare\">186. istio的Authorization policy(白名單)#cloudflare轉發</a> ，\nclient到origin server 都使用ssl連線後，就掛了，\n因為Load balancer當初沒有設定SSL的憑證，導致無法連線。\n正文\n到GCP的憑證管理器，新增憑證。\n一開始是沒有啓用的，必須先啓用\n\n建立新憑證，不支援萬用字元\n\n這邊注意，建立完後，\n可能需要等一陣子(幾小時）才會好，\n建立完後一定要到LB綁定，呈現有使用者的狀態。\n當好的時候，會看到狀態為『有效』。\n此時，再到Load Balancer那邊設定憑證，開啓https的入口即可。\n\n前面步驟，參考 <a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/140. gcs 掛載domain/\">140. gcs 掛載domain</a>",
		"tags": [ "note","☁️"]
},

{
		"title": "GCE硬碟增加容量大小",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/19. GCE硬碟增加容量大小/",
		"content": "前言\n之前有加大過GCE的CPU跟ram，但怎麼加硬碟空間還真的不知道。\n正文\n\n先到GCE，開啟機器。選擇開機磁碟（可在開機的狀態下運作）\n\n按下編輯，修改硬碟大小，儲存。\n\n重新整理畫面，會看到硬碟空間容量變了。但還沒完成，需要到機器裡面重新調整大小。\n\nlsblk 列出可用設備\nlsblk\n\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\nsda 8:0 0 35G 0 disk\n└─sda1 8:1 0 30G 0 part /\n\nNAME：這是塊設備名。\nMAJ:MIN：本欄顯示主要和次要設備號。\nRM：本欄顯示設備是否可移動設備。注意，在設備sdb和sr0的RM值等於1，這說明他們是可移動設備。\nSIZE：本欄列出設備的容量大小信息。例如35G表明該設備大小為35GB，而1K表明該設備大小為1KB。\nRO：該項表明設備是否為只讀。在本案例中，所有設備的RO值為0，表明他們不是只讀的。\nTYPE：本欄顯示塊設備是否是磁盤或磁盤上的一個分區。在本例中，sda和sdb是磁盤，而sr0是只讀存儲（rom）。\nMOUNTPOINT：本欄指出設備掛載的掛載點。\nref. lsblk命令\n\ngrowpart 將全部多餘的空間給指定的分割區\nextend a partition\tin a partition table to\tfill available space\nubuntu 安裝方式：\napt -y install cloud-guest-growpart\n\n2024/08/15更新\napt -y install cloud-guest-utils\nCentOS 安裝方式：\nyum -y install cloud-utils-growpart\ngrowpart /dev/sda 1\n\n這邊的dev/sda 指的是disk,故sda與 1 中間有空格\nref. growpart\np.s 如果出現 錯誤\nFAILED: sfdisk not found\n\n請使用sudo growpart /dev/sda 1 的方式執行\n或 apt install -y fdisk\n\nresize2fs 調整系統大小 resize2fs /dev/sda1\n\n這邊的dev/sda1 指的是part ,故sda與 1 中間沒有空格\nref. Linux基礎命令---resize2fs\n\ndf 檢查分割區大小df -h\n\nref.\ngcp 增加硬碟容量\n不關機、不拔硬碟擴充VM root 磁碟區 - growpart\n調整磁碟區大小後擴展 Linux 檔案系統\nLinux 擴展ext2/3/4 xfs root分割區不用LVM\nTroubleshooting\n\n如果使用resize2fs 碰到錯誤訊息\n\nresize2fs 1.45.6 (20-Mar-2020)\nresize2fs: Bad magic number in super-block while trying to open /dev/sda2\nCouldn't find valid filesystem superblock.\n\n表示你使用的可能是xfs系統，來查一下\n\n\tmount \t\n\n所以必須改用下面指令\nxfs_growfs /dev/sda2\n\nref. # xfs檔案系統擴容報resize2fs: Bad magic number in super-block while trying to open /dev/centos/root Couldn’t find valid filesystem superblock",
		"tags": [ "note","☁︎"]
},

{
		"title": "GCE機器列表內的CPU個數",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/46.GCE機器列表內的CPU個數/",
		"content": "前言\n要算一下各服務所佔的比例，\n大方向用VM的機器 cpu來算\n正文\n如果用gcloud compute list 能夠列出機器的類型，但裡面的cpu數量還是不知道。\n找了一下發現有人寫好一個shell script，拿來改一下能夠直接列出cpu數量\n跟連結的內容相比，我這邊多加了一個狀態。\n因為有些機器是不開機的，當有任務時，才會開機，故大部分時間都是關機狀態。\n# Get instance name,zone for `${PROJECT}\nfor PAIR in $(\\\ngcloud compute instances list \\\n--project=${PROJECT} \\\n--format=&quot;csv[no-heading](name,zone.scope(zones),status)&quot;)\ndo\n# Parse result from above into instance and zone vars\nIFS=, read INSTANCE ZONE STATUS &lt;&lt;&lt; ${PAIR}\n# Get the machine type value only\nMACHINE_TYPE=$(\\\ngcloud compute instances describe ${INSTANCE} \\\n--project=${PROJECT} \\\n--zone=${ZONE} \\\n--format=&quot;value(machineType.scope(machineTypes))&quot;)\n# If it's custom-${vCPUs}-${RAM} we've sufficient info\nif [[ ${MACHINE_TYPE} == custom* ]]\nthen\nIFS=- read CUSTOM CPU MEM &lt;&lt;&lt; ${MACHINE_TYPE}\nprintf &quot;%s: vCPUs: %s; Mem: %s; Status: %s\\n&quot; ${INSTANCE} ${CPU} ${MEM} ${STATUS}\nelse\n# Otherwise, we need to call `machine-types describe`\nCPU_MEMORY=$(\\\ngcloud compute machine-types describe ${MACHINE_TYPE} \\\n--project=${PROJECT} \\\n--zone=${ZONE} \\\n--format=&quot;csv[no-heading](guestCpus,memoryMb)&quot;)\nIFS=, read CPU MEM &lt;&lt;&lt; ${CPU_MEMORY}\nprintf &quot;%s: vCPUs: %s; Mem: %s; Status: %s\\n&quot; ${INSTANCE} ${CPU} ${MEM} ${STATUS}\nfi\ndone\n\nref. gcloud command to display vCPU's and Memory assigned to Instances",
		"tags": [ "note","☁︎"]
},

{
		"title": "列出GCE的所有VM",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/46.列出GCE的所有VM/",
		"content": "前言\n正文\n如果要查特定的欄位，可以先列出instances 的結構，\n看一下要什麼樣的資料。\ngcloud compute instances describe gke-xxxxxx-test-n2-8-819a6098-3b3r --zone=asia-east1-b\n\n然後就能湊出下面的語法了\ngcloud compute instances list --format=&quot;table(\nname,\nzone.basename(),\nnetworkInterfaces[].ipv6AccessConfigs[0].externalIpv6.notnull().list():label=EXTERNAL_IPV6,\nnetworkInterfaces[].subnetwork)&quot; | sort -k 5 | grep -E 'gateway.*test'\n\n最前面的語法，是要取出子網路的名稱，以便計算該子網路內有幾台VM，\n然後排序、比對 名稱裡面有 gateway而且後面是 test的VM。\nref.\ngrep 指令使用 or 及 and 查兩個條件以上\ngcloud compute instances describe\ngcloud compute instances list",
		"tags": [ "note","☁︎"]
},

{
		"title": "Google Cloud Function限制",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/5. Google Cloud Function限制/",
		"content": "前言\n這次要生一個ffmpeg的環境出來，然後找阿找阿，\n找到一篇文章，上面寫\nGoogle將Cloud Functions作業系統升級到了Ubuntu 18.04 LTS，擴大了可用函式庫的範圍，除了Imagemagick一直都有外，還多了Ffmpeg和Libcairo2系統函式庫，連無頭Chrome也有，使用者不只可以在Cloud Functions中處理影片，甚至還能進行網頁截圖。\n\nref Google事件驅動無伺服器平臺Cloud Functions正式上線了!\n然後不用架機器，就來看看這條路行不行的通吧\n正文\n找了一陣子，先注意到的是GCF的限制\n\ntimeout 預設1分鐘，最長9分鐘\n單一function可用memory為2G，超過會中斷\n這兩個是我認為比較重要的部分，其他細項，\n可參考\n配額和限制\n淺談Serverless Solution — 以GCP Cloud Function為例\n\n結論，拿來做影片轉檔的話...應該是沒辦法了。除非這影片夠小。\n不然只寫api看來是挺快的",
		"tags": [ "note","☁︎"]
},

{
		"title": "gcloud compute instance 筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/79. gcloud compute instance 筆記/",
		"content": "前言\n因為公司政策，\n要把label全部統一，之前有些已經命名過一次的但名稱不對，\n一個一個改又好麻煩，\n來寫command吧\n正文\n指令演化順序\nStep 1.\n先用指令撈出 GCE的全部列表\ngcloud compute instances --project=project list --format='table(name,status,labels.list())'\n\nStep 2.\n找出label只有特定字元的列表\ngcloud compute instances --project=project list --format='table(name,status,labels.list())' | grep product=abc\n\n問題來了，如果我只要取name出來，作爲變數的話，\n要怎麼做？\n我也一下子想不出來，就先跑去看一下gcloud文件了。\nStep 3.\ngcloud compute instances --project=project list --filter=&quot;labels.product:abc&quot; --format=&quot;table(name)&quot;\n\n改用 filter去篩選，這樣就能只顯示名稱了。\n\nStep 4.\n加上for迴圈，將label.product改成 zzz\nfor name in $(gcloud compute instances --project=project list --filter=&quot;labels.product:abc&quot; --format=&quot;table(name)&quot;); do\ngcloud compute instances update &quot;$name&quot; --update-labels product=zzz\ndone\n\n附註：\n刪除label的指令\nproduct 為 key值\ngcloud compute instances update beta-api01 --remove-labels product\n\nref.\n\n使用標籤整理資\ngcloud command",
		"tags": [ "note","☁︎"]
},

{
		"title": "GCS使用方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/8.GCS使用方式/",
		"content": "前言\n某個案子，要整個翻掉重做，這次要讓VM的機器跟GKE的pod能夠同時存取一個儲存空間，\n預計使用Google Cloud Storage，踩了一個早上的坑。\n正文\n預計做兩個項目\n\n在VM上面掛載gcs的資料夾\n\n在GKE 上面的pod同時掛載 gcs的資料夾\n\n在VM上面掛載gcs的資料夾\n安裝方式，目前我的vm是 RedHat 所以用下面的方式，CentOS也是用同樣的方式\n\nConfigure the gcsfuse repo:\n\nsudo tee /etc/yum.repos.d/gcsfuse.repo &gt; /dev/null &lt;&lt;EOF\n[gcsfuse]\nname=gcsfuse (packages.cloud.google.com)\nbaseurl=https://packages.cloud.google.com/yum/repos/gcsfuse-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\nhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n\nInstall gcsfuse:\n\nsudo yum install gcsfuse\n\n完成，其他安裝方式，請參考GoogleCloudPlatform/gcsfuse/install.md\n登入使用，\n這邊先使用 gcloud auth login 做測試\n先建立資料夾 mkdir upload\n掛載 gcsfuse my-bucket upload\n卸除掛載 fusermount -u upload\nDebug用\ngcsfuse --foreground --debug_gcs --debug_http --debug_fuse --debug_invariants --key-file=/home/user/Downloads/my-key.json mybucket /upload\n\n永久加入GOOGLE_APPLICATION_CREDENTIALS變數\n開啟 /etc/profiles\n新增 export GOOGLE_APPLICATION_CREDENTIALS=&quot;/home/user/Downloads/my-key.json&quot;\n修改檔案後要想馬上生效還要執行#\nsource /etc/profile不然只能在下次重進此使用者時生效。\n\nref.Linux系統環境變數和別名設定（永久生效和臨時生效）\n附註，json檔案的取得在 IAM角色內的服務賬戶\nref.創建和管理服務帳號密鑰。\n新增完GCS，記得把權限給服務賬戶。記得選擇完整權限（Fig. 1)\n自動掛載\n有權限了已後，再來就是自動掛載了。\n到/etc/fstab上，輸入\nmy_bucket /home/ezio/upload gcsfuse key_file=key/key.json,rw,user,allow_other,uid=1008,gid=1009 0 0\n\n重開機測試看看，收工。\nref.\nHow to use mount command in fstab file\ngcsfuse automount on a non root user\n查詢目前登入使用者的uid 跟 gid\nid $(whoami)\n\n如果要看全部使用者的話，\ncat /etc/passwd\n\nref.Linux 的帳號與群組\n\n在GKE 上面的pod同時掛載 gcs的資料夾\n基本的方式，\n先自己產生一個 image\n\nFROM golang:1.14-alpine AS build-env\nENV GO111MODULE on\n\n# WORKDIR /工作名錄名稱 當前的工作目錄名稱，若是不存在則會新建該目錄，\n# 需要注意的是copy跟run的指令都是以WORKDIR為當前目錄下去跑的，\n# 運用的時候需要注意相對位置。\nWORKDIR $GOPATH/src\n\nRUN go get -u github.com/googlecloudplatform/gcsfuse\n\nCOPY key.json .\n\nFROM alpine:3.6\nRUN apk add --no-cache ca-certificates fuse &amp;&amp; rm -rf /tmp/*\nCOPY --from=build-env /go/bin/gcsfuse /usr/local/bin\nCOPY --from=build-env /go/src/key.json /\nWORKDIR /\n\n佈署 Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: gcs-fuse\nversion: v1\nnamespace: debug\nname: gcsfuse-test\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: gcs-fuse\nversion: v1\ntemplate:\nmetadata:\nlabels:\napp: gcs-fuse\nversion: v1\nspec:\ncontainers:\n- name: gcsfuse-test\nimage: gcr.io/your-project/gcsfuse:v1.2\n# image: golang:1.14-alpine\n# command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]\n# args: [ &quot;while true; do sleep 600; done;&quot; ]\nsecurityContext:\nprivileged: true\ncommand:\n- &quot;/bin/sh&quot;\n- &quot;-c&quot;\n- &quot;while true; do mkdir /upload ; gcsfuse --key-file=key.json your-bucket /folder; sleep 600; done; &quot;\n\n如果跟現有的程式整合的話，\n由於要執行多行指令，需使用shellScript的方式執行指令。\n可能會遇到下列情況\n&quot;exec: &quot;/init.sh&quot;: permission denied\n在Dockerfile上修改權限\nRUN chmod +x /init.sh\n\nref.\ngetting permission denied in docker run\nexec user process caused &quot;exec format error&quot;\n有人說在sh的頂端加上\n#!/bin/bash\n但我試了沒用，在猜想可能是alpine linux沒有bash導致\n所以改用下面這個\nENTRYPOINT [&quot;sh&quot;,&quot;/run.sh&quot;]\n\nref. standard_init_linux.go:178: exec user process caused “exec format error”\n執行sh時卡住，debug方式\n用if else檢查\nif mkdir /upload; then\necho &quot;mkdir directory! Success&quot; 1&gt;&amp;2\ngcsfuse --key-file=key.json yellow-video /upload\nelse\necho &quot;Could not mkdir directory!&quot; 1&gt;&amp;2\nexit 1\nfi\n\n另外，最後找出來原因是因為先執行go的程式，導致後續卡住，所以先建立資料夾後，再執行go\nref.\nShell Script 遇到錯誤時自動退出離開\nref.\n身份驗證入門\nLinux系統環境變數和別名設定（永久生效和臨時生效）",
		"tags": [ "note","☁︎"]
},

{
		"title": "批次修改GCP的label",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/88.批次修改GCP的label/",
		"content": "前言\n發現有一整批的資源沒有加過 label ，\nGCE的硬碟、負載平衡的前端轉導規則..\n有些條件不一樣，一個一個加會瘋掉。\n工程師就是懶...寫sh吧\n正文\n開始構思\n\n先把關鍵值拉出來 (1)\n\ngcloud compute forwarding-rules list\n\n只有ip，要跑去每個叢集查資料，看這該歸屬哪個label，\n太笨了，不行。\n先看一下forwarding-rules 有哪些資訊(2)，再把欄位顯示出來。\ngcloud compute forwarding-rules describe a0589a367cdc1470e839b05803958e2b\n\n裡面的description似乎蠻有用處的，可以幫忙釐清是哪個cluster\n\n用 --format (3)，顯示欄位\ngcloud compute forwarding-rules list --format='table(name,IPAddress,description)'\n\n開始寫bash\n\n將name丟進 文字檔，\n然後執行bash，bash的流程，迴圈讀檔案內容(4)，執行更新label指令(5)。這邊要注意，如果是ingress或是區域型的要額外加上 --global，\n才能更新。\nwhile read p; do\necho &quot;$p&quot;; gcloud compute forwarding-rules update $p \\\n--update-labels=product=abc,env=prod,dept=rd; echo &quot;done&quot;;\ndone &lt;lbfr.txt\n\nref.\n\ngcloud compute forwarding-rules list\ngcloud compute forwarding-rules describe\ngcloud topic formats\nhttp://www.compciv.org/topics/bash/loops/\ngcloud compute forwarding-rules update",
		"tags": [ "note","☁︎"]
},

{
		"title": "GCP網路對外的方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/9.GCP網路對外的方式/",
		"content": "前言\n這算是先前一直有問題，但不知道為什麼，也沒去查原因。\n正文\n為了要正確對應，讓其他機器開白名單，不得不弄清楚，到底怎麼一回事。\n之前同事都說是走cloudNat，但我直接連去 vm or pod裡面，\n用curl ifconfig.me 抓到的ip都不是我所知道的ip，\n(ifconfig.me 是一個網址，單純顯示目前連去網站的ip位置)\nCloudNAT 的基本概念，主要是可提供內部機器對外連線\nref. Cloud NAT 概覽\n基本的設定方式，\nref. 使用 Cloud NAT\nGCE沒有問題，基本上沒有外部ip而且又沒設定cloudNAT的話，就是無法連接外網。\n然而有外部ip位置的話，Google Cloud 會自動對來源與該接口的主內部 IP 地址匹配的數據包執行一對一 NAT。\n但GKE就有點問題了，就算沒有設定外部ip，還是能夠對外，\n而且抓到的ip位置，你不知道是從何而來，也不是CloudNAT的IP\n最後從問題排查這邊找到了答案，\n確保您的 Google Kubernetes Engine (GKE) 集群是一個專用集群。非專用集群中的每個節點虛擬機都具有外部 IP 地址，因此每個節點都可以使用 Virtual Private Cloud (VPC) 網絡中下一個躍點為默認互聯網網關而不依賴 Cloud NAT 的路由。\n\n簡單說，如果沒有設定專有集群（私有叢集），本身的node是具有對外的ip，所以還沒到CloudNAT之前，\n就直接從node節點出去了。這部分可以到pod裡面 抓取對外ip，然後去比對，這個pod所在的node的對外ip就知道了。\n建立私人叢集\n2021/04/19 更新\n今天來算一下數學，\n上面有提到，當設成私有叢集或是VM，需要對外連線時，\n需設定cloud nat ip，\n當對外連線數量不足時，會發生許多奇怪的問題，\n例如，佈署 deployment ，會發生image pull error ，pod無法啟動，一直重開...等等。\n[[9-fig.1.jpg]]\n每個 VM 執行個體的最低通訊埠數量 ：64\n表示每台VM可以對外的連線數量。\n假設(fig.1)\n對外的ip有兩個，\n一個對外ip可以提供 64512的 port 給裡面的來源子網路與IP使用，\n而我們上面設定 每個 VM 執行個體的最低通訊埠數量 為 64 ，\n故 64512 * 2/64 = 2016 可提供的VM數量\n表示 來源子網路與 IP 範圍 的VPC網路 可以提供給2016 個VM （包含 GCE 跟 GKE的節點） 使用。\nref. Cloud NAT 地址和端口概覽",
		"tags": [ "note","☁︎"]
},

{
		"title": "cloud build的問題",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/95.cloud build的問題/",
		"content": "前言\n正文\n在建置cloud build時遇到問題，\n經查詢後推測問題在於，\ncloud build的google service當他在運作過程，\n需要向私有專案內的gke cluster操作，\ncloud build是使用google的浮動ip，\n在此處應該是被block掉了所以無法完成cloud build建置\n錯誤訊息：Unable to connect to the server: dial tcp 124.111.27.75:443: i/o timeout\n(IP為cluster端點ip)\nref. google錯誤排解\n現階段有三種方式\n\n自架CICD系統\nGKE 直接開放 0.0.0.0/0 &gt; 這有安全疑慮\n使用 private conn &gt; 這複雜度很高且不彈性",
		"tags": [ "note","☁︎"]
},

{
		"title": "Anthos 初體驗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/97. Anthos 初體驗/",
		"content": "前言\n正文\n\n每個月增加 50 美\ncreate new cluster ,無法更改現有(Anthos)\n只能用workload identity\ncpu至少四個，total 要八個\nAnthos 跟 Anthos Service Mesh 不一樣\nref. Anthos 與 Anthos Service Mesh 界面差異\n\n如果在mac上，參照文件，執行的話會出現一些錯誤。\n\nmac不支援此指令，\n也不要使用cloudShell，會因為VPCSC 而出現錯誤\n最佳解法，使用linux系統\n\ndocker run -it --rm -v &quot;$HOME&quot;/.config:/root/.config -v /Users/ezio_liu/Documents/kill:/data google/cloud-sdk:latest\n\n因為有將 gcp驗證 掛載到image內，所以不用在驗證一次。\n可以使用下面指令，看人員有沒有驗證\ngcloud auth list\n\n最後，使用下面指令安裝\n./asmcli install \\\n--project_id rd7-project \\\n--cluster_name istio-test \\\n--cluster_location asia-east1-b \\\n--fleet_id rd7-project \\\n--output_dir asmcli_output \\\n--managed \\\n--ca mesh_ca\n\n--managed 是使用 代管式anthos",
		"tags": [ "note","☁︎"]
},

{
		"title": "GCP 硬碟費用比較",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/Price/126. GCP 硬碟費用比較/",
		"content": "睡睡念\n正文\nGCE\n\nStandard : $0.04 per GiB\nSSD : $0.17 per GiB\n\nCloudStorage\n\nStandard ：適合短期儲存和經常存取的資料\nNearline ： 適用於備份項目和一個月存取不到一次的資料\nColdline：最適合用於災難復原，以及儲存「每季存取少於一次」的資料\nArchive ：適合以數位方式長期保存一年存取不到一次的資料\n\n下面為每月每GB的空間費用\n| | Standard 儲存空間 | Nearline 儲存空間 | Coldline 儲存空間 | 封存儲存空間 | |\n|-----------------|---------------|---------------|---------------|------------|\n| 台灣 (asia-east1) | $0.020 美元 | $0.010 美元 | $0.005 美元 | $0.0015 美元 |\n如果有網路流量，額外在算\n| 每月用量 | 輸出至世界各地 (亞洲和澳洲除外) | 輸出至亞洲地區 (中國除外，但含香港) | 輸出至中國地區 (香港除外) | 輸出至澳洲地區，從位於澳洲的 Cloud Storage 區域輸出 | 輸入 | | | | |\n|-----------|-------------------|---------------------|----------------|-----------------------------------|----|\n| 0 至 1 TB | $0.12 美元 | $0.12 美元 | $0.23 美元 | $0.19 美元 | 免費 |\n| 1 至 10 TB | $0.11 美元 | $0.11 美元 | $0.22 美元 | $0.18 美元 | 免費 |\n| 10 TB 以上 | $0.08 美元 | $0.08 美元 | $0.20 美元 | $0.15 美元 | 免費 |\nref. 價目表",
		"tags": [ "note","☁︎"]
},

{
		"title": "GCP 監控費用",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/Price/180. GCP 監控費用/",
		"content": "正文\n\n功能\n價格1\n每月免費配額\n\nLogging 儲存*\n$0.50/GiB； 將日誌流式傳輸到日誌儲存桶儲存空間，以便編入索引、查詢和分析的一次性費用；包括日誌儲存桶中長達 30 天的儲存費用。 查詢和分析日誌資料不會產生額外費用。\n每個項目每月的前 50 GiB\n\nLogging 保留†\n對於保留超過 30 天的日誌，每月每 GiB 為 $0.01；根據留存率按月結算。\n在默認保留期限內保留的日誌不會產生保留費用。\n\n日誌記錄入站流量‡\n無需額外費用\n不適用\n\n日誌路由器♦\n無需額外費用\n不適用\n\nLog Analytics♣\n無需額外費用\n不適用\n\nLogging保留\nLogging保留的費用，在默認保留的天數為以下\n\n日誌儲存桶\n默認保留期限\n自訂保留期限\n\n_Required\n400 天\n不可組態\n\n_Default\n30 天\n可組態\n\n使用者定義\n30 天\n可組態\n\n_Required 日誌儲存桶\nCloud Logging 會自動將以下類型的日誌路由到 _Required 儲存桶：\n\n管理員活動稽核日誌\n系統事件稽核日誌\nGoogle Workspace 管理員稽核日誌\n企業版群組稽核日誌\n登錄稽核日誌\nAccess Transparency 日誌。如需瞭解如何啟用 Access Transparency 日誌，請參閱 Access Transparency 日誌文件。\n\n_Default 日誌儲存桶\n除非您停用或以其他方式修改 _Default 接收器，否則未儲存在 _Required 儲存桶中的任何日誌條目都會被 _Default 接收器路由到 _Default 儲存桶。如需瞭解如何修改接收器，請參閱管理接收器。\nCloud Logging 會自動將以下類型的日誌路由到 _Default 儲存桶：\n\n資料訪問稽核日誌\n政策拒絕稽核日誌\n\n除非您為 _Default 儲存桶組態自訂保留，否則 Cloud Logging 會將日誌保留 30 天。\nref.\n\nGoogle Cloud 的維運套件價格\n日誌保留期限\n路由和儲存概覽",
		"tags": [ "note","☁️"]
},

{
		"title": "GCP CPU用途",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/Price/GCP CPU用途/",
		"content": "<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/Price/GCP CPU用途/#2024-07-09\">#2024/07/09更新</a>\n官方建議\n下表是官方建議的機器型號及用途\nref.通用機器系列\nE2\n\nWeb 服務\n中小型資料庫\n應用提供服務\n後台應用\n微服務\n虛擬桌面\n開發環境\n\nN2、N2D、Tau T2D、N1\n\nWeb 服務\n大中型資料庫\n應用提供服務\n後台應用\n快取\n媒體/流式傳輸\n\nTau T2A\n\n應用服務、Web 服務和遊戲服務\n嵌入式系統開發\nArm 上的 CI/CD 開發\n視訊和圖片編碼、轉碼和處理\n數字廣告交易和投放\n快取伺服器\n計算型藥品研發\nAndroid 開發\n自動駕駛或傳統汽車軟體開發\n\n其他 N2D 跟 Tau T2D都是AMD的CPU，價格比N2的便宜\n以同樣的 4 cpu , 16GB memory 來比較價格的話，\n改天要開到N2的話，可以試試N2D的機器。\n\n類型\nCPU\n記憶體\n價格\n\nn2-standard-4\n4\n16GB\n$0.1942\n\nn2d-standard-4\n4\n16GB\n$0.1690\n\ne2-standard-4\n4\n16GB\n$0.13402\n\n更詳細的價格，參考 虛擬機器實例價格\n2024/07/09更新\n今天來算數學了，\n現在要來比較N2D與C2D的價格差距\n\n類型\nOn-demand price\n1-year\n3-year\nUnit\n\nN2D CPU\n$23.24612\n$14.64453\n$10.4609\nvCPU month\n\nC2D CPU\n$24.98863\n$15.74245\n$11.24492\nvCPU month\n\nC3D CPU\n$26.62383\n$16.77321\n$11.98076\nvCPU month\n\nN2D Memory\n$3.11564\n$1.96297\n$1.40233\nGB month\n\nC2D Memory\n$3.34632\n$2.10824\n$1.50599\nGB month\n\nC3D Memory\n$3.56532\n$2.24621\n$1.60454\nGB month\n\n但要特別注意 N2D，如果自定義CPU與Memory的話，\n價格又不一樣了。\nCPU 會來到 $24.408426 / vCPU month\nMemory 會來到 $3.271422 / GB month\n同樣以4顆CPU來算的話，\n當記憶體到13.82G時，價格將接近標準N2D。",
		"tags": ["2024/07/09更新", "note","☁︎"]
},

{
		"title": "GCP費用",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/☁︎ GCP/Price/GCP費用/",
		"content": "前言\n正文\nGCP的費用主要分三塊，\n\n機器規格的開機費用\n簡單說就是，機器開機的話產生的費用，但要注意的GCP他有續用折扣，當你用越多就能夠打折。而且他會主動合併未滿一個月機器，再一起算費用。\nref.\n\n聽說 GCP 費用最低？各種折扣方案完整說明\n持續使用折扣\n\n硬碟的費用\n恩...就開機器時，所需要的硬碟空間大小費用，就算你機器關機一樣照算。\nref. 磁碟和映像價格\n網路的費用\n以伺服器的角度來說，ingress(流量進來）不算錢，egress（流量出去）算錢。\n以使用者的角度來說，上傳不用錢，下載要錢。\nref. GCP 如何計費？就像水電費一樣\n\nRegistry\n\nArtifact Registry\nContainer Registry\n\n0.5GB以下免費，超過0.5GB，每GB 0.1 美元\n根據cloudStorage計價\n\n網路輸出另計\nref. Artifact Registry 定價",
		"tags": [ "note","☁︎"]
},

{
		"title": " ⛵️ istio MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/0.istio MOC/",
		"content": "<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/73. istio sidecar 注入規則/\">73. istio sidecar 注入規則</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/89. istio 常見狀態碼/\">89. istio 常見狀態碼</a>\n\n建置\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/7. Istio安裝及使用/\">7. Istio安裝及使用</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/11. istio 與 kiali/\">11. istio 與 kiali</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/14.istio安裝時固定ip/\">14.istio安裝時固定ip</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/20. 訪問istio 內部負載平衡ip/\">20. 訪問istio 內部負載平衡ip</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/21. flagger 自動金絲雀佈署 for istio/\">21. flagger 自動金絲雀佈署 for istio</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/23.istio的 prometheus截取GKE metrics/\">23.istio的 prometheus截取GKE metrics</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/34. istio operator 安裝新的 ingress deploy及service/\">34. istio operator 安裝新的 ingress deploy及service</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">199. istio使用gatewayAPI</a>\n\nVirtualService\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/11. istio 與 kiali/\">11. istio 與 kiali</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/30. istio的virtualservice match比對/\">30. istio的virtualservice match比對</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/35. istio virtualService delegate使用/\">35. istio virtualService delegate使用</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/54. istio 掛載ECK kibana/\">54. istio 掛載ECK kibana</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/62. isito 鏡像流量/\">62. isito 鏡像流量</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/75. istio virtual service 常用參數/\">75. istio virtual service 常用參數</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/89. istio 常見狀態碼/\">89. istio 常見狀態碼</a>\n\nAuthorization Policy\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/186. istio的Authorization policy(白名單)/\">186. istio的Authorization policy(白名單)</a>\n\nTroubleshooting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/15.istio服務無法訪問/\">15.istio服務無法訪問</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/18.istio Virtual Service 跨namespace讀取 DestinationRule/\">18.istio Virtual Service 跨namespace讀取 DestinationRule</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/31.istio 抓蟲記/\">31.istio 抓蟲記</a>",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio 與 kiali",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/11. istio 與 kiali/",
		"content": "前言\n從istio扯到kiali，是說kiali是真的好用，看東西挺直覺得。\n但有些警告，要解調就要照他的規範走\n正文\n現在最常碰到的，也還沒解\n\n如果使用GKE的私人叢集，請記得要把 master的防火牆規則，開啟8080 ，不然會發生讀取時需要一分鐘，資料才會出現。\n\nNo matching workload found for gateway selector in this namespace\n\n簡單講就是目前的namespace沒有找到目前的 ingressgateway，未來可能會發生問題\nref.\n\nNo matching workload found for gateway selector in this namespace\n\nPort name must follow &lt;protocol&gt;[-suffix] form\nservice上的 port name 必須要符合格式，\n格式必須為 http- 這樣的\napiVersion: v1\nkind: Service\nmetadata:\nnamespace: rd7-video\nlabels:\napp: video-api-qa\nname: video-api\nspec:\nports:\n# 需遵照格式 前面為 protocol 後面為 -suffix\n- name: &quot;http-video&quot;\nport: 80\nselector:\napp: video-api-qa\n\nref.\n\nPort name must follow &lt;protocol&gt;[-suffix] form\n\n另外，如果編輯config有錯時，\n按了SAVE沒有反應，錯誤訊息可能在最上面，要將捲軸往上拉。\n\nref.\n\nv1.13 validation\n\n為了kiali的健康度，最好是在pod上面寫個 readinessProbe（就緒探針）\n\nreadinessProbe:\nfailureThreshold: 3\ninitialDelaySeconds: 30\nperiodSeconds: 10\nsuccessThreshold: 1\ntcpSocket:\nport: 80\ntimeoutSeconds: 1\n\nkiali健康度 檢查 pod 變成 proxy unsynced\n\nref.\n\nv1.28Health Configuration\nistio之envoy常見術語及狀態碼\nkiali圖表說明\n\nvirtualservice is pointing to a non-existent gateway\n\n因為gateway不在同一層的namespace上面，所以要設定名稱時，\n要改用\ngateways:\n- istio-ingressgateway-video.sex-system.svc.cluster.local",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio安裝時固定ip",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/14.istio安裝時固定ip/",
		"content": "前言\n如果刪除GKE的節點，重建新的話，必須要把istio移除，才能夠使用 遷移工作負載到不同node。\n但如果已經將istio的ip ，綁上固定的domain，一旦移除就必須要重新綁定。\n正文\n以下內容都在GCP上面實作(istio 版本為1.8.2)。 騰訊雲不能這樣玩。\n\n先將IP變成靜態，而不是臨時。\n\n將下面yaml另存成檔案\n\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\ncomponents:\ningressGateways:\n- name: istio-ingressgateway\nenabled: true\nk8s:\nservice:\nloadBalancerIP: 123.123.123.123\nloadBalancerSourceRanges:\n- 111.222.222.111/32\n\n跟舊版的差異在必須加上name的名稱，不然在manifest generate 的時候會發生錯誤\n\n安裝\n\nistioctl install -f value.yaml\n\n可以先產生yaml看內容是不是有跟想象中的一樣\nistioctl manifest generate -f value.yaml\n\nref.\nIstio 1.5 部署指南修正版\nIstioOperator Options",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio服務無法訪問",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/15.istio服務無法訪問/",
		"content": "前言\n今天上班就發現這個情況，istio的服務無法訪問，但同一個叢集的另一個istio卻正常。\n正文\n看事件錯誤寫的是\nReadiness probe failed: HTTP probe failed with statuscode: 503\n\npod 的狀態是running ，但istio-proxy 卻是 containers with unready status\n\n根據錯誤訊息都沒找到相符合的資料\nref.\nReadiness probe failed: HTTP probe failed with statuscode: 503 #23283。\n現在看istio-proxy的日誌，上面寫的是\nEnvoy proxy is NOT ready: config not received from Pilot (is Pilot running?)\n這個似乎是原因..\n\n查看網格的狀況\nistioctl proxy-status\n\n所屬的網格LDS寫 Never Ackonwledged\n各欄位的意思如下\n\nLDS，Listener 發現服務：Listener 監聽器控制 sidecar 啟動端口監聽（目前只支持 TCP 協議），並配置 L3/L4 層過濾器，當網絡連接達到後，配置好的網絡過濾器堆棧開始處理後續事件。\nRDS，Router 發現服務：用於 HTTP 連接管理過濾器動態獲取路由配置，路由配置包含 HTTP 頭部修改（增加、刪除 HTTP 頭部鍵值），virtual hosts （虛擬主機），以及 virtual hosts 定義的各個路由條目。\nCDS，Cluster 發現服務：用於動態獲取 Cluster 信息。\nEDS，Endpoint 發現服務：用於動態維護端點信息，端點信息中還包括負載均衡權重、金絲雀狀態等，基於這些信息，sidecar 可以做出智能的負載均衡決策。\n\nref. Pilot\n\n查看pod錯誤\n\nEnvoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 1 successful, 0 rejected; lds updates: 0 successful, 1 rejected\n\n我一直以爲關鍵字是查前面的config not received from Pilot(is Pilot running?)\n但後來開了工單找了Google工程師，他說重點是後面那句 lds updates: 0 successful, 1 rejected\n然後查看 istiod log，可以看到lds為什麼會rejected\nwarn\tads\tADS:LDS: ACK ERROR router~10.131.1.19~istio-ingressgateway-794bbdb877-np94z.istio-system~istio-system.svc.cluster.local-994 Internal:Error adding/updating listener(s) 0.0.0.0_8443: Invalid path: /etc/istio/ingressgateway-certs/tls.crt\n\n這段在pod剛開始建立時也有出現此錯誤，看起來是https的憑證有問題，\n本來佈署gateway的yaml檔像這樣\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\nname: istio-ingressgateway-video\nnamespace: sex-system\nspec:\nselector:\nistio: ingressgateway # 使用默認的控制器\nservers:\n- port: #加密傳輸\nnumber: 443\nname: http\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-certs/tls.crt #設定憑證路徑\nprivateKey: /etc/istio/ingressgateway-certs/tls.key #設定憑證路徑\nhosts:\n- &quot;*&quot; #該服務對應domain\n- port:\nnumber: 80\nname: http\nprotocol: HTTP\nhosts:\n- &quot;*&quot; #該服務對應domain\n\n憑證路徑是直接指向pod裡面的路徑，\n但\n\ngoogle工程師說 istio v1.6開始gateway的tls已经不支援file mount类的设置了(i.e. privateKey, serverCertificate). 。\n\n所以，要麻自己建立 https的憑證 ，或者是乾脆不要https。\n（ref. Secure Gateways)\n自己建立憑證的話，\n根據上面的網址，產生 crt 跟 key ，然後使用\nkubectl create secret tls credential-httpbin -n istio-system --cert=./httpbin.com.crt --key=./httpbin.com.key\n\n建立sercret，最後再yaml內直接使用這個sercret的name\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\nname: istio-ingressgateway-custom\nnamespace: rd7-video\nspec:\nselector:\nistio: ingressgateway-private # 使用默認的控制器\nservers:\n- hosts:\n- httpbin.com\nport:\nname: httpbin\nnumber: 443\nprotocol: HTTPS\ntls:\ncredentialName: credential-httpbin\nmode: SIMPLE\n- port:\nnumber: 80\nname: http\nprotocol: HTTP\nhosts:\n- &quot;*&quot; #該服務對應domain\n\n目前看起來是有直接解掉這個問題，\n剛剛掛掉的pod，有變回正常的狀態。\n但當初兩個pod，一個活着，另一個陣亡，還不知道爲何會這樣。",
		"tags": ["23283", "加密傳輸", "設定憑證路徑", "設定憑證路徑", "該服務對應domain", "該服務對應domain", "該服務對應domain", "note","⛵️"]
},

{
		"title": "istio Virtual Service 跨namespace讀取 DestinationRule",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/18.istio Virtual Service 跨namespace讀取 DestinationRule/",
		"content": "前言\n因為在寫 vs的match uri，有發生一些坑，使用了prefix導致其他的VS都去抓到另一個VS的路徑，\n於是想要讓所有的VS都用同一個VS，避免未來發生同樣的錯誤，不好找原因。\n正文\n目前的狀況是 A服務在 A 的namespace上面， B服務在 B 的 namespace上面。\n正常的方式，\n\nVirtualService 跟 DestinationRule 都會各自在所屬服務上面的 namespace上面。\n所以，如果有 三個服務，就會有 三個 VirtualService ，\n同時 match uri的prefix部分，也都寫 /api /api2 /api3 的話，就會發現，服務不管怎麼呼叫都是呼叫到api\n其實這也算我發生的錯誤，prefix 是只針對前面的字串符合就過了。\nref. HTTPMatchRequest\n但這個錯誤未來不知道有沒有辦法避免，是也能夠使用exact 完全符合的關鍵字來避免上面的錯誤發生。\n但此時我就在思考，能不能未來都統一使用一個 VirtualService 就好，\n這樣也方便管理。\n過程中的血淚，就不細說了。\n目前能做到的是 A、B 共用一個 VirtualService，此 VS在A的 namespace上面，\n然後 DestinationRule 則分別在A、B 的namespace上面各架設一個。\n如果DR也想要共用的話，會發生錯誤，\nThis subset’s labels are not found in any matching host\n\n想過使用指定的FQDN的label，來強迫他去指定的地方找deployment，但失敗了。\n如果有人有看到方法還麻煩告知。\n剛在寫文章的時候，有看到DR 也有exportTO的指令，\n所以應該也是可以透過exportTO的方法，來共用同一個DR。\n目前的yaml寫法如下\nexportTo:\n- '*'\nhttp:\n- match:\n- uri:\nprefix: /v2\nname: bale-api-v2\nrewrite:\nuri: /api\nroute:\n- destination:\nhost: bale-api.sex-system.svc.cluster.local\nport:\nnumber: 80\nsubset: bale-api2\nweight: 80\n- destination:\nhost: bale-api.sex-system.svc.cluster.local\nport:\nnumber: 80\nsubset: bale-api\nweight: 20\n\n這邊在kiali會看到警告 Subnet not found (Fig.1)，但如果直接從Graph看流量分佈的話，是有在分流的。\n這邊要注意，一定要加上 exportTO ，允許該服務可以跨過邊界去尋找到DR。\nexportTO:\n- '*'\n\nref.\nVirtualService\nDestination Rule is not applied to traffic from all namespaces it's exported to\nIstio 的配置分析\nVirtualService資源詳解",
		"tags": [ "note","⛵️"]
},

{
		"title": "186. istio的Authorization policy(白名單)",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/186. istio的Authorization policy(白名單)/",
		"content": "前言\n經過一個月的搬家事宜，\n這篇應該是第一篇，之後還有許多東西，\n都是這個月搬project碰到的。\n正文\n目的\n只要設定白名單，限制某些ip能進入就好，\n其他不用。\n實作-Network\n這邊使用istio 1.20 版本，雖然gateway API 剛出爐，\n但我還沒去研究，所以下面用的都還是 istio API\n首先確認你的LB類型，\n直連（Network)的話，基本上沒太大問題，白名單很好設定。\n\n開啓 externalTrafficPolicy&quot;:&quot;Local&quot;，\n兩種方法\na. 手動改service\n\nkubectl patch svc istio-ingressgateway -n istio-system -p '{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;:&quot;Local&quot;}}'\n\nb. 加在istioOperator上面\ningressGateways:\n- name: istio-ingressgateway\nenabled: true\nk8s:\nservice:\nexternalTrafficPolicy: Local\nloadBalancerSourceRanges:\n- 10.0.0.0/8\n- 172.16.0.0/12\n- 192.168.0.0/16\n\nAuthorization設定\n\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\nname: manage-ingress-policy\nnamespace: istio-system\nspec:\nselector:\nmatchLabels:\nistio: ingressgateway\naction: ALLOW\nrules:\n- from:\n- source:\nipBlocks::\n- &quot;10.0.0.0/8&quot;\n- &quot;172.16.0.0/12&quot;\n- &quot;192.168.0.0/16&quot;\n\n（注意，這邊的Load Balancer Type指的是Network，沒透過proxy的，\n如果有開CDN，那就是istio說明上面的Load Balancer Type 是TCP Proxy)\nref. Ingress Access Control\n實作-Proxy\n這邊的網路走法是，有透過CDN，或是其他的prxoy代理連過來的服務。\nnumTrustedProxies 這個的值，取決於你的走法過了幾層的proxy，\n我只有cdn轉島到gke上面的istio LB\n\n設定方式\na. 在pod上面新增 anntations\n\nmetadata:\nannotations:\n&quot;proxy.istio.io/config&quot;: '{&quot;gatewayTopology&quot; : { &quot;numTrustedProxies&quot;: 1 } }'\n\nb. 在istioOperator新增\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\nmeshConfig:\ndefaultConfig:\ngatewayTopology:\nnumTrustedProxies: 1\t\n\nAuthorization設定\n\n這邊有點坑，搞了一天才搞清楚。\n強烈建議把istio的httpbin裝起來，實際去打，\n然後一邊查看ingressgateway的deployment的log。\na. 限制全部\n這是第一種方法，但這個只能針對『入口』的ingress做白名單驗證。\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\nname: frontend-ingress-policy\nnamespace: istio-system\nspec:\nselector:\nmatchLabels:\napp: ingressgateway-external\naction: ALLOW\nrules:\n- from:\n- source:\nremoteIpBlocks:\n- &quot;10.0.0.0/8&quot;\n- &quot;172.16.0.0/12&quot;\n- &quot;192.168.0.0/16&quot;\n\nb. 針對單個\n假如沒有在入口的話，無法照上面a.的方式抓到資料，\n所以只能強制指定判斷header。\nref. Incorrect RemoteIP when Authorization Policy is applied to Injected Istio Proxy\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\nname: frontend-ingress-policy\nspec:\nselector:\nmatchLabels:\ngroup: frontend\naction: ALLOW\nrules:\n- when:\n- key: request.headers[X-Envoy-External-Address]\nvalues:\n- &quot;10.*&quot;\n\nref. Configuring Gateway Network Topology\nTroubleshooting\nistioOperator增加log 閱讀性\nspec:\nmeshConfig:\naccessLogFile: /dev/stdout\naccessLogEncoding: JSON\naccessLogFormat: |\n{\n&quot;protocol&quot;: &quot;%PROTOCOL%&quot;,\n&quot;upstream_service_time&quot;: &quot;%REQ(X-ENVOY-UPSTREAM_SERVICE_TIME)%&quot;,\n&quot;upstream_local_address&quot;: &quot;%UPSTREAM_LOCAL_ADDRESS%&quot;,\n&quot;duration&quot;: &quot;%DURATION%&quot;,\n&quot;upstream_transport_failure_reason&quot;: &quot;%UPSTREAM_TRANSPORT_FAILURE_REASON%&quot;,\n&quot;route_name&quot;: &quot;%ROUTE_NAME%&quot;,\n&quot;downstream_local_address&quot;: &quot;%DOWNSTREAM_LOCAL_ADDRESS%&quot;,\n&quot;user_agent&quot;: &quot;%REQ(USER-AGENT)%&quot;,\n&quot;response_code&quot;: &quot;%RESPONSE_CODE%&quot;,\n&quot;response_flags&quot;: &quot;%RESPONSE_FLAGS%&quot;,\n&quot;start_time&quot;: &quot;%START_TIME%&quot;,\n&quot;method&quot;: &quot;%REQ(:METHOD)%&quot;,\n&quot;request_id&quot;: &quot;%REQ(X-REQUEST-ID)%&quot;,\n&quot;upstream_host&quot;: &quot;%UPSTREAM_HOST%&quot;,\n&quot;x_forwarded_for&quot;: &quot;%REQ(X-FORWARDED-FOR)%&quot;,\n&quot;client_ip&quot;: &quot;%REQ(TRUE-Client-IP)%&quot;,\n&quot;requested_server_name&quot;: &quot;%REQUESTED_SERVER_NAME%&quot;,\n&quot;bytes_received&quot;: &quot;%BYTES_RECEIVED%&quot;,\n&quot;bytes_sent&quot;: &quot;%BYTES_SENT%&quot;,\n&quot;upstream_cluster&quot;: &quot;%UPSTREAM_CLUSTER%&quot;,\n&quot;downstream_remote_address&quot;: &quot;%DOWNSTREAM_REMOTE_ADDRESS%&quot;,\n&quot;authority&quot;: &quot;%REQ(:AUTHORITY)%&quot;,\n&quot;path&quot;: &quot;%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%&quot;,\n&quot;response_code_details&quot;: &quot;%RESPONSE_CODE_DETAILS%&quot;\n}\n\nref. Understanding Istio Access Logs\nAuthorization Policy規則\n\nIf there are any CUSTOM policies that match the request, evaluate and deny the request if the evaluation result is deny.\n\n如果有任何符合請求的自訂策略，則評估並拒絕請求，若評估結果為拒絕。\n\nIf there are any DENY policies that match the request, deny the request.\n\n如果有符合請求的任何拒絕策略，則拒絕該請求。\n\nIf there are no ALLOW policies for the workload, allow the request.\n\n如果工作負載沒有任何允許策略，則允許該請求。\n\nIf any of the ALLOW policies match the request, allow the request.\n\n如果有任何允許策略符合該請求，則允許該請求。\n\nDeny the request.\n\n拒絕該請求。\n\nref. Authorization Policy\ncloudflare轉發\n上面有提到用cloudflare的CDN時，白名單會跟着變，\n但在那之前還有個東西要注意，CDN轉發時，\ncloudflare可以指定走http還是走https\n這個會影響到istio的gateway，是開80還是443。",
		"tags": [ "note","⛵️"]
},

{
		"title": "訪問istio 內部負載平衡ip",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/20. 訪問istio 內部負載平衡ip/",
		"content": "前言\n正文\n要指定內部負載平衡IP的話，一般在svc上面是只要指定\nmetadata:\nannotations:\ncloud.google.com/load-balancer-type: &quot;Internal&quot;\nspec:\ntype: LoadBalancer\n\n這樣在GKE上面就會指定成內部負載平衡。\n但在istio上面要指定的話，需在k8s底下新增 serviceAnnotations\n像下面這樣（這是GKE的寫法，其他雲端機器設定的方式會不一樣）\nk8s:\nserviceAnnotations:\ncloud.google.com/load-balancer-type: &quot;Internal&quot;\n\n其他的參數請參考 istio安裝及使用 =&gt; 同一個叢集安裝多個ingressgateway\n\nref.\nSetup Multiple Ingress Gateways in Istio\nHow can I add service annotation in istio operator patch\nVirtualService的寫法\nGateway的寫法與之前的一致，\n比較有差異的是VS的寫法\nhosts 需設定內部ip，不能使用 &quot;*&quot;\ngateways 需增加mesh，表示路由是從網格內的流量來的。\n如果不在gateways上面，寫mesh，會發生\n503 upstream connect error or disconnect/reset before headers. reset reason local reset istio\n\n的錯誤。\n會發現這個錯誤，是因為看了Istio實踐避坑指南：10 大常見異常、最佳實踐及解決方案清楚網格內，與網格邊緣的差異，才知道增加此設定。\n最後yaml如下\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: vs-internal\nnamespace: default\nspec:\nhosts:\n- 10.4.0.37\ngateways:\n- mesh\n- istio-ingressgateway-internal.default.svc.cluster.local\nhttp:\n- corsPolicy:\nallowOrigin:\n- '*'\nmatch:\n- uri:\nexact: /api/v1/health\nname: route-v1\nroute:\n- destination:\nhost: abc.default.svc.cluster.local\nport:\nnumber: 80\nweight: 100\n\nref.\nIstio 運維實戰系列（3）：讓人頭大的『無頭服務』-下\n記一次Istio間歇503的問題排查",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio flagger 自動金絲雀佈署",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/21. flagger 自動金絲雀佈署 for istio/",
		"content": "前言\n本來自動佈署配上istio都好了，\n但在實際上測試的時候發現，如果不導入自動藍綠佈署或金絲雀佈署的話，\n到時開發人員發佈程式時，需要更改istio 的 VirtualService 以及 DestinationRule。\n除非發佈的人懂這些東西，不然...改壞就...，所以只好先研究flagger了。減少開發人員佈署的know how，\n只要git tag ，然後『等』 就好了。\n正文\n\n簡易流程介紹\n\n目前正在執行的deploy 為abc-primary ，當有新的deploy 佈署在 abc 時，flagger啟動，\n先將流量從 abc-primary 轉到 abc ，因為abc 為新版本。\n當流量全部轉到abc後，將 abc-primary 刪除，並佈署成新的版本。\n當abc-primary佈署完後，再將 流量從 abc 轉回 abc-primary，\n等待全部流量轉回來後，再將abc的deploy刪除。\n\n先安裝flagger\n\nkustomize build https://github.com/fluxcd/flagger/kustomize/istio?ref=main | kubectl apply -f -\n\nref. Flagger Install on Kubernetes\n3. 佈署負載測試服務 flagger-loadtester\n如果namespace要先建立test ，如果不想使用這個名稱，請下載yaml檔後修改 kustomization.yaml 的namespace\nflagger-loadtester\nkubectl apply -k https://github.com/fluxcd/flagger//kustomize/tester?ref=main\n\n這裏面主要是使用hey 作爲連線測試的工具，\n在後面的webhook會用到此服務。\n\n設定Canary\n\n此範例為AB佈署，另有金絲雀佈署，請參考\nspec 底下的參數，\n- targetRef.name 是 deploy會截取現有的deploy設定，確認名稱與現有的deploy名稱一樣\n- autoscalerRef.name 是 HPA會截取現有的HPA設定，確認名稱與現有的HPA名稱一樣。\n- service.name是 service會截取現有的service設定，確認名稱與現有的service名稱一樣。\n- service.host 需指定目前服務的domain，如果為ip只能有一組\n- webhooks.metadata.cmd ，網址必須更改成服務能夠訪問的網址。\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\nname: video\nnamespace: sex-system\nspec:\nprovider: istio\ntargetRef:\napiVersion: apps/v1\nkind: Deployment\nname: video-api\nautoscalerRef:\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nname: video-api\n\nservice:\nname: video-api\nport: 80\ntargetPort: 80\nportName: http\nportDiscovery: true\ngateways:\n- istio-gateway-external.istio-system.svc.cluster.local\n- istio-gateway-internal.istio-system.svc.cluster.local\n- mesh\nhosts:\n- 123.123.123.123\ntrafficPolicy:\ntls:\nmode: DISABLE\nmatch:\n- uri:\nprefix: /\nrewrite:\nuri: /\ntimeout: 30s\nanalysis:\n# analysis spend time : interval * iterations\n# 時間間隔 (默認 60s)\ninterval: 30s\n# 回滾前的最大失敗指標檢查次數(可能因為檢查時間差的關係，至少會有兩次失敗)\nthreshold: 5\niterations: 1\nmetrics:\n- name: request-success-rate\nthresholdRange:\nmin: 99\ninterval: 1m\n- name: request-duration\nthresholdRange:\nmax: 500\ninterval: 30s\nwebhooks:\n- name: load-test-get\ntype: pre-rollout\nurl: http://flagger-loadtester.tools/\ntimeout: 15s\nmetadata:\ntype: cmd\ncmd: &quot;hey -z 1m -q 10 -c 2 http://video-api-canary.test/api/v1/Health/health&quot;\n- name: &quot;notify&quot;\ntype: event\nurl: http://webhook-api.tools/webhook/flagger\ntimeout: 5s\nmetadata:\nchannel: &quot;-1023450&quot;\n- name: load-test\ntype: rollout\nurl: http://flagger-loadtester.tools/\ntimeout: 5s\nmetadata:\ntype: cmd\ncmd: &quot;hey -z 1m -q 10 http://video-api-canary.test/api/v1/Health/health&quot;\n\n寫好canary，然後直接佈署。\n此時，k8s會根據你的設定，建立起 一些相關的service以及 deployment ，\n例如，本來的deployment、service以及 hpa的 名稱叫 abc ，\n那同時會建立 abc-primary 的deployment 跟 hpa，\n以及 abc-canary 、 abc-primary 的 service 。\n然後再istio上面也會自動建立 VirtualService 跟 DestinationRule\n所以Canary很重要、很重要、很重要。\nref.\nflagger/artifacts/examples/istio-canary.yaml\nIstio Canary Deployments\nIstio A/B Testing\n\n引入 grafana\n\n因為istio本身就有grafana了，所以我不想重複安裝，\n到他們的git\n將json檔下載下來，\n到grafana匯入json(fig.1)，\n\n此時會發生錯誤\nTemplate variables could not be initialized: Datasource named prometheus was not found\n\n原因是原本的json檔裡面的datasource是 prometheus\n而目前安裝的是 Promethus ，將本來的json更改後再匯入。\nref.\nFlagger - Monitor Your Canary Deployments with Grafana\n\n佈署失敗，重新佈署方式\n\n藍綠佈署或金絲雀佈署上面有提到，會自動新增一個deployment，再原有的名稱上面加上-primary，\n然後原有的deployment還會存在，但他的pod會清除。\n當有新的image時，會觸發動作，在原有的deployment上將pod產生出來，\n然後開始做流量的導轉，並發佈新版本。\n但在測試的時候，如果canary寫錯了，會導致發佈失敗。\n此時修改完，重新apply ，也會卡在Fail的狀態，\n需到 原本的deployment 增加 下面的程式，\n讓canary重新觸發。\napiVersion: apps/v1\nkind: Deployment\nspec:\ntemplate:\nmetadata:\nannotations:\ntimestamp: &quot;2020-03-10T14:24:48+0000&quot;\n\nref. How to retry a failed release\n\n佈署失敗原因\n\n目前測試，~每次一定會跳這個出來~ ，已解決。差別只在次數\nHalt advancement no values found for istio metric request-success-rate probably video-api.sex-system is not receiving traffic: running query failed: no values found\n用curl打 不會產生此錯誤，\ninterval 時間壓在30s ，此錯誤只會發生1次\ninterval 時間壓在20s ，此錯誤會發生2次\nthreshold：回滾最大錯誤發生次數，設成5，只要小於錯誤發生的次數，還是可以成功自動佈署\n有查到會發生這個錯誤的原因，如果在流量導轉期間沒有收到流量的話就會發生此錯誤。\n所以要解決這個錯誤，要再 pre-rollout 的時候，先送一些連線過去。\n這樣就不會有此錯誤發生了。\nref.Load Testing\n\n監控方式\n\n查詢flagger佈署狀態，需額外安裝jq（解析json的軟體)\n\nwatch &quot;kubectl -n istio-system logs deployment/flagger --tail=10 | jq '.ts,.msg'&quot;\n或\nkubectl -n test describe canary/video-api | tail -20f\n\n看流量比重\n\nwatch kubectl get canaries -A\n\nwebhook主動通知\n\n在flagger裡面，有實作webhook，可以將事件的狀態通知給需要的人。\n內建有 slack 、discord ， 這段我沒實作 ，看起來只要加參數即可，\n請參考 Alerting\n但因為公司政策，我不能用slack所以只好自己做telegram了，\ntelegram做機器人還挺快的，改天再寫一篇關於telegram的使用方式。\nwebhooks:\n- name: &quot;notify&quot;\ntype: event\nurl: http://webhook-api-v2.test/webhook/flagger\ntimeout: 5s\nmetadata:\nsome: &quot;some message&quot;\n\n在canary上面加入上面的程式碼，然後當有事件觸發時，\n會回傳下面的json格式。\n注意 webhook-api-v2.test 是一個我自己建立在 namespce ：test ，deployment：webhook-api-v2 的api\nwebhook其實就是自己建立一個api的接收機制，當server端有資料時，\n會根據上面的url送出json給你，此時要怎麼利用，就看各自的需求了。\n{\n&quot;name&quot;:&quot;video&quot;,\n&quot;namespace&quot;:&quot;test&quot;,\n&quot;phase&quot;:&quot;Failed&quot;,\n&quot;metadata&quot;:{\n&quot;eventMessage&quot;:&quot;Canary failed! Scaling down abc.test&quot;,\n&quot;eventType&quot;:&quot;Warning&quot;,\n&quot;some&quot;:&quot;some message&quot;,\n&quot;timestamp&quot;:&quot;1616654792750&quot;\n}\n}\n\nref.\nwebhook 是什麼以及如何創建\nLINE Bot 系列文 — 什麼是 Webhook?\n結論\n有時文章測試有問題的話，建議到他們的git\n找相關的範例，做測試。\nref.\n基於Flagger和Istio實現自動化金絲雀部署\nIstio 實踐\nService Mesh - Istio實戰篇（上）\nAutomated canary deployments with Flagger and Istio\nistio入門系列之自動化灰度發佈",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio的 prometheus截取GKE metrics",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/23.istio的 prometheus截取GKE metrics/",
		"content": "前言\n主要架構弄得差不多的，使用 istio 加上 drone 的 自動佈署，\n再配合 flagger的藍綠佈署，藍綠佈署時使用telegram通知目前佈署進度。\n，現在剩下監控這一塊。\n然後，阿伯，出事拉....\n正文\n一開始想使用grafana上面的樣板，原以爲只要這樣就好。\n\n1. Kubernetes Deployment Statefulset Daemonset metrics\n1 Kubernetes for Prometheus Dashboard CN 20201209\nPrometheus 2 - Deploy Grafana\n\n但是，發現我抓不到資料。直接連線至Prometheus裡面查詢也沒有，\n於是斷定，我根本沒有撈資料到Prometheus。\n如果直接架設一個新的grafana不使用istio的grafana會不會比較好？\n但是這樣本來istio的grafana dashboard (Fig.1) 就要自己建立了。\n\nkube-state-metrics\n查了一些資料，最後找到的方案是安裝 kube-state-metrics\n但是光安裝還是不行，必須要讓Prometheus抓得到資料才行。\n建議先將yaml檔抓下來，因為有些地方需要更改。\n打開 standard/service.yaml\n在metadata底下 加上 prometheus.io/scrape: 'true'\n這是要讓prometheus 可以自動得去發現服務\nmetadata完整的yaml如下\nmetadata:\nlabels:\napp.kubernetes.io/name: kube-state-metrics\napp.kubernetes.io/version: v1.9.8\nannotations:\nprometheus.io/scrape: 'true'\nname: kube-state-metrics\nnamespace: kube-system\n\n然後，佈署\nkubectl apply -f standard\n\nNode-Exporter\n後來查了一下，這邊的指標資料已經在 cadvisor 裡面了，所以不用在額外做。\nref. Prometheus (node_exporter) issue when update from GKE 1.15 to 1.16\nref.\nPrometheus 5 - Node Exporter &amp; Kube State Metrics\nprometheus（九） 收集kube-state-metrics指標資訊",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio的virtualservice match比對",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/30. istio的virtualservice match比對/",
		"content": "前言\n最近因為新的需求，再度跟istio的virtualservice槓上了。\n這次要做的是針對domain去解析網址，host不再自定。\n正文\n踩到的坑很特別，根據官方文件上寫的是\n\nFor example, the following restricts the rule to match only requests where the URL path starts with /ratings/v2/ and the request contains a custom end-user header with value jason\n\n文章的範例如下，官方文件是說明當url path 符合 /ratings/v2 而且 header裡面的end-user的值為 jason 才會轉到正確的路由。\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\nname: ratings-route\nspec:\nhosts:\n- ratings.prod.svc.cluster.local\nhttp:\n- match:\n- headers:\nend-user:\nexact: jason\nuri:\nprefix: &quot;/ratings/v2/&quot;\nignoreUriCase: true\nroute:\n- destination:\nhost: ratings.prod.svc.cluster.local\n\nref.HTTPMatchRequest\n再來就是我踩坑的地方，因為我不管怎麼測試，測試出來的都是 『或』，\n也就是說 當流量進入時， 只要 url path 是 /api 『或』 header 的Host是 符合 qa2.777.+.com的正則，才會導去正確的路由。\n跟官方文件的講法落差有點大，後來測試了老半天才發現，關鍵在 『 - 』。\n下面的程式，條件是 『或』，兩個條件其中一個過都會導到正確的路由\nmatch:\n- headers:\nHost:\nregex: qa2\\.777.+\\.com\n- uri:\nprefix: /api\n\n下面的程式，條件式 『而且』 ，兩個條件都要符合才會導到正確的路由\nmatch:\n- headers:\nHost:\nregex: qa2.?\\.777.+\\.com\nuri:\nprefix: /api\n\n本來還有在探討另一種方法，直接在hosts上面做判斷(fig.1)，\n但是測試出來只能針對 sub domain，如果把萬用字元放在後面，連存檔都會出錯握。\n\nhosts:\n- &quot;*.777.com&quot;\n\nref.\nSupport wildcard/regexp in VirtualService host\nconfig.route.v3.VirtualHost",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio 抓蟲記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/31.istio 抓蟲記/",
		"content": "前言\n狀況很奇特，奇特到我不知道該從哪里找。簡單說就是gateway裡面有mesh的話，\n就會有此坑出現。\n正文\n先說明一下環境，Deployment A 的 VirtualService 有設定 domain , abc.com\n此時進去 Deployment裡面的 pod ， 使用 curl 呼叫外部的ip，會發生404的錯誤。\ne.g. curl -v 123.213.123.213 -H &quot;Host:abc.com&quot;\n但是，如果Host不帶abc.com ，改帶任意的 domain，即可正常執行。\n除錯過程\n\n使用tcpdump ，將所有的封包撈出，但發現如果 host 帶 abc.com ，沒有任何的封包進出。\n但帶任意 domain: def.com ，則會有封包進出。故開始懷疑是 sidecar的某些機制將流量導到本機，\n以至於回傳404的錯誤。\n簡易的tcpdump使用，\n詳細用法請參考連結\n針對 host 是 123.213.123.213 且 port 是 80 的封包截取，並存入demo2.pcap的檔案\ntcpdump host 123.213.123.213 and port 80 -w demo2.pcap\n\n但是從pod裡面看封包，會想哭，所以進一步要把封包的檔案放到自己的機器上，再使用wireshark看。\n將pod(api-beta-v2-primary-69797b47d6-bl6vf)，裡面的檔案 demo2.pcap 複製到本機上\nkubectl cp api-beta-v2-primary-69797b47d6-bl6vf:/app/demo2.pcap -n istio ./demo2.pcap\n\nref. [Linux] Tcpdump 擷取封包指令範例教學\n\n查詢outbound handler\n\nistioctl proxy-config route api-beta-v2-primary-69797b47d6-bl6vf -n istio\n\n終於查到了一個關鍵，在有問題domain，他match的path是另外一個virtualService(fig.1)，最後查到那個VirtualSerice ，在gateway上面多設了一組 mesh，\n導致他一直在網格內繞，所以才會404。\n\n額外補充，查詢 Inbound handler\nistioctl proxy-config listener reviews-v1-54b8794ddf-jxksn\n\n可直接用 istioctl pc 來取代 proxy-config\nref. Istio 中的 Sidecar 注入及透明流量劫持過程詳解",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio operator 安裝新的 ingress deploy及service",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/34. istio operator 安裝新的 ingress deploy及service/",
		"content": "前言\n主管挑戰DC失敗，變成又要變回舊的系統架構。暈...\n用了istio是為了一個ip打所有服務，現在又變回去。\n服務跟 ip 是一對一的存在。\n正文\n最簡單的安裝方式，可看前一篇 <a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/7. Istio安裝及使用/\">7. Istio安裝及使用</a>\n以前新增的時候，都會同時安裝最原始的deploy 以及 service(fig.1)，\n\n所以在新增新的ingressgateway的時候，需要先指定原始的istio-ingressgateway 為 enable:false。\n這樣在安裝的時候，才不會同時安裝。\nkind: IstioOperator\napiVersion: install.istio.io/v1alpha1\nmetadata:\nannotations:\ninstall.istio.io/ignoreReconcile: 'true'\nname: istio-external-test\nnamespace: istio-system\nspec:\naddonComponents:\nistiocoredns:\nenabled: false\ncomponents:\nbase:\nenabled: true\ncni:\nenabled: false\ningressGateways:\n- name: istio-ingressgateway # 20210730新增，可取消單獨佈署此yaml還會額外裝ingress-gateway\nenabled: false\n- name: ingressgateway-external\nnamespace: istio-system\nenabled: true\nk8s:\nservice:\n# loadBalancerIP 要跟著改，最好先將IP設成靜態\nloadBalancerIP:\nloadBalancerSourceRanges:\n# - 0.0.0.0/0\n- 10.0.0.0/8\n- 172.16.0.0/12\n- 192.168.0.0/16\nenv:\n- name: ISTIO_META_ROUTER_MODE\nvalue: standard\nhpaSpec:\nminReplicas: 1\noverlays:\n- kind: HorizontalPodAutoscaler\nname: ingressgateway-external\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-external\n- path: metadata.labels.istio\nvalue: ingressgateway-external\n- path: spec.scaleTargetRef.name\nvalue: ingressgateway-external\n- kind: Deployment\nname: ingressgateway-external\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-external\n- path: metadata.labels.istio\nvalue: ingressgateway-external\n- path: spec.selector.matchLabels.app\nvalue: ingressgateway-external\n- path: spec.selector.matchLabels.istio\nvalue: ingressgateway-external\n- path: spec.template.metadata.labels.app\nvalue: ingressgateway-external\n- path: spec.template.metadata.labels.istio\nvalue: ingressgateway-external\n- kind: Service\nname: ingressgateway-external\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-external\n- path: metadata.labels.istio\nvalue: ingressgateway-external\n- path: spec.selector.app\nvalue: ingressgateway-external\n- path: spec.selector.istio\nvalue: ingressgateway-external\n- kind: PodDisruptionBudget\nname: ingressgateway-external\npatches:\n- path: metadata.name\nvalue: ingressgateway-external\n- path: metadata.labels.app\nvalue: ingressgateway-external\n- path: metadata.labels.istio\nvalue: ingressgateway-external\n- path: spec.selector.matchLabels.app\nvalue: ingressgateway-external\n- path: spec.selector.matchLabels.istio\nvalue: ingressgateway-external",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio virtualService delegate使用",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/35. istio virtualService delegate使用/",
		"content": "前言\n正文\n會用到delegate的原因，是為了要解決在維護時上的痛點。\n因目前使用flagger canary，\n導致每個服務就一個virtualService (下面會用VS代替)，\n而多個服務會共用一個host ，導致在維護上的困難。\n而且在kiali上面會出現警告(fig.1)，看了實在礙眼。\n\n所以使用delegate。這樣的話，流量會先導到 delegate vs，之後才會根據path到各服務的vs。\ndelegate 有幾點要注意，\n\n不支援 regex\n在delegate上寫rewrite不會被轉導，要在導過去的vs寫。\n目前測試只能在同一個namespace上面。如果不是用藍綠佈署自定義的話，在destination 上面，可以使用完整的FQDN，是可以執行的。(例如: video-admin-primary.default.svc.cluster.local)\n\nVS delegate\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\nname: control-dashboard\nnamespace: default\nspec:\nhosts:\n- &quot;*&quot;\ngateways:\n- istio-gateway-external.istio-system.svc.cluster.local\nhttp:\n- delegate:\nname: video-admin\nnamespace: default\nmatch:\n- uri:\nprefix: /ezio\n- uri:\nprefix: /abc\n- uri:\nprefix: /api\nname: reviews-v2-routes\n- delegate:\nname: tw-frontend\nnamespace: default\nmatch:\n- uri:\nprefix: /\nname: tw-frontend\n\n服務的 vs，（注意：此處的yaml是由flagger自動產生的）\nkind: VirtualService\napiVersion: networking.istio.io/v1alpha3\nmetadata:\nname: video-admin\nnamespace: default\nspec:\nhosts: []\nhttp:\n- match:\n- uri:\nprefix: /api\n- uri:\nprefix: /abc\n- uri:\nprefix: /ezio\nrewrite:\nuri: /api\nroute:\n- destination:\nhost: video-admin-primary\nweight: 100\n- destination:\nhost: video-admin-canary\nweight: 0\n\nps. 使用canary的話，在service下面加上\ndelegation: true\n\nref. How does Flagger interact with Istio?\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\nname: example-api-qa-v2\nnamespace: istio-example\nspec:\nprovider: istio\ntargetRef:\napiVersion: apps/v1\nkind: Deployment\nname: example-api-qa-v2\nautoscalerRef:\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nname: example-api-qa-v2\n\nservice:\ndelegation: true # use delegate\nname: example-api-qa-v2\nport: 80\ntargetPort: 80\nportName: http\nportDiscovery: true\n\ntrafficPolicy:\ntls:\nmode: DISABLE\nmatch:\n- uri:\nprefix: /api-v2\nrewrite:\nuri: /api\ntimeout: 30s\nanalysis:\n# analysis spend time : interval * iterations\n# 時間間隔 (默認 60s)\ninterval: 30s\n# 回滾前的最大失敗指標檢查次數(可能因為檢查時間差的關係，至少會有兩次失敗)\nthreshold: 5\niterations: 1\nmetrics:\n- name: request-success-rate\nthresholdRange:\nmin: 99\ninterval: 1m\n- name: request-duration\nthresholdRange:\nmax: 500\ninterval: 30s\nwebhooks:\n- name: load-test-get\ntype: pre-rollout\nurl: http://flagger-loadtester.tools/\ntimeout: 15s\nmetadata:\ntype: cmd\ncmd: &quot;hey -z 1m -q 10 -c 2 http://example-api-qa-v2-canary.istio-example&quot;\n- name: &quot;notify&quot;\ntype: event\nurl: http://webhook-api.tools/webhook/flagger\ntimeout: 5s\nmetadata:\nchannel: &quot;-1001429024676&quot;\n- name: load-test\ntype: rollout\nurl: http://flagger-loadtester.tools/\ntimeout: 5s\nmetadata:\ntype: cmd\ncmd: &quot;hey -z 1m -q 10 http://example-api-qa-v2-canary.istio-example&quot;\n\nref.\nVS Delegate\nVS HTTPMatchRequest",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio 掛載ECK kibana",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/54. istio 掛載ECK kibana/",
		"content": "前言\n正文\n正常設定vs的時候，\n- match:\n- uri:\nprefix: /\nname: kibana\nroute:\n- destination:\nhost: quickstart-kb-http.default.svc.cluster.local\nport:\nnumber: 5601\n\n會發生錯誤\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\n不管怎樣改vs還是不通，最後是要把kibana的設定改掉，\n完整的yaml如下\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\nname: quickstart\nspec:\nversion: 7.15.0\ncount: 1\nelasticsearchRef:\nname: quickstart\nhttp:\ntls:\nselfSignedCertificate:\ndisabled: true\nservice:\nspec:\nports:\n- name: http\nport: 5601\ntargetPort: 5601\n\n重點在於一定要加 http 下面的設定。\nselfSignedCertificate ，請參考[3]\n補充：\n[1] 的連結有提到\nmetadata:\nannotations:\nsidecar.istio.io/rewriteAppHTTPProbers: &quot;true&quot;\n\n這邊請參考[2]\nref.\n\nExpose kibana (eck operator) via istio gateway - https problem (Error 503)\nIstio 服務的健康檢查\nHTTP configuration",
		"tags": [ "note","⛵️"]
},

{
		"title": "isito 鏡像流量",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/62. isito 鏡像流量/",
		"content": "前言\n正文\n最近在測試 istio的鏡像流量，\n想要針對舊的 websocket 做鏡像將流量導到新版的websocket上面。\n實測的結果，websocket不能用。\n但http能夠使用，這是由於 istio的鏡像流量是 『 fire and forget』\n但websocket 是需要交握後，才能持續進行下面的步驟。\n所以導致連線無法建立，但 RESTful的請求，是沒問題的。\n先決條件，\n2個deployment，分別叫 A 跟 B，\nlabel 使用 version：v1 以及 version: v2 做區別\n1個 service ，分別指到這2個deployment\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ws-qa\nnamespace: istio\nspec:\nhosts:\n- www.abc.com\n- yabo-ws-qa\ngateways:\n- istio-system/istio-gateway-external\n- istio-system/istio-gateway-internal\nhttp:\n- match:\n- uri:\nregex: ^/ws/.*\nroute:\n- destination:\nhost: ws-qa\nsubset: v1\nport:\nnumber: 80\nweight: 100\nmirror:\nhost: ws-qa\nsubset: v2\nmirrorPercentage:\nvalue: 100.0\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: ws-qa\nnamespace: istio\nspec:\nhost: ws-qa\nsubsets:\n- labels:\nversion: v1\nname: v1\n- labels:\nversion: v2\nname: v2 \t\t\n\n用任一的壓測軟體，呼叫本來的網站，就會得到下圖的數據。\nv2是鏡像流量，流量過去後就不會管是否回應。\n\nref.\n\n流量鏡像\nMirroring",
		"tags": [ "note","⛵️"]
},

{
		"title": "Istio安裝及使用",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/7. Istio安裝及使用/",
		"content": "前言\nk8s還沒摸很熟，新的東西就過來了。\n這次是server mesh架構...\n正文\n前個工作還沒結束，新的東西就來了。\n這次搞Istio，當然可以在東西都建立完後，再開始玩。\n但這樣就會變成要把舊的pod砍掉重新建立了。\nIstio 知識圖譜\n\nref.Istio Knowledge Map\n安裝前\n有查到要安裝istio 有最低的資源限制，但沒看到有明確的指定。\n目前測試GKE的機器 設成 cpu 1 ram 2G 會發生 timeout的錯誤，\n改成 cpu 2,ram 4G 則無此狀況。\n\nref. istioctl install fails with multiple timeouts\n機器大小設定\n主要的節點區域，cpu則會吃到 900 mCPU，記憶體 2.75G\n還沒安裝 kiali,jaeger,prometheus...等，\n開機器大小時，需特別注意。\n開始安裝\n這次要安裝在GKE上，此次安裝的配置是 minimal版，\n各版本的差異，不要看中文的，沒有更新。如果要看原理可以看。\n請參考 Installation Configuration Profiles\ncurl -L https://istio.io/downloadIstio | sh -\ncd istio-1.8.1\nexport PATH=$PWD/bin:$PATH\nistioctl install\n\n移除安裝\n1.20指令更改為 istioctl uninstall -y --purge\nistioctl x uninstall --purge\n(注意 purge會把共用的東西通通刪光光，不使用purge也可刪除，但會留下 deployment以及 service)\nistioctl x uninstall -f /Users/ezio_liu/Documents/k8s/Sample-Yaml/istio-sample-newingeressway.yaml （當初安裝的yaml)\nref. Getting Started\nUninstall Istio\n安裝套件\n在原目錄下執行\nkubectl apply -f samples/addons\n第一次執行完可能會有錯誤，可以在執行一次。\n因執行建置的機器，會有時間差。\nref. Telemetry Addons\n在namespace上面 植入 istio\nkubectl label namespaces auto istio-injection=enabled\n\n本來想使用自動植入，就是連這行也不用打。\n但是 有些 k8s版本，本身會禁止這個行為，需要的話要另行打開。\nref. Automatic sidecar injection\n此時可建立一個pod ，就可看到裡面多了一個 container\n\n如果植入sidecar失敗，出現\nInternal error occurred: failed calling webhook &quot;sidecar-injector.istio.io&quot;: Post https://istiod.istio-system.svc:443/inject?timeout=30s: context deadline exceeded\n\n需開啟GKE的master防火牆規則，增加 tcp:15017 的 port\n(題外話，如果要使用kiali的話，請開啟 tcp: 8080 的 port，不然你會看kiali一直在轉圈圈)\n\n不知道要設定哪個防火牆規則，可下指令尋找\ngcloud compute firewall-rules list --filter=&quot;name~gke-${CLUSTER_NAME}-[0-9a-z]*-master&quot;\n\n其中的 ${CLUSTER_NAME} 需改為你要尋找的叢集名稱\ngcloud compute firewall-rules update &lt;firewall-rule-name&gt; --allow tcp:10250,tcp:443,tcp:15017,tcp:8080\n\nref.Google Kubernetes Engine\n開啟套件\n我是使用GKE，原本以為要能夠連到內部的服務，都要走外部ip\n今天在亂點的時候，點到了這個地方，將那行指令複製，在本機執行。\n前提是你的gcloud要有認證。\n\n他就能夠透過通訊埠轉發的方式存取。\n\n由(fig.3) 可以看到，他將本機的8080對到了service內的20001，\n所以直接開啟網頁指向 localhost:8080 就可以看到該網頁的內容了。\n佈署時由上往下開始佈署\nDestinationRule\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\nname: localgo\nnamespace: auto\nspec:\nhost: localgo\ntrafficPolicy:\nloadBalancer:\nsimple: LEAST_CONN\nsubsets:\n- name: Localgov1\nlabels:\nversion: v1\n- name: v2\nlabels:\nversion: v2\n\nref. Destination Rule\n參數說明\ntrafficPolicy內的 loadBalancer:\n代表的是幾種流量分發方式：\n•Round_Robin： 輪詢演算法，顧名思義請求將會依次發給每一個例項，來共同分擔所有的請求。\n•Random： 隨機演算法，將所有的請求隨機分發給健康的例項\n•Least_Conn: 最小連線數，在所有健康的例項中任選兩個，將請求發給連線數較小的那一個例項。\nref. idou老師教你學Istio 19 : Istio 流量治理功能原理與實戰\nVirtualService\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: localgo-vs\nnamespace: auto\nspec:\nhosts:\n- &quot;*&quot;\ngateways:\n- istio-ingressgateway\nhttp:\n- name: &quot;route-v1&quot;\nroute:\n- destination:\nport:\nnumber: 80\nhost: localgo.auto.svc.cluster.local #這裡就是指向你真正服務 service 的名稱喔\nsubset: Localgov1 #這裡指的是DestinationRule的subset.name\nweight: 80\n- destination:\nport:\nnumber: 80\nhost: localgo.auto.svc.cluster.local\n# 上面 destionation 下面的 host 名稱會這麼長，這是 k8s 內部實際 dns 全名，如果需要跨 namspace 存取，就需要用這樣的全名方式指定。\n# {service name}.{namespace}.svc.cluster.local 這就是它的規則\nsubset: v2\nweight: 20\ncorsPolicy: ##限制\nallowOrigin: #允許所有來源\n- '*'\n\ncorsPolicy 有以下的選項\n\nallowOrigins\nallowMethods\nallowHeaders\nexposeHeaders\nmaxAge\nallowCredentials\n\nweight 則是用來做流量分配用的\n同一個route內，相加必須 =100\n詳細說明，參考下面連結\nref.\nCorsPolicy細項設定\n然後在 這邊還有另一種是 match的用法，\n虛擬服務中定義的第一條規則有最高優先級，\n當第一個沒有match到相對應的規則時，\n則繼續往下執行。\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: localgo-vs\nnamespace: auto\nspec:\nhosts:\n- &quot;*&quot;\ngateways:\n- istio-ingressgateway\nhttp:\n- name: &quot;route-v1&quot;\nmatch:\n- uri:\nprefix: /reviews\nroute:\n- destination:\nport:\nnumber: 80\nhost: localgo.auto.svc.cluster.local #這裡就是指向你真正服務 service 的名稱喔\nsubset: Localgov1 #這裡指的是DestinationRule的subset.name\n- name: &quot;route-v3&quot;\nmatch:\n- uri:\nprefix: /Player\nroute:\n- destination:\nport:\nnumber: 80\nhost: localgo.auto.svc.cluster.local #這裡就是指向你真正服務 service 的名稱喔\nsubset: Localgov1 #這裡指的是DestinationRule的subset.name\n- name: &quot;route-v2&quot;\nroute:\n- destination:\nport:\nnumber: 80\nhost: localgo.auto.svc.cluster.local\n# 上面 destionation 下面的 host 名稱會這麼長，這是 k8s 內部實際 dns 全名，如果需要跨 namspace 存取，就需要用這樣的全名方式指定。\n# {service name}.{namespace}.svc.cluster.local 這就是它的規則\nsubset: v2\n\nmatch 下面的用法\n\nexact (完全匹配)\nprefix (只有前綴)\nregex (正規表示法)\n\n上面那段的規則為，當網址進來後，如果網域的後面不是reviews的話，\n則往下執行到route-v3，如果網址為 Player的話，則執行下面的路由\nref.\nStringMatch\n路由規則\nDay24 什麼是 istio virtual service?\ngateway\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\nname: istio-ingressgateway\n# 這裡就是填入kubectl get service -n istio-system 所查到的 istio-ingressgateway，實務上如果你有多個 ip ，就會有多個 gateway 代理，\n# 然後就要描述多個 gateway 去使用 gateway 代理\nnamespace: auto\nspec:\nselector:\nistio: ingressgateway # 使用默認的控制器\nservers:\n- port: #加密傳輸\nnumber: 443\nname: https\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-certs/tls.crt #設定憑證路徑\nprivateKey: /etc/istio/ingressgateway-certs/tls.key #設定憑證路徑\nhosts:\n- &quot;*&quot; #該服務對應domain\n- port:\nnumber: 80\nname: http\nprotocol: HTTP\nhosts:\n- &quot;*&quot; #該服務對應domain\n\nref. istio gateway 是什麼？\n同一個叢集安裝多個ingressgateway\n滿滿的坑，卡了兩天才搞定。\n這邊要改使用operator安裝\n所以先初始化\nistioctl operator init\n\nref. Istio Operator Install\n再來設定新增的 ingressgateway\nkind: IstioOperator\napiVersion: install.istio.io/v1alpha1\nmetadata:\nannotations:\ninstall.istio.io/ignoreReconcile: 'true'\nname: video-state\nnamespace: istio-system\nspec:\naddonComponents:\nistiocoredns:\nenabled: false\ncomponents:\nbase:\nenabled: true\ncni:\nenabled: false\ningressGateways:\n- name: ingressgateway-private\nnamespace: istio-system\nenabled: true\nk8s:\nenv:\n- name: ISTIO_META_ROUTER_MODE\nvalue: standard\nhpaSpec:\nminReplicas: 2\noverlays:\n- kind: HorizontalPodAutoscaler\nname: ingressgateway-private\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-private\n- path: metadata.labels.istio\nvalue: ingressgateway-private\n- path: spec.scaleTargetRef.name\nvalue: ingressgateway-private\n- kind: Deployment\nname: ingressgateway-private\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-private\n- path: metadata.labels.istio\nvalue: ingressgateway-private\n- path: spec.selector.matchLabels.app\nvalue: ingressgateway-private\n- path: spec.selector.matchLabels.istio\nvalue: ingressgateway-private\n- path: spec.template.metadata.labels.app\nvalue: ingressgateway-private\n- path: spec.template.metadata.labels.istio\nvalue: ingressgateway-private\n- kind: Service\nname: ingressgateway-private\npatches:\n- path: metadata.labels.app\nvalue: ingressgateway-private\n- path: metadata.labels.istio\nvalue: ingressgateway-private\n- path: spec.selector.app\nvalue: ingressgateway-private\n- path: spec.selector.istio\nvalue: ingressgateway-private\n- kind: PodDisruptionBudget\nname: ingressgateway-private\npatches:\n- path: metadata.name\nvalue: ingressgateway-private\n- path: metadata.labels.app\nvalue: ingressgateway-private\n- path: metadata.labels.istio\nvalue: ingressgateway-private\n- path: spec.selector.matchLabels.app\nvalue: ingressgateway-private\n- path: spec.selector.matchLabels.istio\nvalue: ingressgateway-private\n\n先使用\nistioctl manifest generate -f newingeressway.yaml\n預先編譯一下，看有沒有成功。 istioctl manifest的原理，有點像是kustomize，\n有一個基底，後面的程式都是根據這個基底去修改覆蓋。\n安裝的話，則是使用\nistioctl manifest install -f newingeressway.yaml\nref.\nMultiple ingress controller services via IstioOperator?\nistio\nIstio入門與實戰",
		"tags": ["這裡就是指向你真正服務", "這裡指的是DestinationRule的subset", "允許所有來源", "這裡就是指向你真正服務", "這裡指的是DestinationRule的subset", "這裡就是指向你真正服務", "這裡指的是DestinationRule的subset", "加密傳輸", "設定憑證路徑", "設定憑證路徑", "該服務對應domain", "該服務對應domain", "note","⛵️"]
},

{
		"title": "istio sidecar 注入規則",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/73. istio sidecar 注入規則/",
		"content": "前言\n因為drone 的 kubernetes Runner的關係，\n導致我有一個ns要預設取消istio inject，\n但某些特定的pod要注入就失敗了。\n正文\n先來看一張官方做的表(1)，\n其實就很好解釋為什麼不會注入了。\n\nnamespaceSelector match\ndefault policy\nPod override annotation sidecar.istio.io/inject\nSidecar injected?\n\nyes\nenabled\ntrue (default)\nyes\n\nyes\nenabled\nfalse\nno\n\nyes\ndisabled\ntrue\nyes\n\nyes\ndisabled\nfalse (default)\nno\n\nno\nenabled\ntrue (default)\nno\n\nno\nenabled\nfalse\nno\n\nno\ndisabled\ntrue\nno\n\nno\ndisabled\nfalse (default)\nno\n\n各欄位解釋，\n\nnamespaceSelector match：各namespace的 istio-inject是 enable或 disable\ndefault policy： 這個預設都是enable，要查看使用command或到configmap查看 istio-sidecar-injector\n\nkubectl -n istio-system get configmap istio-sidecar-injector -o jsonpath='{.data.config}' | grep policy:\n\nPod override annotation ： 直接指定pod會不會注入的 annotation\nSidecar injected ：結果\n\n結論，\nnamespace只要是 disabled的，通通不會inject sidecar\n\nref.\n1. Automatic Sidecar Injection\n2. Sidecar 自動注入問題\n3. 關於Istio Automatic Sidecar Injection那檔事",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio virtual service 常用參數",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/75. istio virtual service 常用參數/",
		"content": "前言\n有些東西偶爾會用，但碰到時就忘了怎麼寫，\n還要回去翻一下，\n此篇包含\n\nset/remove header\nmatch header host 正規表示\nfault 回傳特定狀態\nallow cors\nrewrite\nmatch uri 正規表示\n\n正文\n\n當同一個virtual service內，有多個match uri時，優先順序，是由上到下。\n　\n例如：\n\nmatch:\n\t- uri:\n\t\t\tprefix : /api\n\t- uri:\n\t\t prefix: /api/video\t\t\n\n這樣是永遠到不了 /api/video的\n\nset/remove header\n\nadd or modify header\nspec:\nhttp:\n- name: api\nheaders:\nrequest:\nset:\nx-forward-for: 127.0.0.1\nmatch:\n- uri:\nprefix: /api\n\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\n\nremove header\nspec:\nhttp:\n- name: api\nheaders:\nrequest:\nremove:\n- x-forward-for\nmatch:\n- uri:\nprefix: /api\n\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\n\nmatch header host\n當 header host有對應到 這兩個 domain其中一個的話，\n則轉到對應的路徑\n\nhttp:\n- match:\n- headers:\nHost:\nregex: (api.asia.com|api.american.com)\nuri:\nprefix: /api-v2\nrewrite:\nuri: /api\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\nweight: 100\n\n符合正則的皆進入\n\nhosts:\n- '*'\ngateways:\n- istio-system/istio-gateway-external\n- istio-system/istio-gateway-internal\nhttp:\n- match:\n- headers:\nHost:\nregex: sxqa2.?\\.777.+\\.com\nroute:\n- destination:\nhost: video-admin-api-primary\nport:\nnumber: 80\nweight: 100\n\nfault\n\n當路徑有倒到 api/v1/abc時，直接回傳 487的錯誤碼。\n（這其實是拿來模擬錯誤用的）\nhttp:\n- fault:\nabort:\nhttpStatus: 487\npercentage:\nvalue: 100\nmatch:\n- uri:\nprefix: /api/v1/abc\nname: fault\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\n\nallow cors\n開啟 cors的允許範圍\n\nhttp:\n- corsPolicy:\nallowHeaders:\n- '*'\nallowOrigin:\n- '*'\nmatch:\n- uri:\nprefix: /api\nname: filedownload\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\n\nrewrite\n覆寫本來的uri\n\nhttp:\n- match:\n- uri:\nprefix: /apmapi\nname: api\nrewrite:\nuri: /api\nroute:\n- destination:\nhost: reverse-proxy.abc.svc.cluster.local\nport:\nnumber: 80\n\n在api之後的通通進去，但這種就無法使用rewrite了。\n因為會將uri全部取代掉。\nhttp:\n- match:\n- uri:\nregex: ^/api/.*\nroute:\n- destination:\nhost: yabo-api-qa-primary\nport:\nnumber: 80\nweight: 100",
		"tags": [ "note","⛵️"]
},

{
		"title": "istio 常見狀態碼",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/⛵️ istio/89. istio 常見狀態碼/",
		"content": "前言\n最近用istio偵測，常碰到某個服務狀態會掉到80%以下，\n那時都有看到APM的程式報錯，但沒人反應，最近有次發生在上班時間，\n馬上聯絡同事看他的服務有沒有正常，\n最後一路追，\n發現狀態碼是回 503 DC ，對這個關鍵字有印象，但常會忘記他的全名。\n正文\n\n下列為envoy常見縮寫比對\nHTTP and TCP\n\nUH: No healthy upstream hosts in upstream cluster in addition to 503 response code.\n\nUF: Upstream connection failure in addition to 503 response code.\n\nUO: Upstream overflow (circuit breaking) in addition to 503 response code.\n\nNR: No route configured for a given request in addition to 404 response code, or no matching filter chain for a downstream connection.\n\nURX: The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached.\n\nNC: Upstream cluster not found.\n\nDT: When a request or connection exceeded max_connection_duration or max_downstream_connection_duration.\n\nHTTP only\n\nDC: Downstream connection termination.\n\nLH: Local service failed health check request in addition to 503 response code.\n\nUT: Upstream request timeout in addition to 504 response code.\n\nLR: Connection local reset in addition to 503 response code.\n\nUR: Upstream remote reset in addition to 503 response code.\n\nUC: Upstream connection termination in addition to 503 response code.\n\nDI: The request processing was delayed for a period specified via fault injection.\n\nFI: The request was aborted with a response code specified via fault injection.\n\nRL: The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code.\n\nUAEX: The request was denied by the external authorization service.\n\nRLSE: The request was rejected because there was an error in rate limit service.\n\nIH: The request was rejected because it set an invalid value for a strictly-checked header in addition to 400 response code.\n\nSI: Stream idle timeout in addition to 408 response code.\n\nDPE: The downstream request had an HTTP protocol error.\n\nUPE: The upstream response had an HTTP protocol error.\n\nUMSDR: The upstream request reached max stream duration.\n\nOM: Overload Manager terminated the request.\n\nDF: The request was terminated due to DNS resolution failure.\n\nref.\n- envoy Access logging\n- istio之envoy常見術語及狀態碼",
		"tags": [ "note","⛵️"]
},

{
		"title": "Google sheet Query函數",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🆒 SideProject/125. Google sheet Query/",
		"content": "睡睡念\n今天收到個消息，說要給user hosts的檔案，\n裏面內容要包含目前的dns跟ip的對應，\n但我日後不想手動更新阿...\n正文\n開始發想\n\n本來是想用if ，單獨判斷一個欄位是不是符合規則。\n但這樣會有個問題，當資料變動時，就必須手動拉公式。\n\n查到一個importrange的函數，可以整批匯入資料，\n但這樣我還是要分析截取。\n\n在查資料過程中，發現query函數，能夠整個sheet搜尋 ，\n就像SQL語法一樣，就決定是你了！！！\n\n實作\nQUERY使用 欄位的做關鍵字，\n不過只有基本的查詢功能。\n=query(Prod!4:1010,&quot;select D,E,G where G='Y' order by E desc&quot;)\n\n比較特別的應該是\nwhere裏面的 文字搜尋\nWhere A contains 'Ezio' 查詢包含Ezio的字\nWhere A starts with 'Ezio' 查詢Ezio開頭的字\nWhere A ends with 'Ezio' 查詢Ezio結尾的字\nWhere A like '%Ezio' 跟SQL的like差不多意思\n\n注意，like 後面接 % 是 任意字元 , 如果接 _ 就是一個字元\n多了一些mysql 的語法\nselect A,B limit 3 只取3行的資料\nselect A,B offset 3 跳過3行，開始取資料\n\n還有一個特別的，\nselect A,B SKIPPING 2 每隔2行抓一次資料\n\n其他相似的就\n聚合函數 SUM()、AVG()、COUNT()、MAX()、MIN()\n排序 order by\n另外還支援 PIVOT\nref.\n\nQUERY 函式大解析（一）：基本原理與 SELECT",
		"tags": [ "note","🆒"]
},

{
		"title": "Obsidian發佈網站",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🆒 SideProject/160.Obsidian發佈網站/",
		"content": "前言\n因為被google的blog搞了一波（原因請看這 ），\n讓我終於下定決心研究 md轉html的方法。\n發佈方式\n從Obsidian發佈主要有幾種方法，\n\n官方提供 ，每個月8美金，詳情參考 這裏。\njekyll ，也有人是用這個掛上去的，格式稍微改一下，然後在github上面改一下設定，push上去就會自動部署了，改天再用另一篇介紹，因為要改許多東西，我就沒用這種了\nObsidian的第三方plugin , Digital Garden ，本篇就是透過這套來發佈網站。\n\n正文\n原文連結： Digital Garden-Get start\n\n先在obsidian安裝 digital Garden\n\n一個Github帳號。\n\n申請 Vercel，用github帳號登入。\n如果要用Netlify應該也可以，\n這兩個網站都是直接用子域名作爲你的網站名稱，\ngithub以前也有，但現在已經改掉了。\n(記得在這邊，會問你要給Vercel哪個repostory的權限，可以全給，之後再到github 的設定指定repostory就好)\n\n打開作者的repository ，按下下面的部署按鈕，會到Vercel部署一個網站。\n\n如果你熟git的話，也可以直接clone那個repository，\n再自己到vercel或Netlify部署上去也可以。\n如果你想要再本機測試開啓的話，\nclone這個專案後，執行下面程式，注意需先安裝nodejs\n\nnpm install\nnpm run build\nnpm run start\n\n再來需建立github token，這是為了讓Vercel可以讀取到你的repostory。\n開啓github token，取個名字、建立。\n\n建立github token\n\nExpiration ：到期天數，如果不想一直改的話，就設定沒有期限。\n下面紅框是必須要有的權限。\n\n打開你的obsidian的Digital Garden設定\n在四個方塊裏面輸入你的資訊\n\nGithub Repo Name： 前面所新增的repostory的名稱\nGithub UserName：你的帳號名稱\nGithub Token：步驟5建立的資料\nBaseUrl ：如果你剛剛已經初步在Vercel建立一個網站的話，應該就會看到你的網址。\n\n我這邊有兩個domain，前面的http://daimom.vercel.app 是後來自定的。\n\n發佈\n先到你要設成首頁的文章上面，設定\n\ndg-publish: true\ndg-home: true\n\ndg-publish ：表示發佈\ndg-home：表示設爲首頁\n再來你有兩種方法可以發佈，叫出Obsidian的命令面板\nWin： CTRL+P\nMac：Cmd+P\n\n先輸入dg，然後會看到 Digital Garden: Publish Single Note\n執行後就會發佈到Vercel。\n或是 GUI畫面\n\n按下 Publish unpublished Note\n\n建置\n如果你速度夠快，切到Vercel的網站，會看到Deployment正在執行的畫面，或是已經執行完成的畫面。\n\nQ&amp;A\n\n切換主題的時候，有時候會沒有更改，此時請按下 設定的 Update site 的Manage site Template按鈕，他會幫你建立PR。 ref. Updating the template\n主題改得只有顏色跟字體大小，如果要右邊的路徑圖可以到設定打開 Show local graph for notes\n\n更多的設定，請參考Note settings\n本篇的theme是 Rmaki ，一些小設定是我改css跟js改出來的。\n如果要整批加入發佈的yaml，可以考慮使用 MetaEdit\n將Vercel的權限限縮特定repostory，先到github的個人設定畫面，找到 Application\n\n按下Configure\n\n從原先的All repostories改成 Only select repositories，\n最後指定你的repository即可。\n\nref.\n\nDigital Garden Doc\n免費直接把筆記發佈成網站:Obsidian + GitHub + Netlify\nobsidian 目前最完美的免費發佈方案 - 漸進式教學",
		"tags": [ "note","🆒"]
},

{
		"title": " 🌐 Network MOC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/0.Network MOC/",
		"content": "基礎\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/92. proxy 代理/\">92. proxy 代理</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/102. OSI七層/\">102. OSI七層</a>\n\nNginx\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/169. Nginx的負載平衡/\">169. Nginx的負載平衡</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/176. Nginx cheat sheet/\">176. Nginx cheat sheet</a>\n\nTroubleshooting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/99. nginx的除錯筆記/\">99. nginx的除錯筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/106. zyxel 與 fortigate 建立VPN 點對點通道/\">106. zyxel 與 fortigate 建立VPN 點對點通道</a>",
		"tags": [ "note","🌐"]
},

{
		"title": "OSI七層",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/102. OSI七層/",
		"content": "前言\n因為一直背不起來 OSI七層，上網查了下發現還真的有人發明了口訣\nOSI由最上層往下\nAll People Seem To Need Domino's Pizza\n（所有的人看來都需要達美樂比薩)\n正文\n\nApplication ：應用層\n主要功能是處理應用程式，進而提供使用者網路應用服務，\n透過通訊協定(DHCP、FTP、HTTP...)和資料操作，將資訊整合起來，提供給使用者進行操作\nPresentation ：展示層\n展示層主要負責轉譯、加密和壓縮資料\nSession ：會議層\n會議層主要負責建立網絡連線，等到資料傳輸結束時，再將連線中斷。\nTransport ：傳輸層\n傳輸層的主要用途是，協助OSI前三層與OSI後三層進行溝通，傳輸層也負責處理流量控制和錯誤控制。\nNetwork ：網路層\n網路層是針對位於不同網路(WAN)的兩個裝置，促進兩者之間的資料傳輸。\n(路由器及Layer 3交換器即屬於第三層的網路設備，主要以IP作為資料傳輸依據)\nData Link：資料連結層\n資料連結層是針對位於相同網路(LAN)的兩個裝置，促進兩者之間的資料傳輸。\n(網路交換器（Switch）是這個層級常見的設備，主要在區域網路上運作，能依據MAC位址，將網路資料傳送到目的主機上。)\nPhysical：實體層\n實體層主要是用來定義設備裝置之間位元資料傳輸，也就是透過物理線材連接至其他實體設備，傳遞0和1的數位訊號。\n\nref.\n- OSI 七層架構\n- 什麼是OSI的7層架構？",
		"tags": [ "note","🌐"]
},

{
		"title": "zyxel 與 fortigate 建立VPN 點對點通道",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/106. zyxel 與 fortigate 建立VPN 點對點通道/",
		"content": "前言\n換公司後，DevOps還要兼網管，\n然後，在辦公室還要連vpn才能連到dev的開發環境（？\n我很懶，為什麼要這樣對我，\n所以...就改了吧。\n正文\n\n預計完成的架構圖如上\n有幾個地方的坑很深，\n建議都先查一下。\n本次做的是site to site VPN連線\n1. 檢查小烏龜跟防火牆的連線方式\n因爲有申請固定ip，但本來是透過小烏龜連到外網，\n後來一查才發現應該是要讓Firewall自己擁有實體ip才對。\n不用再透過FW。\n有幾種解法，\n\n小烏龜改bridge mode （網路上看到的都這種居多）\n小烏龜改浮動ip撥接，然後再FW上使用固定ip 撥接方式。\n\n重點是要讓防火牆有直接的對外ip，環境上會比較單純。\n我之前的環境，小烏龜連到Zyxel 是192.168.1.X ，\n然後小烏龜接防火牆走wan port，但到內部的機器ip也是192.168.1.X，\n2.設定 ipSec\n這邊設定的方式網路上很多種，可以先看一下 [Nebula x Firewall]與Fortigate建立IPSec站到站site-to-site VPN\n關鍵點在，Phase 1、Phase 2跟金鑰的設定一定要兩臺一樣。\n我在連線的時候，一直碰到ZyXel 連vpn時 一直timeout，\n最後是在改完 1 的架構以後，一整個雨過天晴\nZyxel的IPsec設定\n本地的ip跟遠端的ip要分清楚\n\nfortigate的IPsec通道 設定\n\n然後檢查連線有沒有通，\n兩邊都要設定好，才會通。\n\n3. 設定路由\n我看的網路上的教學，他們是一設定好後，就連的通了。\n但我怎麼連就是不通。\n\n用tracert看路由，是只跑到192.168.1.1 (Zyxel FW上)，就沒在往下了。於是先弄個（ZyXel )策略路由，看轉不轉的過去。\n\nfortigate的防火牆策略及路由，防火牆記得不能開NAT。\n\nfortigate上面檢查的方式有兩種\n\n撈封包，然後用wireshark看\n\n到fortigate的 CLI控制檯下指令查(這個比較快)\n\ndiag debug reset\ndiag debug flow filter clear\ndiagnose sniffer packet any &quot;host 192.168.103.221 and icmp&quot; 4\n\n正常的話，會像這樣有回應\n\n我當時碰到的是有流量打到 fortigate的機器上，但沒有轉過去，\n所以路由跟防火牆\n結語\n花了我三天的時間，我真的只是個半吊子的網管，網路我不熟阿 嗚嗚...(╥﹏╥)\n讓我懷念起以前當MIS時的防火牆設定，都是廠商幫我們設定的，我們只要稍微知道一些管理就好，感謝ShareTech 。老闆！！如果下次採購時，我還在的話，我要換廠商啊啊啊。\nref.\n\n有關 FortiGate 防火牆相關設定\n[Nebula x Firewall]與Fortigate建立IPSec站到站site-to-site VPN",
		"tags": [ "note","🌐"]
},

{
		"title": "Nginx的負載平衡",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/169. Nginx的負載平衡/",
		"content": "睡睡唸\n雖然知道nginx應該可以做到負載平衡，\n但從來沒弄過，\n查了一下，還挺簡單的。\n正文\n主要有四種模式，\n都是寫在upstream底下\n\n輪詢(Round Robin) ：什麼都不要寫，預設的方法\n最少連接(least_conn)：會轉發到伺服器連接數最少的那臺\nip地址雜湊(ip_hash)：使用者會連到特定的伺服器上\n權重劃分(weight)：根據權重比例切換伺服器，這種寫法跟上面的三種不一樣。\n\nref. Nginx負載平衡的4種方式 ：輪詢-Round Robin 、Ip地址-ip_hash、最少連接-least_conn、加權-weight=n\nconfig.conf\nupstream yolox-brand {\nleast_conn;\nserver yolox-brand-1:8080;\nserver yolox-brand-2:8080;\n}\n\nserver {\nlisten 80;\nserver_name 192.168.1.79;\n\nlocation / {\nproxy_pass http://yolox-brand;\nproxy_redirect off;\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Host $server_name;\n\nproxy_hide_header 'Access-Control-Allow-Origin';\nadd_header Access-Control-Allow-Origin *;\nadd_header Access-Control-Allow-Methods 'GET, POST, OPTIONS';\nadd_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization';\n\nif ($request_method = 'OPTIONS') {\nreturn 204;\n}\n}\n}\n\n然後，docker-compose.yaml如下，\n要注意別把config檔塞錯地方了，不然會碰到 <a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/99. nginx的除錯筆記/\">99. nginx的除錯筆記</a>的錯誤\nversion: '3.8'\nservices:\npython-1:\ncontainer_name: yolox-brand-1\nimage: busybox\npull_policy: if_not_present\nrestart: always\nnetworks:\n- internal\npython-2:\ncontainer_name: yolox-brand-2\nimage: busybox\npull_policy: if_not_present\nrestart: always\nnetworks:\n- internal\nnginx-lb:\ncontainer_name: nginx\nimage: nginx\npull_policy: if_not_present\nrestart: always\nvolumes:\n- ./proxy.conf:/etc/nginx/conf.d/proxy.conf\nports:\n- 80:80\nnetworks:\n- internal\nnetworks:\ninternal:\nname: internal\ndriver: bridge\n\n補充，proxy_pass url 中包含路徑時，結尾的 / 最好同 location 匹配規則一致。\nref. proxy_pass url 反向代理的坑",
		"tags": [ "note","🌐"]
},

{
		"title": "Nginx cheat sheet",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/176. Nginx cheat sheet/",
		"content": "前言\n驗證nginx的http2開啓方式時，\n才突然驚覺我需要一個nginx cheat sheet\n不然怎麼單純回個狀態都不知道去哪抄\n正文\nresponse 200\nlocation / {\nadd_header Content-Type text/plain;\nreturn 200 'Hey James,Say Hello Nginx';\n}\n\n自簽憑證\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout example.key -out example.crt\n\n在 Common Name時，記得輸入你目前電腦的ip，\n產生完後，掛載對應的volume到nginx裏面。\n更改config。\ndocker-compose.yaml\n\nversion: '3.8'\nservices:\nnginx-lb:\ncontainer_name: nginx\nimage: nginx\npull_policy: if_not_present\nrestart: always\nvolumes:\n- ./certs:/etc/ssl\n- ./nginx.conf:/etc/nginx/nginx.conf\n- ./proxy.conf:/etc/nginx/conf.d/proxy.conf\nports:\n- 443:443\n- 80:80\nnetworks:\n- internal\nnetworks:\ninternal:\nname: internal\ndriver: bridge\n\nproxy.conf(某個nginx版本後 http2開啓的方式要改用下面的方式)\nserver {\nlisten 443 ssl;\nserver_name 192.168.1.106;\nhttp2 on;\n\nssl_certificate /etc/ssl/example.crt;\nssl_certificate_key /etc/ssl/example.key;\nssl_protocols TLSv1.3 TLSv1.2;\nssl_prefer_server_ciphers on;\nssl_ecdh_curve X25519:secp521r1:secp384r1;\nssl_ciphers TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;\nssl_session_cache shared:TLS:2m;\nssl_buffer_size 4k;\n\n\tlocation / {\n\t add_header Content-Type text/plain;\n\t return 200 'Hey James,Say Hello Nginx';\n\t}\n}\n\nProxy_pass\nproxy_pass url 中包含路徑時，結尾的 / 最好同 location 匹配規則一致。\nref. proxy_pass url 反向代理的坑",
		"tags": [ "note","🌐"]
},

{
		"title": "Fortigate 設定 內部服務負載平衡",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/182. Fortigate 設定 內部服務負載平衡/",
		"content": "前言\n&amp;(&amp;%(%^&amp;%@#$%^((&amp;^% 一連串消音，\n說過我只會簡易的網路管理，然後現在叫我搞這個，\n目前是第二次，再來一次，直接畢業好了。\n直接去改客戶的防火牆設定是你瘋了，還是我瘋了。\n弄不好網路全斷誰負責！？\n還是你認爲網路很簡單，那你他馬的怎麼不自己搞。\n正文\n要達成的目標，user呼叫一個ip網址，會自動轉到其他的伺服器上面 。\n主要參考fortigate官網說明，Virtual server\n測試使用的FortiOS版本為 7.2.5\n簡略說明\n\n打開 Virtual server\n如果沒看到，需到 System -&gt; Feature Visibility 開啓 Additional Features底下的 Load Balance\n\n新增Virtual Server\n\n新增Firewall Policy\n問題點在 Inspection Mode ，選擇 Flow-based的話，會找不到你剛剛設定的virtual server，必需設定Proxy-based才有\n\ntroubleshoting\n\n測試結果跟沒走virtual server不一樣？\n檢查看看 virtual server 的 Persistence 有沒有設定 HTTP Cookie\n呼叫api，發現呼叫的都是同一個ip\n檢查一下 Load Balancing method 是不是設定成 Static，上面設定就設錯了，導致固定只打同一個。\n\nFirewall Policy的 Inspection Mode 沒出現選項\n這塊比較不確定，但我是先到 System -&gt; Feature Visibility 開啓 Security Features底下的 Zero Trust Network Access後，才突然跑出來，但我把他關掉，也沒有消失...\n\nref. Virtual server",
		"tags": [ "note","🌐"]
},

{
		"title": "proxy 代理",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/92. proxy 代理/",
		"content": "前言\n代理伺服器，以前最常用過的應該是Hinet的proxy，\n在以前網路不快的時候，有時會掛prxoy，來讓瀏覽速度變快。\nproxy的用途主要也是如此，將user的請求透過prxoy去跟網站取得資料，再回給user。\n但proxy分成很多種..下面簡單說明\n正文\n\nHTTP代理\n主要用於存取網頁，一般有內容過濾和快取功能（將HTTP請求轉發到所需的HTTP伺服器）。\nuser使用方法，是瀏覽器掛http代理伺服器後，去瀏覽網頁\n\nTCP代理\n伺服器端實作轉發TCP請求，當訪問本機的port時，會轉到指定ip的port上\n\nUDP代理\n伺服器端實作轉發UDP請求，當訪問本機的port時，會轉到指定ip的port上\n\n內網穿透\n將內網服務提供給外部做使用, Ngrok,frp\n，當請求連到外網server的固定port時，轉發到內網服務的port上。\n（需在提供服務的內網server及外網的server分別設定）\n\nSOCKS代理\n只是單純傳遞封包，不關心具體協定和用法，所以速度快很多（更低的協定，SOCKS5能處理udp protocol)。\n第五層協定，故能處理HTTP，HTTPS，POP3，SMTP和FTP。\nuser簡易使用方法：使用 ssh tunnel 連到內部主機。\n(比較偏伺服器端建設，需額外的軟體透過socks協定連入)\n\nSS代理\nShadowsocks，基於SOCKS5代理方式的加密協定\n\nref.\n1. SOCKS\n2. 代理伺服器\n3. 實戰frp內網穿透　內部主機也能對外服務\n4. SOCKS5代理\n5. Golang動手寫一個Http Proxy\n\n題外話補充：\n正向代理：\n代理用戶端，讓伺服器不知道用戶的IP\n反向代理：\n代理伺服器端，讓用戶不知道伺服器端的IP\nref.\n- 用人類語言跟你說甚麼是正向代理(Forward Proxy)和反向代理(Reverse Proxy)",
		"tags": [ "note","🌐"]
},

{
		"title": "nginx的除錯筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🌐 Network/99. nginx的除錯筆記/",
		"content": "前言\n我一直對nginx不太熟，\n目前也只到了會用，大部分的指令看得懂。\n但湊在一起，還是要查一下。\n這次的錯誤就發生在我以為的問題上\n正文\n又多了一個新服務要架設，\n所以想當然，東西又丟到我這來了。\n這次跟以前的比較不一樣，這次是一個全新的服務。\n所以之前的自動佈署、yaml、nginx設定檔通通重來。\n雖然說是重來，但也只是複製貼上改一改，然後就炸了XDDDD\n我複製之前的設定檔像這個樣子。中間刪除了很多東西，\n請不要照抄。\nserver_tokens off;\nlog_format client '$remote_addr - $remote_user [$time_local] , '\n'http-host: &quot;$http_host&quot; , URL: &quot;$request&quot; , request-status : &quot;$status&quot; , '\n'body-byte: $body_bytes_sent ,http-referer: &quot;$http_referer&quot; ,'\n'user-agent: &quot;$http_user_agent&quot; , X-Forwarded-For : &quot;$http_x_forwarded_for&quot; , '\n' request-time: &quot;$request_time&quot; , response_time : &quot;$upstream_response_time&quot; ';\n\nclient_max_body_size 100m;\ngzip on;\ngzip_min_length 1k;\ngzip_buffers 4 16k;\ngzip_comp_level 5;\ngzip_types text/plain text/css application/x-javascript application/javascript application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;\n\n# GCP HTTP(S) LoadBalancer will add Via, but nginx check it to disable compress by default...\ngzip_proxied any;\ngzip_vary on;\n\nserver {\nlisten 80 default_server;\nlisten [::]:80 default_server;\naccess_log /dev/stdout client;\nerror_log /dev/stderr;\n\nindex index.html index.htm;\nlocation = /favicon.ico { access_log off; log_not_found off; }\nlocation = /robots.txt { access_log off; log_not_found off; }\n\nlocation /api/socket.io/ {\nproxy_pass http://rd5-api;\nproxy_set_header Host $host;\nproxy_set_header token $rd5_token;\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n# proxy_set_header X-NginX-Proxy true;\nproxy_set_header X-Real-IP $remote_addr;\n# proxy_set_header via 'proxy_pass/token-proxy ws';\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection &quot;Upgrade&quot;;\nproxy_read_timeout 1d;\n\n}\n\nlocation /50x.html {\nroot /usr/share/nginx/html/frontend-client/;\ninternal;\n}\nlocation /healthz {\naccess_log off;\nreturn 200 'ok';\n}\n\nlocation / {\nerror_page 418 = @fjysdz;\nrecursive_error_pages on;\n\nif ($host = 'yb-test.fjysdz.cn') {\nreturn 418;\n}\nrewrite &quot;^(.*)\\$&quot; $1 permanent; # 去除url尾部重複斜線\nroot /usr/share/nginx/html/frontend-client/$dir;\nadd_header via frontend-proxy;\ntry_files $uri $uri/ /index.html /index.htm =404;\n}\n\n}\n\n然後就出現錯誤了\n\n&quot;server_tokens&quot; directive is not allowed here\n\n我百思不得其解，我明明是照抄的怎麼會錯。\n後來將server_tokens搬進去 server 的區塊內，好了能跑了。\n但又爆出\n\n&quot;log_format&quot; directive is not allowed here\n\n我又只好在最外層加上http {} ，但心中一直有一個疑問，\n為什麼會失敗！？\n最後變成這樣\n\nevents {\nworker_connections 1024;\n}\nhttp {\n\nlog_format client '$remote_addr - $remote_user [$time_local] , '\n'http-host: &quot;$http_host&quot; , URL: &quot;$request&quot; , request-status : &quot;$status&quot; , '\n'body-byte: $body_bytes_sent ,http-referer: &quot;$http_referer&quot; ,'\n'user-agent: &quot;$http_user_agent&quot; , X-Forwarded-For : &quot;$http_x_forwarded_for&quot; , '\n' request-time: &quot;$request_time&quot; , response_time : &quot;$upstream_response_time&quot; ';\n\nclient_max_body_size 100m;\ngzip on;\ngzip_min_length 1k;\ngzip_buffers 4 16k;\ngzip_comp_level 5;\ngzip_types text/plain text/css application/x-javascript application/javascript application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;\n\n# GCP HTTP(S) LoadBalancer will add Via, but nginx check it to disable compress by default...\ngzip_proxied any;\ngzip_vary on;\n\nserver {\n\nserver_tokens off;\nlisten 80 default_server;\nlisten [::]:80 default_server;\naccess_log /dev/stdout client;\nerror_log /dev/stderr;\n\nindex index.html index.htm;\nlocation = /favicon.ico { access_log off; log_not_found off; }\nlocation = /robots.txt { access_log off; log_not_found off; }\n\nlocation /conf/domain {\naccess_log off;\ndefault_type application/json;\nadd_header 'Access-Control-Allow-Origin' '*';\nadd_header 'Access-Control-Allow-Methods' 'GET';\nreturn 200 '{&quot;domain&quot;:&quot;500015&quot;, &quot;site&quot;: &quot;porn1&quot; ,&quot;cdn&quot;:&quot;x-cdn-yb&quot;}';\n}\n\n# frontend-client\nerror_page 404 /404.html;\nerror_page 500 502 503 504 /50x.html;\nlocation /404.html {\nroot /usr/share/nginx/html/frontend-client/;\ninternal;\n}\nlocation /50x.html {\nroot /usr/share/nginx/html/frontend-client/;\ninternal;\n}\nlocation /healthz {\naccess_log off;\nreturn 200 'ok';\n}\n\nlocation / {\nrewrite &quot;^(.*)\\$&quot; $1 permanent; # 去除url尾部重複斜線\nroot /usr/share/nginx/html/$dir;\nadd_header via frontend-proxy;\ntry_files $uri $uri/ /index.html /index.htm =404;\n}\n\n}\n\n}\n\n跑是能正常跑了，但頁面載入一直出問題，\n因為nginx log正常了，就聯絡前端同仁幫看一下。\n他說從伺服器取得的css context-type有問題。\n查了一下，又發現一個東西沒帶。\ninclude /etc/nginx/mime.types;\n\n為什麼一個能跑一個不能跑？？？\n最後發現一個重點。\n結論\nNginx 的設定檔預設的位置是\n\n/etc/nginx/nginx.conf\n\n然後，這個檔案裡面會先寫好預設的設定。\nuser nginx;\nworker_processes auto;\n\nerror_log /var/log/nginx/error.log notice;\npid /var/run/nginx.pid;\n\nevents {\nworker_connections 1024;\n}\n\nhttp {\ninclude /etc/nginx/mime.types;\ndefault_type application/octet-stream;\n\nlog_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '\n'$status $body_bytes_sent &quot;$http_referer&quot; '\n'&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';\n\naccess_log /var/log/nginx/access.log main;\n\nsendfile on;\n#tcp_nopush on;\n\nkeepalive_timeout 65;\n\n#gzip on;\n\ninclude /etc/nginx/conf.d/*.conf;\n}\n\n所以能跑的服務，是因為他的設定檔放在 conf.d 裡面，\n所以是從 nginx.conf載入 /etc/nginx/conf.d/*.conf 的設定。\n而不能跑的服務，我是將設定檔直接覆蓋 /etc/nginx/nginx.conf ，\n所以上面那一些設定我都沒寫到。\nref.\n\nnginx中CSS重定向解析有問題——mime.type詳解\nNginx筆記（二）Nginx基礎",
		"tags": ["tcp_nopush", "gzip", "note","🌐"]
},

{
		"title": " 🍎 MAC MOC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🍎 Mac/0.ＭAC MOC/",
		"content": "安裝事項\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🍎 Mac/iTerm2安裝/\">iTerm2安裝</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🍎 Mac/鼠鬚管輸入法/\">鼠鬚管輸入法</a>\n小麥注音輸入法\ncheatSheet\nhidden Bar\nXnip\n\n之後可以試試看\n\n威注音輸入法 （小麥真的好用，資源又吃不兇）\nAmphetamine (進階能源節約模式)\n\nMAC 軟體\n\n<a class=\"internal-link is-unresolved\" href=\"/404\">64.mac用的fiddler，Whistle</a>\niina(影片播放)\nskitch(截圖)\nxnip(截圖)\n簡單截圖用\nshotter(多功能截圖)\n目前都改用這套了\ncoconutbattery(查詢ios裝置電池)\nhiddenbar\nRectangle（視窗管理)\n\nMAC操作\n\n<a class=\"internal-link is-unresolved\" href=\"/404\">mac在terminal開啓finder</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🍎 Mac/永久別名 alias/\">永久別名 alias</a>\nmac開啓隱藏檔案：Shift+Command+ .",
		"tags": [ "note","🍎"]
},

{
		"title": "iTerm2-Powerlevel10k設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🍎 Mac/iTerm2-Powerlevel10k設定/",
		"content": "前言\n我記得以前我的iTerm最後面有顯示時間，\n但目前用的沒有，\n亂找了一下，發現一個簡單plugin，不像<a class=\"internal-link\" data-note-icon=\"\" href=\"/🍎 Mac/iTerm2安裝/\">iTerm2安裝</a>，\n需要自己手動裝字體。\n內容\n\n安裝好iTerm2後，參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/🍎 Mac/iTerm2安裝/\">iTerm2安裝</a>\n安裝 zsh主題 Powerlevel10k\n\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n\n設定zsh theme\n\nZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;\n\n開啟設定畫面\n可以選擇重開iTerm2 或是 執行指令\n\np10k configure\n\n再來就看他的說明選擇你想要的設定。\n以下是我目前的畫面\n\n最後一項，選擇要不要開啟 Instant Prompt\n根據文章說法，可以消除啟動時間過長的情況（如果你的套件很多時）。\n我選擇關掉，沒啥差別。\n如果想再重新設定參數的話，再執行一次指令 p10k configure\nref. 讓 macOS 的 Terminal 又潮又實用：手把手設定教學 iTerm2 + oh-my-zsh + Powerlevel10k\n故障排除\n\n我輸入任何指令，都會跳出錯誤\n\nprompt_context:2: command not found: prompt_segment\n開啟 ~/.zshrc ，尋找關鍵字 prompt_segment，\n把那整個function砍了。\n\nref. prompt_context:2: command not found: prompt_segment",
		"tags": [ "note","🍎"]
},

{
		"title": "iTerm2安裝",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🍎 Mac/iTerm2安裝/",
		"content": "2024/05/27 更新\n換了一台mac mini，\n東西要重裝，然後發現，我指令都過時了，順便重新整理一下。\n\n安裝 Homebrew\n\n/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;\n\n安裝iTerm2\n\nbrew install --cask iterm2\n\n套用配色\n到iterm2 color schemes 選擇喜歡的配色後import。\n\n安裝ZSH\n\nbrew install zsh zsh-completions\n\n將預設終端機改爲ZSH\n\nsudo sh -c &quot;echo $(which zsh) &gt;&gt; /etc/shells&quot;\nchsh -s $(which zsh)\n\n安裝 Oh My ZSH\n\nsh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;\n\n套用主題\n\nopen ~/.zshrc\n\n找關鍵字， SH_THEME=\n修改成\nZSH_THEME=&quot;agnoster&quot;\n\n安裝 powerline font\nbrew install --cask font-sauce-code-pro-nerd-font\n\n參考上面圖片，改選擇Text，修改字體，字體名稱為 SauceCodePro Nerd Ford\n如果要找其他字體的話，搜尋關鍵字Nerd\nbrew search --cask nerd\n\n修改字體，不然有亂碼（已棄用）\n\n# clone 複製\ngit clone https://github.com/powerline/fonts.git --depth=1\n# install 安裝\ncd fonts\n./install.sh\n\n套用字體\niTerm → Preferences → Profiles → Text → Change Font→ Meslo\n\n程式碼高亮\n\nbrew install zsh-syntax-highlighting\n\n然後修改設定\nopen ~/.zshrc\n\n設定檔最後加上\n# For zsh syntax-highlighting\nsource /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n\nref.\n\n超簡單！十分鐘打造漂亮又好用的 zsh command line 環境\n用Oh My Zsh把iTerm變美美\n客製我的 CLI — 終於稍微搞懂 iTerm + ZSH",
		"tags": [ "note","🍎"]
},

{
		"title": "mac 永久別名 alias",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🍎 Mac/永久別名 alias/",
		"content": "修改路徑 ~/.zshrc\n然後輸入要新增的alias.\nalias tf=&quot;terraform&quot;",
		"tags": [ "note","🍎"]
},

{
		"title": "鼠鬚管輸入法",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🍎 Mac/鼠鬚管輸入法/",
		"content": "安裝 RIME\n\nbrew install --cask squirrel\n\n安裝完後需要登出再登入\n\n下載最新的檔案\n\nhttps://github.com/Ponpon55837/Squirrel/releases\n\nmac的話，先增加鼠鬚管的輸入法（在簡體中文內），再開啓他的用戶設定\n將(3)的內容全部刪除後，將(2)的檔案貼到上面\n然後點『重新部署』\nsquirrel.custom.yaml 修改主題或顏色",
		"tags": [ "note","🍎"]
},

{
		"title": " 🐧 Linux MOC",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/0.Linux MOC/",
		"content": "basic\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/132. shell script 雙引號與單引號/\">132. shell script 雙引號與單引號</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/133. shell script字串處理/\">133. shell script字串處理</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/163.Shell的區別及查詢/\">163.Shell的區別及查詢</a>\n\nSetting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/33. centos 7安裝redis 改port/\">33. centos 7安裝redis 改port</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/49.npm 與 yarn 比較/\">49.npm 與 yarn 比較</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/116. 安裝linux server 基本設定/\">116. 安裝linux server 基本設定</a>\n\nTools\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/16.dustise sleep 測試工具簡介/\">16.dustise sleep 測試工具簡介</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/51.curl 筆記/\">51.curl 筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/65. Linux command - grep/\">65. Linux command - grep</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/93. shellscript解析/\">93. shellscript解析</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/108. linux shell 進階指令/\">108. linux shell 進階指令</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/134. 關閉ssh連線後，仍可執行程式/\">134. 關閉ssh連線後，仍可執行程式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/165. Linux網路追蹤工具- MTR/\">165. Linux網路追蹤工具- MTR</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/171. linux top指令詳解/\">171. linux top指令詳解</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/173. Linux殭屍程式/\">173. Linux殭屍程式</a>\n\nTroubleshooting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/php exec ssh 發生 Host key verification failed 錯誤/\">php exec ssh 發生 Host key verification failed 錯誤</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/114. alpine linux 宣告變數,error bad variable name/\">114. alpine linux 宣告變數,error bad variable name</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/48.SSH無法連線，REMOTE HOST IDENTIFICATION HAS CHANGED/\">48.SSH無法連線，REMOTE HOST IDENTIFICATION HAS CHANGED</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/63.alpine linux apk ERROR unable to select packages/\">linux安裝測試package</a>",
		"tags": [ "note","🐧"]
},

{
		"title": "linux shell 進階指令",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/108. linux shell 進階指令/",
		"content": "碎碎念\n我也不知道這算不算進階？\n但平常我應該不會用，本文有\n\n計算某個開頭的tag數量有多少\nfor迴圈的資料寫到文字檔\n取得檔案詳細時間\n取得檔案列表後刪除\nxargs 參數\n只顯示資料夾\n只顯示檔案\nls參數\n計算目錄底下的檔案數\n查詢使用者\n消耗最多CPU的process\n\n正文\n計算某個開頭的tag數量有多少\nfor tag in $(git tag --sort -version:refname -l &quot;1.1.*&quot;); do\necho $tag;\nlet i++\ndone\necho $i;\nunset i;\n\nfor迴圈的資料寫到文字檔\nfor i in {20221201..20221202}\ndo\nfor j in {00..01}\ndo\necho &quot;mkdir $i$j&quot;\ndone\ndone &gt; 1.txt\n\n取得檔案詳細時間\nls --full-time | sort -k6\nor\nstat file.txt\n\n取得檔案列表後刪除\nls | grep 4793877c | xargs -p rm -f\n\nxargs 參數\n\n-I {} 將指令帶到後面的{}執行\n\nls nfs | head -10 | xargs -t -I {} ls nfs/{}\n\n-t 顯示執行的指令\n-p 執行前的確認\n\nref.\n\nLinux 系統 xargs 指令範例與教學\nLinux xargs 命令\n\n只顯示資料夾\nls -l | grep\n{ #d}\n# 利用詳細資訊，判斷開頭是不是 d(directory)\nls -d */\nls -F | grep \\/$ # 檔案名稱的最後如果有/表示是folder\n\n只顯示檔案\nls -F /etc | grep -v '/$'\n\nls參數\n\n-l 顯示檔案與目錄的詳細資訊\n-a 顯示隱藏的檔案與目錄\n-h 輸出的資訊以比較容易閱讀的格式呈現\n-F 檔案名稱的後面加上檔案類型的標示字元\n-r 檔案的列表以反向的排序列出\n-R 用遞迴的方式列出所有子目錄的檔案\n-S 檔案依照檔案的大小來排序\n-d 只列出目錄，後面不加上*/ 無法顯示\n--full-time 顯示完整時間\n\n計算目錄底下的檔案數\n find ./ -type f -name *.*| wc -l\n\nfind [obj_directory] [-type f] [-name filename]\nfind參數說明：\nobj_directory：要搜尋的路徑\n-type：要搜尋的類型，f=檔案，d=目錄\n-iname：不區分大小寫\n-name：後面接檔名，可加萬用字元(*)，搜尋類似檔名的檔案\nwc [-l]\nwc參數說明：\n-c: 統計位元組數\n-l：統計行數\n-m：統計字元數。不能與 -c 一起使用。\n-w 統計字數。一個字被定義為由空白、跳格或換行字元分隔的字串。\n-L 列印最長行的長度。\n-help 顯示説明資訊。\nref.\n[Linux] 計算目錄底下的檔案數\n查所有使用者\ncat /etc/passwd | awk -F: '{print $1}'\n\nref. How To List Users and Groups on Linux\n刪除使用者\nuserdel -r &lt;userID&gt;\n\nref. [GCP]遇到permission denied (publickey)怎麼辦？\nref.\n\nLinux 的 ls 指令教學與常用範例整理\n管線/重新導向\n\nlinux何時安裝\nstat / | awk '/Birth: /{print $2 &quot; &quot; substr($3,1,5)}'\n\nref. Linux Installation Date: How to Discover Your System’s Age\n\ncondition條件判斷\n\nif [[ ! -d &quot;$1&quot; ]]; then\nmkdir -p &quot;$1&quot;\nfi\n\n可以使用 兩個 [[ 或 一個 [ ，\n代表的意思差不多，只是兩個 [ 算是新版用法，\n他可以多做字串的比較操作== , != ,=~ 。\n-d 是一個條件測試標誌，檢查是否為目錄\n其他的相似的標誌如下\n\n-d ，用於檢查指定路徑是否為目錄。\n-f：檢查指定路徑是否為普通檔案。\n-x：檢查檔案是否可執行。\n-r：檢查檔案是否可讀。\n-w：檢查檔案是否可寫。\n-s：檢查檔案是否存在且大小不為零。\n-e：檢查檔案是否存在。\n\nfunction 建立\nfunction bindmount() {\nif [[ ! -d &quot;$1&quot; ]]; then\nmkdir -p &quot;$1&quot;\nfi\n\nchown -R :100 &quot;$1&quot; &quot;$2&quot;\n\tmount --bind &quot;$1&quot; &quot;$2&quot;\n}\n\nbindmount &quot;/home/mike/upload&quot; &quot;/home/bms/mike&quot;\n\n傳參數進去時，記得在後面加上&quot; 雙引號，包起來，\n避免中間有特別的字元導致，輸入失敗。\n稍微提一下，mount --bind 前面是來源 ，後面是目的。\n我不太懂GPT的回答\n\n消耗最多CPU的process\nps H -eo pid,pcpu | sort -nk2 | tail\n\n對應的服務名稱\nps aux | fgrep &lt;PID&gt;",
		"tags": ["d", "note","🐧"]
},

{
		"title": "alpine linux 宣告變數,error bad variable name",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/114. alpine linux 宣告變數,error bad variable name/",
		"content": "碎碎念\n頭一次碰到這種坑，\n我linux有待加強 （哭\n正文\n先看執行結果，\n變數後面不能接空格，不然會出現錯誤。\n等號後面也不能接空格，不然會出現錯誤。\n平常程式寫太順，都習慣按空格了，\n結果....\n/ # export ab = &quot;hello world&quot;\nsh: export: : bad variable name\n/ # EXPORT ab = &quot;hello world&quot;\nsh: EXPORT: not found\n/ # export ab=&quot;helloworld&quot;\n/ # echo $ab\nhelloworld\n/ # export ab = &quot;helloworld&quot;\nsh: export: : bad variable name\n/ # export ab =&quot;helloworld&quot;\nsh: export: : bad variable name\n/ # export ab= &quot;helloworld&quot;\n/ # echo $ab\nhelloworlda",
		"tags": [ "note","🐧"]
},

{
		"title": "安裝linux server 基本設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/116. 安裝linux server 基本設定/",
		"content": "碎碎念\n好久沒弄ubuntu了，有些預設要設定的東西，\n還真的久沒用就忘記，\n筆記筆記。\n正文\n\n新增使用者\n\nsudo useradd it\n\n修改密碼\n\nsudo passwd it\n\nref. Linux 新增使用者 useradd 指令用法教學與範例\n\n指定帳號，輸入sudo免密碼\n\necho &quot;username ALL=(ALL) NOPASSWD: ALL&quot; &gt; /etc/sudoers.d/username\n\nref. 第8章：管理本地端主機之使用者與群組(二)\n\n更改固定ip，修改裏面的檔案 *.network-manager-all.yaml\n\ncd /etc/netplan/\n\nnetwork:\nversion: 2\nrenderer: NetworkManager\nethernets:\nens33:\ndhcp4: no\naddresses: [192.168.103.231/24]\ngateway4: 192.168.0.1\nnameservers:\naddresses: [8.8.8.8,8.8.4.4]\n\n完成後，先測試\nsudo netplan try\n\nref. [Ubuntu] 設定固定ip\n\n一般使用者執行docker\n\nsudo usermod -a -G docker ${USER}\n\n執行完後，需重新登入",
		"tags": [ "note","🐧"]
},

{
		"title": "shell script 雙引號與單引號",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/132. shell script 雙引號與單引號/",
		"content": "睡睡念\n最近寫CI/CD，碰到一堆shell script的指令，\n以前都是try error一路試過去的，\n沒碰到什麼太大問題，直到現在那一長串，我才終於發瘋。\n正文\n簡單說法\nshell script的 單引號 『 ' 』括起來的字，\n會保留引號內每個字元，就是說裏面的變數不會變。\nabc=&quot;a&quot;\necho '$abc';\n\n就是印出\n\n$abc\n\n而雙引號 『 &quot; 』 括起來的字，\n不會保留引號內的字元，就是說裏面的變數會跟着變\nabc=a\necho &quot;$abc&quot;\n\n印出\n\na\n\n所以看起來會像這樣\n\n如果變成單引號包雙引號，或是 雙引號包單引號\necho '&quot;$abc&quot;'\necho &quot;'$abc'&quot;\n\n$abc\n'a'\n\n詳細說法\n單引號：\nshell 將逐字解釋單引號內的封閉文字，並且不會插入任何內容，包括變數、反引號、某些 \\ 轉義符等，所有字元都會維持原始字面意義\n雙引號：\n允許 shell 解釋美元符號 ( $ )、反引號 ( ` )、反斜槓 ( \\ ) 和驚嘆號 (！）， 這些字元與雙引號一起使用時具有特殊含義，並且在顯示之前對其進行評估\nref.\n\nBash 程式設計教學與範例：單引號、雙引號、跳脫字元\n終於知道 Shell 中單引號雙引號的區別了\n\np.s 上面宣告的 abc 叫做本地變數(local variable)，\n如果用上了export 則是 環境變數(environment variable)\n常見變數錯誤\nA = B #=號前後不能有空格\n1A=B #變量名稱不能以數字開頭\n$A=B #變量的名稱裡有$\na=B #這跟a=b是不同的\n\nref. shell十三問之5:問var=value 在export前後的差在哪?",
		"tags": ["變量名稱不能以數字開頭", "變量的名稱裡有", "這跟a", "note","🐧"]
},

{
		"title": "shell script字串處理",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/133. shell script字串處理/",
		"content": "睡睡念\nshell script的字串處理，原本以爲只能用grep 跟 awk 走pipeline來用，\n原來還能直接透過${} 來處理，shell script的水好深阿...\n正文\n來個例子，\n有個tag 叫 dev_1.1.23 ，我只要後面的版號就好，該怎麼做。\n本來做法\n#!/bin/bash\nset -x\nabc=&quot;dev_1.1.23&quot;\necho $abc|grep -o '\\d.\\d.\\d.'\n\n利用 grep 取值，並只顯示符合正則的字串， 所以要用 -o\n\n1.1.23\n\n如果改用${}的文字非貪婪左刪除\n#!/bin/bash\nset -x\nabc=&quot;dev_1.1.23&quot;\necho ${abc#*_}\n\n1.1.23\n\n除了左刪除，也能做到右刪除\n# 小比對左\n## 大比對左\n% 小比對右\n%% 大比對右\n* 用來比對然後刪除\n\nps.\nset -x 是屬於xtrace的應用，用來除錯的。\n也可以在執行的時候，加上 bash -x curl.sh ，\n這樣使用\n詳細參考Shell script前言\nref. Linux Shell 指令碼中 ${} 用法",
		"tags": [ "note","🐧"]
},

{
		"title": "關閉ssh連線後，仍可執行程式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/134. 關閉ssh連線後，仍可執行程式/",
		"content": "睡睡念\n怎麼最近都在搞shell script阿..\n這次是要把gce的圖片搬到gcs上面，\n總計有三萬多張，一般執行時，如果碰到ssh斷線，執行就失敗了，\n所以要讓他就算ssh斷線，也仍會執行\n正文\n目前有兩種方式\n\nnohup\n算是最簡單，容易用的方式，直接 在後面接command就能用了，\n但我禮拜五掛着，然後禮拜一回來看，發現程式掛了，cpu還飆到100，\n就算用 指令找到pid後，強制刪除，還是有command在執行，最後只能重開。\n\n\tps -aux | grep &quot;copyfile.sh&quot;\n\tkill -9 &lt;PID&gt;\n\nref. SSH 遠端登出後繼續執行\n\nscreen\nyum直接搜尋會找不到package，需要先額外安裝epel-release\n\nyum install epel-release\nyum install screen\n\n使用方式\n先建立一個screen\n可指定名稱\nscreen -S &lt;name&gt;\n\n在screen裏面，執行程式，離開時按\nctrl+a -&gt; ctrl（不放）+d\n\n查看目前有哪些 screen\nscreen -ls\n\n回到剛剛的screen，可以上面指令看到這個screen的id是58245。\nscreen -r 58245\n\nref.\n\n【Linux】screen解決退出終端會話結束問題\n學會使用Screen,不再怕SSH中斷 Linux（Centos 7）",
		"tags": [ "note","🐧"]
},

{
		"title": "blog-16.dustise sleep 測試工具簡介",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/16.dustise sleep 測試工具簡介/",
		"content": "前言\n之前就有用過 dustise/sleep ，用for 迴圈來打 網址做測試，\n今天想說那裡面的http到底是哪一套工具，才發現原來裡面有那麼多的工具能用。\n2021.03.18更新：又找到了幾款不錯用的工具，同步增加\n正文\n裡面的工具列表，可以參考他的git，\n然後我們看到他的dockerfile,裡面有\nbind-tools curl wrk vim\ntcptraceroute iptables httpie bash\ntini apache2-utils stress-ng strace\n這次會抓幾個記錄一下\n\nbind-tools\n安裝nslookup用，可查詢域名\n\nnslookup tw.yahho.com\n2. curl\n取得網頁資料\ncurl ifconfig.me\n參數很多，通常會使用postman將要截取的資料轉成curl，再到command做測試。\n如果只是要簡單測試，可以加上 -v 看詳細內容\n3. wrk\n模擬多人來網站時的效能，有點像壓力測試工具。\nwrk -t12 -c400 -d30s -T30s --latency http://localhost/productpage\n# -t12 用 12 個線程\n# -c400 模擬 400 個併發連接\n# -d30s 持續 30 秒\n# -T30s 設定超過 30 秒就算連接超時\n# --latency 響應時間的分佈情況\n\n<a class=\"internal-link is-unresolved\" href=\"/404\">[16.fig-1.jpg</a>\n\nLatency: 響應時間\nReq/Sec: 每個線程每秒鐘的完成的請求數\nAvg: 平均值\nStdev (Standard Deviation): 即標準偏差，是統計學的一個名詞，這裡表示請求響應時間的離散程度，值越大代表請求響應時間的差距越大，系統的響應約不穩定。\nMax: 最大值\n+/- Stdev: 正負一個標準差佔比\nLatency Distribution: 50% 在 3.23s 以內完成 / 99% 在 22.27s 以內完成\nSocket errors: 分為 連接錯誤, 讀取錯誤, 寫入錯誤, 超時錯誤\nRequests/sec: 每秒請求數量，也就是並發能力\n\n更詳細的說明，請參考：HTTP Benchmark 工具 Wrk\n\nvim\n文字編輯器，就不介紹了。\n\ntcptraceroute\n路由追蹤器，追蹤網路死在哪一層\n\ntraceroute tw.yahoo.com\n\niptables\n路由表\n\nhttpie\n之前使用\nfor i in seq 20;do http --body http://ipaddress/api/v1/Health/health; done;\n打網址的指令。\nhttpie 是一個 HTTP 的命令行客戶端。其目標是讓 CLI 和 web 服務之間的交互儘可能的人性化。\n想用來取代wget ,curl 等功能，然後畫面比較美觀，如果回來的是json，那差異會更大。\n\n[[16.fig-2.jpg]]\n下載\nhttp --download example.org/file\n只看header\nhttp -h http://ipaddress/productpage\nref.\nHTTPie:超爽的HTTP命令行客戶端\nHTTPie 官方文檔中文翻譯版\n\n同場加註\njid\nhttps://github.com/simeji/jid\njq\nhttps://stedolan.github.io/jq/manual/#Basicfilters\nhey\nhttps://github.com/rakyll/hey\ne.g. 加上 method ,content/type ,dataraw\nhey -z 3m -q 10 -c 3 -m POST -T 'application/json' -d '{ &quot;method&quot;: &quot;GetUrlForWard&quot;, &quot;host&quot;: &quot;landing500023.com&quot;}' https://landing500023.com/query.php",
		"tags": [ "note","🐧"]
},

{
		"title": "Shell的區別及查詢",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/163.Shell的區別及查詢/",
		"content": "前言\n在測試gcloud安裝後，還是找不到gcloud command ，\n於是開始了檢查之路。\n正文\n熟linux的應該都知道sh只是個代稱，你的sh是bash 、 zsh 或 dash，\n都有可能。\n所以判斷你的sh是哪一種\necho $SHELL\n\n然後你的sh會決定你的設定檔名稱是哪一個。\n例如\nbash =&gt; ~/.bashrc\nzsh =&gt; ~/.zshrc\n\n如果指令新增後，不想重啓Terminal的話，\n可使用下面指令，將被指定目錄的bash讀入目前的環境中。\nsource ~/.bashrc\n\nref.\n\ngcloud command not found - while installing Google Cloud SDK",
		"tags": [ "note","🐧"]
},

{
		"title": "Linux路由追蹤工具",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/165. Linux網路追蹤工具- MTR/",
		"content": "mtr\n\nMTR 在一開始會針對指定的主機，以 traceroute 找出中間的每一個網路節點（閘道器、路由器、橋接器等），然後使用 ping 去檢查每一個節點的網路連線狀況\n\n補充，traceroute -P -t 1935 &lt;ip&gt; 可透過指定port測試連線\n安裝方式\n# Debian/Ubuntu\nsudo apt install mtr\n\n# RHEL/CentOS/Fedora\nsudo yum install mtr\n\n# MAC\nbrew install mtr\n\nMAC安裝後，無法執行，因爲安裝的路徑是在 /usr/local/sbin底下，\n找尋自己的shell<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/163.Shell的區別及查詢/\">163.Shell的區別及查詢</a>，增加指定路徑\nvim ~/.bash_profile\nexport PATH=$PATH:/usr/local/sbin\n\n使用 sudo 執行\nsudo mtr -c 5 -wbr https://daimom.vercel.app &gt; output.txt\n\n參數介紹\n\nmtr -h 提供幫助命令\n\nmtr -v 顯示mtr的版本資訊\n\nmtr -r 以報告模式顯示\n\nmtr -s 用來指定ping封包的大小\n\nmtr -n no-dns不對IP地址做域名解析\n\nmtr -a 來設定傳送封包的IP地址 這個對一個主機由多個IP地址是有用的\n\nmtr -i 使用這個參數來設定ICMP返回之間的要求默認是1秒\n\nmtr -4 IPv4\n\nmtr -6 IPv6\n\nmtr -c 設定每秒傳送封包的數量\n\nmtr -T 使用TCP連線\n\nmtr -P 指定port\n\n數據所代表的意思\n以下是各個英文字母所代表的欄位：\nRTT (Round Trip Time) 亦可稱為往返時間\n\nL：封包遺失率（Loss ratio）。\nD：封包遺失數（Dropped packets）。\nR：封包接收數（Received packets）。\nS：封包發送數（Sent Packets）。\nN：最新的 RTT（Newest RTT，單位為 ms）。\nB：最佳的 RTT（Min/Best RTT，單位為 ms）。\nA：平均的 RTT（Average RTT，單位為 ms）。\nW：最差的 RTT（Max/Worst RTT，單位為 ms）。\nV：標準差（Standard Deviation）。\nG：幾何平均（Geometric Mean）。\nJ：目前的 Jitter 值（Current Jitter）。\nM：平均的 Jitter 值（Jitter Mean/Avg.）。\nX：最差的 Jitter 值（Worst Jitter）。\nI：Interarrival Jitter\n\nref.\n\nMTR 追蹤網路節點\nMTR：Linux 網路診斷工具使用教學\nmac 安裝 mtr",
		"tags": [ "note","🐧"]
},

{
		"title": "linux top指令詳解",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/171. linux top指令詳解/",
		"content": "睡睡唸\nlinux常用 top ，看目前os裏面哪個程式資源吃比較兇，\n但每次都是只看固定幾個欄位，\n最近剛好喵到完整的說明。\n正文\n一般top會長這樣\n\n第一行\ntop\n\n15:40:37\n目前時間\n\nup 6 days\n已開機時間\n\n3 users\n目前登入的使用者數量\n\nload average\n1 min,5 min,15min的系統瓶均負載\n\n第二行\nTasks\n\n419 total\n總處理程序\n\n1 running\n執行中的程式\n\n418 sleeping\n睡眠中的程式\n\n0 stopped\n停止中的程式\n\n0 zombie\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/173. Linux殭屍程式/\">173. Linux殭屍程式</a>\n\n第三行\n%Cpu(s):\n\n0.2 us\n使用者佔用的CPU\n\n0.1 sy\n核心空間佔用的CPU\n\n0.0 ni\n改變過優先順序的程式佔用的CPU\n\n99.7 id\n閒置的CPU\n\n0.0 wa\nIO等待佔用的CPU\n\n0.0 hi\n硬體中斷(Hardware IRQ)佔用C的PU\n\n0.0 si\n軟體中斷(Software IRQ)佔用的CPU\n\n0.0 st\n偷取時間(Steal Time)佔用的CPU\n\nHardware IRQ : 硬體中斷是從外部裝置（如磁碟控製器、網路卡、鍵盤控製器、外部計時器、硬體感應器等）發送到處理器的電子警報訊號\nref . Hardware Interrupt time – ‘hi’ time in top\nSoftware IRO：軟體中斷從程式引發的中斷行爲\nref. Software Interrupt time – ‘si’ time in top\nSteal CPU Time ：虛擬機器的CPU在虛擬機監護程式正在處理其他虛擬機器時，等待實際CPU的時間百分比。\nref. Steal CPU time – ‘st’ time in top\n竊取 CPU 時間 – 頂部的「st」時間\n第四行\nMiB Mem\n\n64207.4 total\n物理記憶體總量\n\n7866.7 free\n可用記憶體\n\n16649.0 used\n已使用記憶體\n\n39691.6 buff/cache\n緩存的記憶體量\n\n第五行\nMiB Swap\n\n2048.0 total\n虛擬記憶體總量\n\n2048.0 free\n可用虛擬記憶體總量\n\n0.0 used\n已使用虛擬記憶體\n\n46765.3.0 avail Mem\n可用記憶體[free+(buff/cache)]\n\n補充 Linux的計算方式跟windows的不一樣，\n當 『可以被應用程式使用，但被暫時借去做別的用途』，\n在Linux裏面也是顯示為used。\n所以要查目前記憶體使用量可以改用htop或free -m\nref . Linux 的記憶體快取（Cache Memory）功能：Linux 系統把記憶體用光了？\n第七行\n預設的欄位名稱\n\nPID: 執行任務的 Process ID\nUSER: 執行任務的使用者名稱\nPR: 任務的優先度 (Priority)\nNI: 任務的 Nice Value，負的值代表優先度高，正的值代表優先度低\nVIRT: 總共用到多少 kB 虛擬記憶體 (Virtual Memory)\nRES: 實體記憶體 (Resident Size) 大小 kB\nSHR: 總共用到多少 kB 的共享記憶體 (Shared Memory)\nS: 狀態 (Status)\n\nR 代表執行中\nD 代表不可中斷睡眠 (不可被 signal 打斷通常在等 I/O)\nS 代表睡眠 (可被喚醒)\nT 中斷中或停止，可能是被 SIGSTOP 或 SIGTSTP 停止，或是被 degubber 中斷 (ptrace)\nZ 代表殭屍，通常發生在 Child 已經執行完，等待 Parent 結束或回收\n\n%CPU: 佔用到多少 CPU %，注意到一個核心是 100%，所以多核心是可以操過 100% 的\n%MEM: 佔用到多少全部記憶體多少比例\nTIME+: 程式已經執行多少時間，單位1/100秒\nCOMMAND: 任務的指令名稱\n\n非預設的欄位名稱，\n叫出來的方式為 f -&gt; 選擇欄位(space) -&gt; 離開(q)\n\nPPID: 執行任務的 Process ID的父親 （原因參考<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/173. Linux殭屍程式/\">173. Linux殭屍程式</a>\nUID:執行程式的使用者ID\nRUID: 真正的使用者ID\nRUSER: 真正的使用者\nSUID: 暫存使用者ID\nSUSER: 暫存使用者名稱\nGID: 群組ID\nGROUP: 群組名稱\nPGRP: Process Group Id\nTTY: 控制終端的名稱\nTPGID: Tty Process Group Id\nSID: session ID （一個login裏面的所有process)，有點像 <a class=\"internal-link\" data-note-icon=\"\" href=\"/🐧 Linux/134. 關閉ssh連線後，仍可執行程式/\">134. 關閉ssh連線後，仍可執行程式</a>裏面的screen\nnTH: 與一個進程關聯的執行緒數量\np: 最後使用的CPU\nTIME: 程式已經執行多少時間，單位 1秒\nSWAP: 當前被移到交換區的記憶體大小\nCODE: 目前用於執行程式碼碼的物理記憶體\nDATA:一個程式保留的私有記憶體量\nnMaj:已發生的主要分頁錯誤次數\nnMin:已發生的次要分頁錯誤次數\nnDRT: （已棄用）\nWCHAN: 顯示了每個進程或執行緒目前正在等待的內核函數或事件的名稱或指針\nFlags:目前任務的排程標誌，以十六進製表示，並且去掉了零\nCGROUPS:表示程式所在的控制組的名稱\nSUPGIDS:表示進程所在的父控制組的 ID\nSUPGRPS:表示程式所在的父控制組的名稱\nTGID: Thread Group Id\nOOMa: 用來調整每個進程的 OOMS 值，以影響其被終止的機率\nOOMs: 記憶體分數，評估進程或任務對內存需求的數值\nENVIRON: 顯示所有的環境變數\nvMj: 主要分頁錯誤計數增量\nvMn: 次要分頁錯誤計數增量\nUSED: 記憶體 Res+Swap Size\nnsIPC: IPC 命名空間，用於隔離進程間通訊資源，確保它們在不同的命名空間中互不干擾\nnsMNT: MNT 命名空間，用於隔離和管理不同命名空間中的檔案系統掛載點資訊\nnsNET: NET 命名空間，用於隔離和管理不同命名空間中的網絡資源和網絡設置\nnsPID: PID 命名空間，用於隔離不同命名空間中的進程識別號，確保它們在各自的命名空間中具有唯一的 PID\nnsUSER:USER 命名空間，用於隔離不同命名空間中的用戶和用戶許可權，確保它們在各自的命名空間中具有獨立的用戶身份和權限\nnsUTS:UTS (UNIX Time-sharing System)命名空間，用於隔離不同命名空間中的主機名和域名設置，確保它們在各自的命名空間中具有獨立的組態\nLXC: LXC(Linux Containers)容器名稱\nRSan: 代表一個進程或任務正在使用的匿名內存的大小\nRSfd: 代表一個進程或任務正在使用的檔案內存的大小\nRSlk: 代表一個進程或任務正在使用的已鎖定頁面的大小\nRSsh: 代表一個進程或任務正在使用的共享內存的大小\nCGNAME: 控制組名稱，用於標識和管理相關的進程或資源的分組。\nNU: &quot;最後已知的 NUMA 節點&quot;\n\nref.\n\n記憶體都去哪了\ntop命令詳解示例\nUnix/Linux TOP 指令使用詳解\nubntu-top\nLInux中的process",
		"tags": [ "note","🐧"]
},

{
		"title": "殭屍程式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/173. Linux殭屍程式/",
		"content": "前言\n看文章看到一篇詳解，TOP指令，重新把linux的top理解一遍。\n然後看到一個怪名詞 zombie ，這是我理解的意思嗎！？\n正文\n簡單說有點像是windows 的應用程式，要死不死，卡在那邊當掉的情況。\n每一個process都會有一個parent process，\n當child process處於結束與尚未結束中間時會發生。\n詳細，請參考Day 17: 殭屍與孤兒\ntop 顯示PPID\n\n執行 top\n按下f\n選擇需要的欄位，按d 或 space 選擇\n按下→ 鍵盤的方向鍵，然後按上下鍵可以調整順序\n按q離開",
		"tags": [ "note","🐧"]
},

{
		"title": "centos 7安裝redis 改port",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/33. centos 7安裝redis 改port/",
		"content": "前言\n正文\n最簡單地安裝方式，\n\nyum install redis\n\n編輯 /etc/redis.conf\n找到 port 6379 ，將它改為 port 8090\n\n重啟服務 sudo systemctl restart redis\n\n發現服務無法啟動(fig.1)，查log(fig.2)，發現沒有權限。\n\n[[33.fig-1.jpg]]\n[[33.fig-2.jpg]]\n\nselinux的問題，要麻關掉，要麻新增port\n\nsudo: semanage: command not found\n\n安裝\nyum install policycoreutils-python\n\n然後執行\nsudo semanage port -a -t redis_port_t -p tcp 8090\n\nref.\nRedis: Creating Server TCP listening socket *:6388: bind: Permission denied\n解決 semanage command not found\n\n重開redis。\n\nsudo systemctl restart redis\nsudo systemctl status redis\n\nref.\n安裝 Redis 時遇到的錯誤排除",
		"tags": [ "note","🐧"]
},

{
		"title": "SSH無法連線，REMOTE HOST IDENTIFICATION HAS CHANGED",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/48.SSH無法連線，REMOTE HOST IDENTIFICATION HAS CHANGED/",
		"content": "前言\n正文\n出現下圖的錯誤，\n[[48.fig-1.jpg]]\n\n一般都是用文字編輯器開啟 /root/.ssh/known_hosts\n將舊10.7.10.48 的key刪除。\n\n但這次碰到沒有權限，只能下指令刪除了。\n\nssh-keygen -R 10.7.10.48",
		"tags": [ "note","🐧"]
},

{
		"title": "npm 與 yarn 比較",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/49.npm 與 yarn 比較/",
		"content": "前言\n正文\n\n回傳值\n方法名稱\n說明\n\nnpm install\nyarn install\n安裝 json.package 所有依賴\n\nnpm install [package]\n(N/A)\nYarn不支援直接安裝套件\n\nnpm install --save [package]\nyarn add [paakage]\n儲存在 json.package中的dependencies\n\nnpm install --save-dev [package]\nyarn add [paakage] --dev\n儲存在 json.package中的devDependencies\n\nnpm install --global [package]\nyarn global add [package]\n安裝在電腦全域中\n\nnpm uninstall [package]\n(N/A)\nYarn不支援直接安裝與移除套件\n\nnpm uninstall --save [package]\nyarn remove [package]\n移除dependencies某套件\n\nnpm uninstall --save-dev [package]\nyarn remove [package]\n移除devDependencies某套件\n\nrm -rf node_modules &amp;&amp; npm install\nyarn upgrade\n更新node_modules\n\nref.\n用Yarn取代npm加速開發",
		"tags": [ "note","🐧"]
},

{
		"title": "curl 筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/51.curl 筆記/",
		"content": "前言\n正文\n最近在K網路架構，有些東西有點概念了。\n此時來看這張圖終於瞭解一點。\n\nref. A Question of Timing\n先來看這張圖，\n有幾個關鍵字縮寫必須要先知道\nSYN ： 同步序列號\nACK ： 要求回應\nFIN ：結束\nSEQ ： 序列號\n這邊簡單講，要更詳細的內容，建議去看一下TCP 三向交握 (Three-way Handshake)\n在這張圖裡面沒有SEQ這個，因為沒畫出來，在SYN的時候，就會包含SEQ在裡面了。\n流程簡單說是這樣的，TCP是有順序性的。\n\nclinet 詢問DNS server ，網域與IP的對應\nclient 發出請求\nserver 回應\nclient 回應，我知道你回了。\nserver 開始建立ssl憑證\nclient 發出內容請求\nserver 回應內容\n結束\n\n好了，再來才是講curl ，\n先建立一個 txt檔，\n\\n\n# DNS 解析時間，也就是查詢到 IP 的時間\\n\ntime_namelookup: %{time_namelookup}\\n\n# TCP 連線時間，就是 TCP 三項交握的時間\\n\ntime_connect: %{time_connect}\\n\n# SSL 連線的時間\\n\ntime_appconnect: %{time_appconnect}\\n\n# 從開始到最後一個請求的時間，如果網頁有跳轉就會有時間\\n\ntime_redirect: %{time_redirect}\\n\n# 從開始到響應開始的時間\\n\ntime_pretransfer: %{time_pretransfer}\\n\n# 從開始到響應開始傳輸的時間\\n\ntime_starttransfer: %{time_starttransfer}\\n\n----------\\n\n# 整體時間\\n\ntime_total: %{time_total}\\n\n\\n\n\ncurl -w @curl-format.txt -o /dev/null https://daimom3020.blogspot.com/\n\n這邊要注意， @curl-format.txt 這邊不能使用路徑方式去找檔案。如果怕改天找不到檔案的話，\ncurl 可以改成下面的指令\ncurl -o /dev/null https://daimom3020.blogspot.com/ -w &quot;\n# DNS 解析時間，也就是查詢到 IP 的時間\ntime_namelookup: %{time_namelookup}\n# TCP 連線時間，就是 TCP 三項交握的時間\ntime_connect: %{time_connect}\n# SSL 連線的時間\ntime_appconnect: %{time_appconnect}\n# 從開始到最後一個請求的時間，如果網頁有跳轉就會有時間\ntime_redirect: %{time_redirect}\n# 從開始到響應開始的時間\ntime_pretransfer: %{time_pretransfer}\n# 從開始到響應開始傳輸的時間\ntime_starttransfer: %{time_starttransfer}\n----------\n# 整體時間\ntime_total: %{time_total}\n\\n&quot;\n\n搭配上方的圖片來看，\n這邊顯示的時間是累加的，如果要算出區間的時間，\n需要自行做相減。\n例如，\n要知道server處理資料的時間多長，\n可使用 time_starttransfer - time_pretransfer ，\n就可得知server處理資料的時間。\nref.\n\nCURL 詳細資料包含 TCP 與 SSL 連線時間\nCURL -w 參數詳解\ncurl命令\n15 Special Characters You Need to Know for Bash\nCommand symbol 符號_Linux\ncurl 所有參數介紹\n\n驗證網站是否為http2 ，\n簡單說明\n-v 顯示呼叫的步驟\n-s Silent模式。不輸出任務內容\n-o 指定輸出目錄\n只用 -s 的話，還是會輸出內容。\n只用 -o的話，不會輸出內容\n\tcurl https://www.cloudflare.com --http2 -vso /dev/null\n\n是的話，會出現 using http2",
		"tags": [ "note","🐧"]
},

{
		"title": "編譯ffmpeg 硬體加速的坑",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/6. 編譯ffmpeg 硬體加速的坑/",
		"content": "前言\n最近的案子..要幫同仁搞ffmpeg的機器，如果沒有使用顯卡加速是個很愉快的事情\n但扯到顯卡就滿滿的坑了。\n正文\n環境：GKE，開啟GPU節點，詳細的使用方法可參考之前寫過的 GKE使用GPU\n環境設定那部分，有開工單去問google，他們有說要改文件。\n1.編譯時找不到 libnpp\nERROR: libnpp not found\n安裝 CUDA Toolkit\nref.\nHow to install CUDA 9.2 on Ubuntu 18.04\nCUDA Toolkit 11.2 Downloads\nwget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux.run\nsudo sh cuda_11.2.0_460.27.04_linux.run\n安裝時，不要另外裝驅動，只裝cuda\n\n參數設定編碼允許png,出現錯誤\n--enable-decoder=png --enable-encoder=png\n\nDisabled png_decoder because not all dependencies are satisfied: zlib\n需要先安裝 zlib\nref. ffmpeg &amp; png watermark issue\napt-get install zlib1g-dev\n\n編譯完成後，執行發生錯誤，找不到libnpp\nffmpeg: error while loading shared libraries: libnppig.so.11: cannot open shared object file: No such file or directory\n\nref. libnppig.so.8.0 Missing FFmpeg\n設定參數\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.2/targets/x86_64-linux/lib\n\n驅動不支援\nDriver does not support the required nvenc API version. Required: 11.0 Found: 9.0\n\nThe minimum required Nvidia driver for nvenc is 455.28 or newer\nError initializing output stream 0:0 -- Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height\n切到 nv-codec-headers 的資料夾底下，切換分支\ngit checkout sdk/9.0\nref. [記錄一個bug]關於運行ffmpeg with cuda出錯問題：Driver does not support the required nvenc API version..... [已解決|記錄一個bug]關於運行ffmpeg with cuda出錯問題：Driver does not support the required nvenc API version..... [已解決]\n\n使用浮水印轉檔出現錯誤\nImpossible to convert between the formats supported by the filter 'graph 0 input from stream 0:0' and the filter 'auto_scaler_0'\nError reinitializing filters!\nFailed to inject frame into filter network: Function not implemented\n\n之前寫的\nffmpeg -hwaccel cuvid -y -i 49805.mp4 -i logo.png \\\n-filter_complex &quot;[0]scale='min(1280,iw)':-1[bg];[1][bg]scale2ref=w='iw*15/100':h='iw*15/100*166/446'[wm][vid];[vid][wm]overlay=10:10&quot; \\\n-c:v h264_nvenc ezio.mp4\n\n會發生上面的錯誤(auto_scaler_0)，但只要將 cuvid改成 nvdec 就正常了。\nffmpeg -hwaccel nvdec -y -i 49805.mp4 -i logo.png \\\n-filter_complex &quot;[0]scale='min(1280,iw)':-1[bg];[1][bg]scale2ref=w='iw*15/100':h='iw*15/100*166/446'[wm][vid];[vid][wm]overlay=10:10&quot; \\\n-c:v h264_nvenc ezio.mp4\n\nref. Filter complex through CUDA hevc_cuvid with FFMPEG input 4k hevc, help needed\n\n轉碼時查詢 詳細的log記錄\n在ffmpeg的後面加上 -loglevel debug\nref. FFMPEG的默認像素格式將圖像堆棧編碼為電影\n\nffmpeg 常用指令\n\nffmpeg -hwaccels 查詢能用的硬體加速\nffmpeg -codecs | grep cuvid 查看ffmpeg支持的cuvid的編碼格式\nref. Ubuntu 16.04下編譯ffmpeg支持CUDA下的cuvid vnenc和NPP\n使用GPU硬件加速FFmpeg視頻轉碼及排坑\n其他參考項目\nref. Matching CUDA arch and CUDA gencode for various NVIDIA architectures\nFFmpeg再學習 -- Linux 安裝說明(參數說明)\n在k8s中調用NVIDIA-GPU\n在k8s中調用NVIDIA-GPU(Dockerfile)",
		"tags": ["0", "note","🐧"]
},

{
		"title": "alpine linux apk ERROR unable to select packages",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/63.alpine linux apk ERROR unable to select packages/",
		"content": "前言\n這好像是在查問題的時候碰到的，\n因為某些奇怪的網路問題，\n導致必須抓封包查詢，然後又因為在GKE的內網，\n不能在本機撈.\n正文\n要安裝一個linux類似fiddler的軟體，\n搜尋alpine liunx 的 repository有找到，\n但一直無法安裝。\n會出現\nERROR: unable to select packages:\nmitmproxy (no such package):\nrequired by: world[mitmproxy]\n\n後來才發現，他的repository是 測試的\n\n所以安裝時，要指定 repository的位置\napk add mitmproxy --repository=http://dl-cdn.alpinelinux.org/alpine/edge/testing/\n\nref.\n\nERROR: unable to select packages error on Alpine Linux\nHow to add a edge/testing package to Alpine Linux?\nalpine Linux - package search\nalpine Linux - Enable Community Repository",
		"tags": [ "note","🐧"]
},

{
		"title": "Linux command - grep",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/65. Linux command - grep/",
		"content": "前言\n正文\n用途\n通常是用在某些指令後的處理上。\n以前的話，通常是把指令執行完後，\n再將內容貼去excel做處理\n這邊也很常用到pipe ，\n簡單的例子，要找出這個資料夾內有多少檔案。\nls | wc\n\n會有三個值， 換行數、字數、字元數\n但如果要針對 副檔名是 jpg的話，\n就必須要用grep 在做加總\nls | grep -c \\.jpg\n\n如果只單純做加總的話，後面必須給個要加總的東西。\nls | grep -c .\n\n後面的參數為 正則表示法\nref.\n\nLinux I/O 輸入與輸出重新導向，基礎概念教學\nLinux 使用 wc 指令計算字數、行數教學與範例\nLinux grep 命令\nLinux 匹配文字 grep 指令用法教學與範例",
		"tags": [ "note","🐧"]
},

{
		"title": "shellscript解析",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/93. shellscript解析/",
		"content": "前言\n正文\n#!/bin/bash\nF=&quot;proxy-linux-amd64.tar.gz&quot;\nset -e\nif [ -e /tmp/proxy ]; then\nrm -rf /tmp/proxy\nfi\nmkdir /tmp/proxy\ncd /tmp/proxy\n\necho -e &quot;\\n&gt;&gt;&gt; downloading ... $F\\n&quot;\nmanual=&quot;https://snail.gitee.io/proxy/manual/&quot;\nLAST_VERSION=$(curl --silent &quot;https://mirrors.host900.com/https://api.github.com/repos/snail007/goproxy/releases/latest&quot; | grep -Po '&quot;tag_name&quot;:&quot;\\K.*?(?=&quot;)')\nwget -t 1 &quot;https://mirrors.host900.com/https://github.com/snail007/goproxy/releases/download/${LAST_VERSION}/$F&quot;\n\necho -e &quot;&gt;&gt;&gt; installing ... \\n&quot;\n# #install proxy\ntar zxvf $F &gt;/dev/null\nset +e\nkillall -9 proxy &gt;/dev/null 2&gt;&amp;1\nset -e\ncp -f proxy /usr/bin/\nchmod +x /usr/bin/proxy\nif [ ! -e /etc/proxy ]; then\nmkdir /etc/proxy\ncp blocked /etc/proxy\ncp direct /etc/proxy\nfi\nif [ ! -e /etc/proxy/proxy.crt ]; then\ncd /etc/proxy/\nproxy keygen -C proxy &gt;/dev/null 2&gt;&amp;1\nfi\nrm -rf /tmp/proxy\nversion=`proxy --version 2&gt;&amp;1`\necho -e &quot;&gt;&gt;&gt; install done, thanks for using snail007/goproxy $version\\n&quot;\necho -e &quot;&gt;&gt;&gt; install path /usr/bin/proxy\\n&quot;\necho -e &quot;&gt;&gt;&gt; configuration path /etc/proxy\\n&quot;\necho -e &quot;&gt;&gt;&gt; uninstall just exec : rm /usr/bin/proxy &amp;&amp; rm -rf /etc/proxy\\n&quot;\necho -e &quot;&gt;&gt;&gt; How to using? Please visit : $manual\\n&quot;\n\n幾個不常用到的指令\n\ntar :解壓縮\n\n/dev/null ，這個在linux上面表示 黑洞，不做任何處理。\n所以 tar zxvf 表示說，解壓縮F&gt;/dev/null表示說，解壓縮F的檔案，但不印出任何的訊息\n\nset -e 在 &quot;set -e&quot;之後出現的代碼，一旦出現了返回值非零，整個腳本就會立即退出。\n\nset +e 在 &quot;set +e&quot;之後出現的代碼，一旦出現了返回值非0，腳本還是會繼續執行\n\nif -e ...判斷此目標是否存在，存在回true\n\nref.\n- “&gt; /dev/null 2&gt;&amp;1” 的意思\n- 學習筆記： shell 中的 set -e ， set +e 用法\n- What does &quot;if [ -e $name ]&quot; mean? Where $name is a path to a directory",
		"tags": ["install", "note","🐧"]
},

{
		"title": "1.php exec ssh 發生 Host key verification failed 錯誤",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐧 Linux/php exec ssh 發生 Host key verification failed 錯誤/",
		"content": "前言\n前幾天一直在找php的exec坑，\n最後終於解了，\n記錄一下。\n正文\n首先來段code，\n這段的用途是要把 檔案從A 複製到 B ，\n如果直接在A執行 scp 的指令，是正常的。\n&lt;?php\nexec(&quot;scp -i /var/www/.ssh/id_rsa -r /var/www/html/WebService/upload/tmp/fe8abe8e88fb3a2d88cc17a45a7575b1.png www-data@10.7.0.17:/var/www/html/tmp/ 2&gt;&amp;1&quot;, $output, $return_var);\nprint_r($output);\necho &quot;return&quot; .$return_var;\necho &quot;&lt;br&gt;&quot;;\nexec(&quot;ssh -i /var/www/.ssh/id_rsa www-data@10.7.0.17 2&gt;&amp;1&quot;, $output,$return_var);\nprint_r($output);\necho &quot;return:&quot;.$return_var;\n?&gt;\n\n這段程式，有額外加上 2&gt;&amp;1 這是將錯誤印出來的方式\n\n使用 2&gt;&amp;1, 命令就會輸出shell執行時的錯誤到$output變量, 輸出該變量即可分析。\n備註: exec有3個參數，第一個是要執行的命令，第二個是參數是一個數組，數組的值是由第一個命令執行後生成的，第三個參數執行的狀態，0表示成功，其他都表示失敗。\n(ref. https://blog.csdn.net/zy112289/article/details/52671373)\n然後終於有錯誤訊息了，這個錯誤卻讓我很慌....\nHost key verification failed\n查了一下是驗證的錯誤，都說基本上就是刪除 known_host 就好了。\n但刪了主機、本機都一樣\n(ref. https://blog.csdn.net/iteye_19045/article/details/103704924)。\n使用find 去尋找所有的*.pub檔案，根據裡面的內容做修改，並在主機及本機加上權限\n(ref. https://xenby.com/b/220-教學-產生ssh-key並且透過key進行免密碼登入)\n/var/www/.ssh/id_rsa，有參照之前主機設定，權限設定成 www-data（此時我沒注意到，另一個關鍵點）。\n解決方案\n1.先登入最高權限\nsudo -i\n2.切換成www-data\nsu www-data\n3.先連線\nssh 10.7.0.17\n釐清原因\n\n一開始我以為只要是使用者帳號，都應該會在 home底下有使用者的資料夾，但不知道www-data，\n不會建立這個資料夾，預設截取 /var/www/.ssh的 id_rsa\nexec 可能讀不到 /var/www/.ssh/id_rsa 的檔案\nssh的機制是這樣的，假設A要連線到B，要連線的話，須將 A的 .pub 金鑰，複製到B的 authorized_keys。\n第一次進入時，會將A金鑰記憶再B的known_host上，讓日後A可以快速連線。\n\n所以，只要連線過一次，就會記憶目前的金鑰到主機上，日後就能夠直接使用IP連線\n程式會像以下，不再需要-i ，預設就會抓id_rsa來使用。\n\n&lt;?php\nexec(&quot;scp -r /var/www/html/WebService/upload/tmp/fe8abe8e88fb3a2d88cc17a45a7575b1.png 10.7.0.17:/var/www/html/tmp/ 2&gt;&amp;1&quot;, $output, $return_var);\nprint_r($output);\necho &quot;return&quot; .$return_var;\necho &quot;&lt;br&gt;&quot;;\nexec(&quot;ssh 10.7.0.17 2&gt;&amp;1&quot;, $output,$return_var);\nprint_r($output);\necho &quot;return:&quot;.$return_var;\n?&gt;",
		"tags": [ "note","🐧"]
},

{
		"title": " 🐳 Container MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/0.Container MOC/",
		"content": "Docker Debug\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/29.Dockerfile echo something/\">29.Dockerfile echo something</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/152. 硬碟空間不足，無法進入容器/\">152. 硬碟空間不足，無法進入容器</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/168. Docker build失敗跳出killed/\">168. Docker build失敗跳出killed</a>\n\nDockerfile\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/32. nuxtJS 的 Dockerfile/\">32. nuxtJS 的 Dockerfile</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/112. pyenchant & opencc error fix/\">112. pyenchant &amp; opencc error fix</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/115. .net core 打包dockerfile/\">115. .net core 打包dockerfile</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/138. gerapy+scrapyd+mongo爬蟲整套流程/\">138. gerapy+scrapyd+mongo爬蟲整套流程</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/170. Docker啓用GPU/\">170. Docker啓用GPU</a>",
		"tags": [ "note","🐳"]
},

{
		"title": "python打包dockerfile除錯日記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/112. pyenchant &amp; opencc error fix/",
		"content": "碎碎念\n新公司的第一個dockerfile就撞牆，\n莫名的錯誤，裝了還是過不了，\n寫這個程式的說也沒用過，他們都是用本機跑AI。\n只好自己亂搞了..\n正文\ndebug方式，都是直接去翻python的程式碼看裏面的內容。\n這兩個大部分都用 ctypes.util.find_library 去找library。\n然後程式內都有寫額外條件，當find_library找不到時，\n可以從環境變數取值。\n所以下面都是使用環境變數的方式，讓package能夠取得library。\npython debug時的備忘事項\n\npython3 package 的位置在 /usr/local/lib/python3.10/site-packages\nfind_library，根據同事的說法，應該是抓path的路徑下去做搜尋\n\n然後開始看code吧。\nalpine linux安裝enchant\n解決 'enchant' C library was not found\nImportError: The 'enchant' C library was not found and maybe needs to be installed.\nSee https://pyenchant.github.io/pyenchant/install.html\nfor details\n\n其實有安裝 py3-enchant 就好了，\n後續是因為找不到library才會噴錯。\napk add py3-enchant aspell aspell-en\n\n所以設定一下環境變數，收工。\n下面程式是dockerfile用的，如果要測試就把env改成export吧\nENV PYENCHANT_LIBRARY_PATH=/usr/lib/libenchant-2.so.2\n\nref.\nCannot find Enchant C library on Apple Silicon\nalpine linux 安裝opencc\n解決 libopencc.so.1 : No such file\nTraceback (most recent call last):\nFile &quot;/usr/local/app/Speech_control.py&quot;, line 3, in &lt;module&gt;\nfrom opencc import OpenCC\nFile &quot;/usr/local/lib/python3.10/site-packages/opencc.py&quot;, line 24, in &lt;module&gt;\nlibopencc = CDLL('libopencc.so.1', use_errno=True)\nFile &quot;/usr/local/lib/python3.10/ctypes/__init__.py&quot;, line 374, in __init__\nself._handle = _dlopen(self._name, mode)\nOSError: Error loading shared library libopencc.so.1: No such file or directory\n\n因為opencc在 repository的testing上面，\n用下面的方法裝\napk add opencc --repository=http://dl-cdn.alpinelinux.org/alpine/edge/testing/\n\n好了後設定環境變數(這是dockerfile用的)\nENV LIBOPENCC=/usr/lib/libopencc.so.1.1\n\n完整dockerfile\n這個alpine linux , build出來 181.73MB\nFROM python:alpine3.16\nENV PATH &quot;$PATH:/usr/local/bin&quot;\nWORKDIR /usr/local/app\nRUN apk update &amp;&amp; apk add py3-enchant aspell aspell-en &amp;&amp; apk add opencc --repository=http://dl-cdn.alpinelinux.org/alpine/edge/testing/\nENV LIBOPENCC=/usr/lib/libopencc.so.1.1\nENV PYENCHANT_LIBRARY_PATH=/usr/lib/libenchant-2.so.2\nCOPY ./Chatbot_Speech /usr/local/app\nRUN pip3 install --upgrade pip\nRUN pip3 install --no-cache-dir -r requirements.txt\nENTRYPOINT [&quot;python&quot;, &quot;Speech.py&quot;]\n\n用ubuntu ,build 出來 511.74MB，\n本來是6xx，後來清了一下cache，縮減一下步驟後，才變成511MB\nFROM ubuntu:20.04\nENV PATH &quot;$PATH:/usr/local/bin&quot;\nWORKDIR /usr/local/app\nCOPY ./Chatbot_Speech /usr/local/app\nRUN apt-get update &amp;&amp; apt-get install -y enchant &amp;&amp; apt-get install -y python3.6 python3-pip &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --upgrade pip\nRUN pip3 install --no-cache-dir -r requirements.txt\nENTRYPOINT [&quot;python3&quot;, &quot;Speech.py&quot;]\n\nref.\n- 最佳化 Dockerfile - 精簡 image",
		"tags": [ "note","🐳"]
},

{
		"title": ".Net Core打包Dockerfile",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/115. .net core 打包dockerfile/",
		"content": "碎碎念\n今天才知道vistual studio 的神奇地方，\n直接滑鼠按一按，dockerfile就自動產生了，\n也不用寫，不過不是vs code的版本。\n正文\nFROM mcr.microsoft.com/dotnet/aspnet:3.1 AS base\nRUN apt-get update -y &amp;&amp; apt-get install -y libc6-dev libgdiplus libx11-dev &amp;&amp; apt-get clean &amp;&amp; ln -s /usr/lib/libgdiplus.so /usr/lib/gdiplus.dll\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:3.1 AS build\nWORKDIR /src\nCOPY ./**/*.csproj ./\n# RUN pwd &amp;&amp; ls -ahl\nRUN for file in $(ls *.csproj); do mkdir -p ./${file%.*}/ &amp;&amp; mv $file ./${file%.*}/; done\n# RUN pwd &amp;&amp; ls SystemManageService.Backend -ahl\nRUN dotnet restore SystemManageService.Backend/SystemManageService.Backend.csproj\n\nCOPY . .\nRUN dotnet build SystemManageService.Backend/SystemManageService.Backend.csproj -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish SystemManageService.Backend/SystemManageService.Backend.csproj -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nRUN ls\n\n這邊的重點在這兩段，取得專案內所有的csproj，然後丟進去build的image裏面。\n但是，這樣並不會根據資料夾放到其他特定的地方，\n所以才有for file建立資料夾跟移動檔案。\nCOPY ./**/*.csproj ./\n# RUN pwd &amp;&amp; ls -ahl\nRUN for file in $(ls *.csproj); do mkdir -p ./${file%.*}/ &amp;&amp; mv $file ./${file%.*}/; done\n\n第一行重點，\nFROM mcr.microsoft.com/dotnet/aspnet:3.1 AS base\n\n此image不能拿掉，一定要有一個base的，\n不然在開發的時候，每次都會重build一次image\nref. .NET Core Docker 再也不用逐個 COPY csproj 檔案啦！",
		"tags": [ "note","🐳"]
},

{
		"title": "gerapy+scrapyd+mongo爬蟲整套流程",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/138. gerapy+scrapyd+mongo爬蟲整套流程/",
		"content": "睡睡念\n有一天突然發現，我的爬蟲不會動了，原因是網站改版，所以規則要改。\n於是，既然都要改了，那就把很久以前沒有概念的docker都改一改吧，\n順便改成docker compose一次解決。\n正文\n更新，後來也沒用這套了，他在排程執行時有點問題，\n最後還是回本來的scrapyweb+scrapyd+mongoDB，\n請見文章最後的Docker-compose.yaml\n架設服務\ndocker-compose.yml\nversion: '3'\nservices:\nscrapyd:\ncontainer_name: scrapyd\nimage: vimagick/scrapyd\nvolumes:\n- ./data:/var/lib/scrapyd\n- /usr/local/lib/python3.7/dist-packages\nrestart: unless-stopped\nnetworks:\n- scrapy_net\ngerapy:\ncontainer_name: gerapy\nimage: germey/gerapy\nvolumes:\n- ./gerapy:/app/gerapy\nports:\n- 8000:8000\nnetworks:\n- scrapy_net\nmongo:\ncontainer_name: mongo\nimage: mongo:latest\nrestart: always\nvolumes:\n- ./mongodbdata:/data/db\nenvironment:\nMONGO_INITDB_ROOT_USERNAME: daimomadmin\nMONGO_INITDB_ROOT_PASSWORD: d@imom\nnetworks:\n- scrapy_net\nports:\n- 27017:27017\nmongo-express:\ncontainer_name: mongo-express\nimage: mongo-express\nrestart: always\nports:\n- 8081:8081\nenvironment:\nME_CONFIG_MONGODB_SERVER: 192.168.1.78\nME_CONFIG_MONGODB_PORT: 27017\nME_CONFIG_MONGODB_ADMINUSERNAME: daimomadmin\nME_CONFIG_MONGODB_ADMINPASSWORD: d%40imom\nnetworks:\n- scrapy_net\nnetworks:\nscrapy_net:\nname: scrapy\ndriver: bridge\n\n執行\ndocker compose up -d\n\n注意事項\nmongo-express 底下的 ME_CONFIG_MONGODB_ADMINPASSWORD\n為什麼會出現 %40？\n因為直接用@ 會出現錯誤。\n\nUnhandledPromiseRejectionWarning: MongoParseError: Unescaped at-sign in authority section\n\n以下是chatGPT的回答\n\n設定gerapy\n如果是根據上面的docker compose建立的話，\n網址用 localhost:8000登入，\n預設的帳密是 admin/admin，進去後最好改一下。\n\n設定 scrapyd的位置，在同一個network裏面，\n所以能這樣互打。\n\n上傳專案\n這邊要注意，上傳的壓縮檔案裏面要有『 .cfg』的檔案\n檔案結構如下\n\n壓縮的時候，請針對上層目錄直接壓縮，以上圖來說就是壓縮TaazeSpider這個資料夾。\n上傳完後，可以按Edit，可以看到程式。\n\n部署到主機上\n\n必須先build，然後才可以deploy到主機上\n\n如果碰到打包失敗或是部署失敗，可以到主機上看一下log。\n\ndocker logs gerapy\ndocker logs scrapyd\n\n測試運作\n到主機的設定畫面，選擇schedule ，然後按下run 跑跑看有沒有正常。\n\n設定排程\nName : 隨意\nProject ：projects的名稱\nSpider：在Schedule的名稱，可參考上圖\nClients：主機名稱，可以用選的\nTrigger：crontab 是固定的時間執行\ninterval 是 隔多久執行\ndate 是 特定日期\n\n好了後可以按下status，可以看到下次執行時間，從這看有沒有設定成功。\n\n這邊可以看到 Next time的時間\n\nref.\n- docker部署打包失敗\n- gerapy\n- CSS Selector Reference\nScrapyweb+scrapyd\n最後還是用回這套了，\n穩定度大勝，db的帳號密碼，必須與爬蟲一致\n\nversion: '3'\nservices:\nscrapyd:\ncontainer_name: scrapyd\nimage: germey/scrapyd\nvolumes:\n- ./data:/var/lib/scrapyd\n- /usr/local/lib/python3.7/dist-packages\nrestart: unless-stopped\nnetworks:\n- scrapy_net\nscrapydweb:\ncontainer_name: scrapydweb\nimage: ryanvin/scrapydweb\ndepends_on:\n- scrapyd\nports:\n- 5000:5000\nenvironment:\nUSERNAME: daiomom\nPASSWORD: password\nSCRAPYD_SERVERS: scrapyd\nvolumes:\n- ./scrapydweb_data:/app\nnetworks:\n- scrapy_net\nmongo:\ncontainer_name: mongo\nimage: mongo:latest\nrestart: always\nvolumes:\n- ./mongodbdata:/data/db\nenvironment:\nMONGO_INITDB_ROOT_USERNAME: daimomadmin\nMONGO_INITDB_ROOT_PASSWORD: password\nnetworks:\n- scrapy_net\nports:\n- 27017:27017\nmongo-express:\ncontainer_name: mongo-express\nimage: mongo-express\nrestart: always\nports:\n- 8081:8081\nenvironment:\nME_CONFIG_MONGODB_SERVER: 192.168.1.78\nME_CONFIG_MONGODB_PORT: 27017\nME_CONFIG_MONGODB_ADMINUSERNAME: daimomadmin\nME_CONFIG_MONGODB_ADMINPASSWORD: password\nnetworks:\n- scrapy_net\nnetworks:\nscrapy_net:\nname: scrapy\ndriver: bridge",
		"tags": [ "note","🐳"]
},

{
		"title": "硬碟空間不足，無法進入容器，清除資料大作戰",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/152. 硬碟空間不足，無法進入容器/",
		"content": "<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/152. 硬碟空間不足，無法進入容器/#2024-07-16-docker-compose\">#2024/07/16，docker compose設定方法</a>\n睡睡念\n上週同事問我怎麼連進去容器裏面，\n我很順的回答指令，\n然後他就跳出 這個錯誤\n\nfailed to create runc console socket: mkdir /tmp/pty2070058578: no space left on device: unknown\n\n正文\n處理問題\n從錯誤訊息看起來，就是空間不足。\n先看看是哪邊硬碟吃滿了\ndf -h\n\n然後看到user 100%...然後掛載在 / 根目錄下。\n再來檢查當前目錄，每個資料夾的大小\ndu -hcd 1 | sort -n\n\n最後追阿追的，追到了container的目錄\n\n/var/lib/docker/\n裏面有一些container容量吃得很多，\n最後確認是該container的log，那就砍了吧。\n\nrm -rf &lt;container_id&gt;.log\n\n再請同事連線，還是出現同樣的錯誤，\n是由於linux的特性，就算你砍了檔案，\n此檔案還是被咬住沒有釋放開(window是連砍都不能砍)，\n所以此時去查硬碟大小，還是100% used的狀態，\n將docker重開後，就能釋放空間了。\n\n解決問題\n現在知道是container的log會一直增長，\n那就設個條件，讓他長到一定程度後就不能再長大了。\n如果此路徑沒檔案，則建立\nvim /etc/docker/daemon.json\n\n輸入\n{\n&quot;log-driver&quot;:&quot;json-file&quot;,\n&quot;log-opts&quot;:{\n&quot;max-size&quot; :&quot;500m&quot;,&quot;max-file&quot;:&quot;3&quot;\n}\n}\n\nmax-size : 單個容器的單檔大小\nmax-file: log數量\n好了後，重啓服務\nsystemctl daemon-reload\nsystemctl restart docker\n\n需要注意的是：設定的日誌大小規則，只對新建的容器有效\nref.\n\n【docker】docker限制記錄檔大小的方法+查看記錄檔的方法\ndocker 報錯： failed to create runc console socket: mkdir /tmp/pty328598598: no space left on device\n\n2024/07/16，docker compose設定方法\n在docker compose 增加下面語法\nservices:\nsrs:\ncontainer_name: srs\nlogging:\ndriver: &quot;json-file&quot;\noptions:\nmax-size: &quot;300m&quot;\nmax-file: &quot;3&quot;",
		"tags": ["2024/07/16，docker", "note","🐳"]
},

{
		"title": "python PaddlePaddle 打包image",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/154. PaddlePaddle 打包image/",
		"content": "碎碎念\n這次的AI程式又更特別了，\n一堆不相容的套件，\nRD是在windows上開發，要包成image一堆牆，嘖嘖。\n正文\n\nlibGL.so.1\n\napt-get install libgl1\n\npycocotools\n\n這個好解決，直接在requirements.txt上面增加 pycocotools\n\nPaddle\n\npip3 install paddlepaddle==2.4.2 -i https://mirror.baidu.com/pypi/simple\n\nPaddle安裝失敗\n這個就真的機車了，後來查是因爲paddle，不支援alpine linux，\n最後只好裝在ubuntu上面。\nref. github issue\n\ntorch\n\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\nFROM ubuntu:20.04\nENV PATH &quot;$PATH:/usr/local/bin&quot;\nWORKDIR /usr/local/app\nCOPY . /usr/local/app\n\nRUN apt-get update &amp;&amp; apt-get install -y enchant &amp;&amp; apt-get install -y python3.7 python3-pip libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --upgrade pip\nRUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\nRUN pip3 install paddlepaddle==2.4.2 -i https://mirror.baidu.com/pypi/simple\nRUN pip3 install --no-cache-dir -r requirements.txt\nENTRYPOINT [&quot;gunicorn&quot;,&quot;-c&quot; ,&quot;gunicorn_config.py&quot;,&quot;wsgi:app&quot;]",
		"tags": [ "note","🐳"]
},

{
		"title": "NGINX-Certbot 自動跟Let'sEncrytp取得憑證",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/157. Docker NGINX-Certbot 自動跟Let&#39;sEncrytp取得憑證/",
		"content": "睡睡念\n前人很強，很厲害，喜歡自己動手，\n所以他當初造的自動取得Let's Encrypt的程式，都是自己寫一整套來用。\n然後，交接了整套，但程式沒人知道在幹嘛，只有每次出問題時，\n會去翻API看是要先call哪個，再call哪個，然後再重開某些image\n才能正確取得，身爲一個懶人，跟腦細胞容量過小的我，實在記不住阿，\n只好改了。\n正文\n這個只適合Docker，k8s請改用cert-manage\n使用說明\n我這邊是掛載user_conf.d 到 /etc/nginx/user_conf.d ，\nuser_conf.d就是設定檔存放的位置。\ndocker-compose.yaml\nversion: '3'\n\nservices:\nnginx:\nimage: jonasal/nginx-certbot:latest\ncontainer_name: nginx\nrestart: unless-stopped\nnetworks:\n- cicd_net\nenv_file:\n- ./nginx-certbot.env\nports:\n- 80:80\n- 443:443\nvolumes:\n- nginx_secrets:/etc/letsencrypt\n- ./user_conf.d:/etc/nginx/user_conf.d\n\nvolumes:\nnginx_secrets:\n\n# 跟drone servere掛在同一層\nnetworks:\ncicd_net:\nname: cicd_net\ndriver: bridge\n\n記得更改email，不然啓動會失敗\nnginx-certbot.env\n# Required\nCERTBOT_EMAIL=&lt;email&gt;\n\n# Optional (Defaults)\nDHPARAM_SIZE=2048\nELLIPTIC_CURVE=secp256r1\nRENEWAL_INTERVAL=8d\nRSA_KEY_SIZE=2048\nSTAGING=0\nUSE_ECDSA=1\n\n# Advanced (Defaults)\nCERTBOT_AUTHENTICATOR=webroot\nCERTBOT_DNS_PROPAGATION_SECONDS=&quot;&quot;\nDEBUG=0\nUSE_LOCAL_CA=0\n\nuser_conf.d資料夾內的檔案名稱\nserver {\n# Listen to port 443 on both IPv4 and IPv6.\nlisten 443 ssl http2;\nlisten [::]:443 ssl http2;\n\n# Domain names this server should respond to.\nserver_name otpat.domain.com;\n\n# Load the certificate files.\nssl_certificate /etc/letsencrypt/live/otpat-domain-com/fullchain.pem;\nssl_certificate_key /etc/letsencrypt/live/otpat-domain-com/privkey.pem;\nssl_trusted_certificate /etc/letsencrypt/live/otpat-domain-com/chain.pem;\n\nssl_protocols TLSv1.3 TLSv1.2;\nssl_prefer_server_ciphers on;\nssl_ecdh_curve X25519:secp521r1:secp384r1;\nssl_ciphers TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;\nssl_session_cache shared:TLS:2m;\nssl_buffer_size 4k;\n\n# OCSP stapling\nssl_stapling on;\nssl_stapling_verify on;\nresolver 1.1.1.1 1.0.0.1 [2606:4700:4700::1111] [2606:4700:4700::1001]; # Cloudflare\n\n# return 200 'Let\\'s Encrypt certificate successfully installed!';\n# add_header Content-Type text/plain;\n\nlocation / {\nproxy_set_header X-Forwarded-For $remote_addr;\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header Host $http_host;\n\nproxy_pass http://totp-auth:9987;\n\nproxy_redirect off;\nproxy_http_version 1.1;\nproxy_buffering off;\n\nchunked_transfer_encoding off;\n}\n\n}\n\n完成品，因為要將proxy_pass導到服務上面，最好是設成同一個network，這邊設為 cicd_net\n\n如果啓動失敗的話，\n可以查一下nginx裏面的錯誤訊息，上面通常寫的很清楚。\n建議可以先看看這兩篇\nref.\n- D15 - NGINX-Certbot Image\n- github_good_to_know",
		"tags": [ "note","🐳"]
},

{
		"title": "Docker build失敗跳出killed",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/168. Docker build失敗跳出killed/",
		"content": "睡睡唸\n內部要搞一個AI的開發平臺，\n所以我負責python的負載平衡架構，\n因爲他的程式開發好像有點困難，就用nginx的LB來做了。\n正文\n驗證<a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/169. Nginx的負載平衡/\">169. Nginx的負載平衡</a>時，需要讓範例程式顯示一下請求的資訊，\n確認真的有做到這塊。\n打包以前的舊AI程式時就暴掉了，\n出現 killed的error。\n&gt; [6/8] RUN pip3 install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu &quot;https://download.pytorch.org/whl/cpu&quot;):\n\n1.293 Looking in indexes: [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu &quot;https://download.pytorch.org/whl/cpu&quot;) 2.901 Collecting torch 2.917 Downloading [https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl](https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl &quot;https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl&quot;) (195.4 MB) 11.30 Killed ------ Dockerfile:25 -------------------- 23 | RUN apt-get update &amp;&amp; apt-get install -y enchant &amp;&amp; apt-get install -y python3.7 python3-pip libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* 24 | RUN pip3 install --upgrade pip 25 | &gt;&gt;&gt; RUN pip3 install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu &quot;https://download.pytorch.org/whl/cpu&quot;) 26 | RUN pip3 install paddlepaddle==2.4.2 -i [https://mirror.baidu.com/pypi/simple](https://mirror.baidu.com/pypi/simple &quot;https://mirror.baidu.com/pypi/simple&quot;) 27 | RUN pip3 install --no-cache-dir -r requirements.txt -------------------- ERROR: failed to solve: process &quot;/bin/sh -c pip3 install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu &quot;https://download.pytorch.org/whl/cpu&quot;)&quot; did not complete successfully: exit code: 137\n\n查了一下，說是因爲記憶體不足導致docker build的時候暴掉。\nref. PyTorch Docker build error\n`Killed` means that the compiler was killed by the kernel because it used too much memory. There are a few workarounds:\n\n- increase the memory limit for docker container\n- limit the number of concurrent builds by changing the `Dockerfile` to include `MAX_JOBS=1` [here](https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile#L30)\n\n把我的docker resource 的記憶體資源加大後就好了（我mac記憶體只有16G)。\n[[168-fig.1.jpg]]",
		"tags": [ "note","🐳"]
},

{
		"title": "Docker啓用GPU",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/170. Docker啓用GPU/",
		"content": "睡睡唸\n上面<a class=\"internal-link\" data-note-icon=\"\" href=\"/🐳 Container/168. Docker build失敗跳出killed/\">168. Docker build失敗跳出killed</a><a class=\"internal-link\" data-note-icon=\"\" href=\"/🌐 Network/169. Nginx的負載平衡/\">169. Nginx的負載平衡</a>，\n這兩篇都是為了要達到內部AI平臺再用，\n不要讓每個人的電腦都自己裝一堆程式。\n正文\n電腦裝完顯卡驅動後，要讓docker識別，\n還需要額外裝 NVIDIA Container Toolkit\n照著官方文件來，挺快的。\n這邊記錄ubuntu的方法，其他的安裝方式，請參考上面文件\n\n更新\n\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n&amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\nsed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\nsudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n&amp;&amp; \\\nsudo apt-get update\n\n安裝\n\nsudo apt-get install -y nvidia-container-toolkit\n\ndocker設定 （containerd、CRI-O的話，參考上面連結）\n\nsudo nvidia-ctk runtime configure --runtime=docker\n\n重啓\n\nsudo systemctl restart containerd\n\n驗證\n\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n\n顯示下面的資料就表示有了。\n\nDocker-compose\n如果使用docker compose的話，\n須在yaml上面增加一些設定，command 到 capabilities\n當然也可以指定此container要吃第幾個gpu，\n更詳細說明請參考Turn on GPU access with Docker Compose\n提示：主要在count的設定\nversion: '3.8'\nservices:\npython-1:\ncontainer_name: yolox-brand-1\nimage: busybox\ncommand: nvidia-smi\ndeploy:\nresources:\nreservations:\ndevices:\n- driver: nvidia\ncount: all\ncapabilities: [gpu]\npull_policy: if_not_present\nrestart: always\nnetworks:\n- internal\nnetworks:\ninternal:\nname: internal\ndriver: bridge",
		"tags": [ "note","🐳"]
},

{
		"title": "Dockerfile除錯模式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/29.Dockerfile echo something/",
		"content": "前言\n在幫人寫自動佈署的時候，發現以往的Docker會顯示畫面上的指令，\n這次都沒有出來。\n正文\n解決方案，加上 --progress=plain\ndocker build --progress=plain -t ey:0.1 .\n\n如果沒有加的話，會像 (fig.1)，有加的話，則會像(fig.2)，將螢幕上的資料顯示出來\nFROM gcr.io/cloud-builders/yarn:node-8.12.0 as build-env\nWORKDIR /app\n\nCOPY . .\nRUN pwd\nRUN ls\n\n(fig.1)\n(fig.2)\nref.Docker build not showing any output from commands",
		"tags": [ "note","🐳"]
},

{
		"title": "nuxtJS的Dockerfile",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🐳 Container/32. nuxtJS 的 Dockerfile/",
		"content": "前言\n又多了一個新的套件要做，這次用的是nuxt.js ，只知道是vue.js延伸出來的東西...\n然後，上吧。\n正文\n因 nuxt.js ，跟go有點像都會自己啟動一個伺服器的服務，\n走的是SSR（Server-Side Rendering），\n不是卡牌遊戲的SSR阿。\nSSR的意思是，頁面由後端渲染，HTML由後端產生（據說是要讓速度比較快）\nref.02. 講歷史，話說 SSR...\n因為nuxt.js會自己建立一個js，\n所以有踩到一個坑，\n在設定 nuxt.config.js的時候，\n請確認好port 以及 是不是開了https 。\n建議先查一下 nuxt.config.js 的內容屬性，再來打包及佈署。\nref. 16. Nuxt 全域設定檔 (nuxt.config.js)，可以吃嗎？\n這是第一個版本，使用npm編譯。\nFROM node:11.13.0\nCOPY . /usr/src/nuxt-app/\n\nRUN npm install\nRUN npm run build:qa\n\nEXPOSE 80\n\nENV NUXT_HOST=0.0.0.0\nENV NUXT_PORT=80\n\nCMD [ &quot;npm&quot;, &quot;start&quot; ]\n\n第二個版本，改使用yarn安裝，這邊要注意，node版本，請抓最新的，不然編譯的時候會出現python錯誤。\nFROM node:16.3.0-alpine\nWORKDIR /usr/src/app\n\nCOPY . .\nRUN yarn install &amp;&amp; yarn cache clean\nRUN yarn build\nCMD [ &quot;yarn&quot;, &quot;start&quot; ]\n\n這邊只寫 yarn build ，是因為在設定nuxt的時候，有寫 build:qa 跟 build。\n因為是最後測試完的成果了，就改用yarn build ，做結束。\nref.\n使用 yarn install 指令安裝套件時鎖定 yarn.lock 版本\ndocker-hub node",
		"tags": [ "note","🐳"]
},

{
		"title": " 👁 Observability MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/0.Observability MOC/",
		"content": "概念\n\n<a class=\"internal-link is-unresolved\" href=\"/404\">blog.45.網站效能指標一覽（未）</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">96. SRE監控的四大指標及SLI、SLO、SLA</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/195. SRE可觀測性三大支柱/\">195. SRE可觀測性三大支柱</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/198. SRE好文連結/\">198. SRE好文連結</a>\n\nGrafana\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/24.grafana設計篇-第二個Y軸/\">24.grafana設計篇-第二個Y軸</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/25.在GKE上的grafana上安裝grafana-image-renderer/\">25.在GKE上的grafana上安裝grafana-image-renderer</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/26.grafana alert message parameters傳入/\">26.grafana alert message parameters傳入</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/28.Prometheus的專用語言promQL/\">28.Prometheus的專用語言promQL</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/122. Grafana 8.X以後的alert 參數/\">122. Grafana 8.X以後的alert 參數</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/124. grafana alert每四小時重發一次/\">124. grafana alert每四小時重發一次</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">184. grafana error reverse proxy settings on istio</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/197. Grafana 以及Prometheus 除錯方式/\">197. Grafana 以及Prometheus 除錯方式</a>\n\nPrometheus\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/131. rabbitMQ on Prometheus/\">131. rabbitMQ on Prometheus</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/👁 Observability/178. Install Prometheus Operator/\">178. Install Prometheus Operator</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">179. install Bindplane</a>",
		"tags": [ "note","👁"]
},

{
		"title": "Grafana 8.X以後的alert 參數",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/122. Grafana 8.X以後的alert 參數/",
		"content": "睡睡念\n正文\n新版的Grafana換了alert ,\n現在把alert通通整合在一起了，\n然後我以前用的傳參數方法不能用了，\n最近都在測試這個東西\nPrint Labels\n如果在summary上面寫\n\n會印出此alert上面的預設labels標籤，\n下圖，是我自訂的標籤\n\n但實際印出來的都沒有出現\n\n陣列印出來後，就能夠指定名稱顯示了。\n\nPrint Value\n這次改使用這個參數\n\n會顯示 三個值。\n\n設定所用的條件有兩個，\n\n結論\n基本使用變數的方法，\n如果要進階點，可能就需要改用alert template了。\nalert template參考下面網址\n\nHow To Use Alert Message Templates in Grafana\nCreate and edit message templates\n\n參考來源\n\nTemplating labels and annotations\nLabels in Grafana Alerting",
		"tags": [ "note","👁"]
},

{
		"title": "grafana alert每四小時重發一次",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/124. grafana alert每四小時重發一次/",
		"content": "睡睡念\n新的Grafana alert manage ，可調整的地方還真多阿..\n正文\n告警上線後，只要告警狀態沒解除的話，他每4小時會重發一次。\n\n到 Alerting 的 Notification policies ，設定Root policy\n\n展開 Timing options，設定 Repeat interval\n\nref\n- Unified Alerting: alert triggers every 4 hours",
		"tags": [ "note","👁"]
},

{
		"title": "rabbitMQ on Prometheus",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/131. rabbitMQ on Prometheus/",
		"content": "睡睡念\n最近demo環境發生問題，初步懷疑是MQ因為訊息量太大掛了。\n但沒有證據，因為MQ的管理介面只能看到一小時的資料。\n正文\n本來用operator安裝完後，看了一些介紹都說要額外啓動外掛，\n但operator要怎麼啓動外掛，我就兩眼一抹黑了。\n看到 RabbitMQ學習筆記：內建Prometheus支援rabbit_prometheus外掛\n原來，我的MQ的15692 port 已經開了，prometheus已經設定好。\n\n用curl去抓資料也有metrics，看來沒問題了。\n想說可以結案了，但是（就怕這個但是），裏面沒有queue的資料，\n所以看不到每個queue的message rate\n\n撈了相關的metric裏面也沒有Queue的名稱，可供filter\nmetrics說明\n所以，rabitMQ on k8s建好後，只要在prometheus寫個job就能去撈資料了。\n- job_name: 'rabbitmq'\nscrape_interval: 30s\nstatic_configs:\n- targets:\n- 'prod-rabbitmq.rabbitmq-system.svc.cluster.local:15692'\n\nprod-rabbitmq 指的是service name.\n然後，grafana dashboard可以用 # RabbitMQ-Overview",
		"tags": [ "note","⎈"]
},

{
		"title": "Install Prometheus Operator",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/178. Install Prometheus Operator/",
		"content": "前言\n最近看完2023 Google Cloud summit ，\n看到一個新東西，OpenTelemetry，可以不用埋程式用sidecar的方式，\n就能做到Observability的trace，\n但在那之前，我要先在測試叢集裝一下Prometheus Operator，\n不然資料傳不過去。\n正文\n\ngit clone\n\ngit clone https://github.com/prometheus-operator/kube-prometheus\n\ninstall\n\nkubectl apply --server-side -f manifests/setup\nkubectl wait \\\n\t--for condition=Established \\\n\t--all CustomResourceDefinition \\\n\t--namespace=monitoring\nkubectl apply -f manifests/\n\n刪除\n\nkubectl delete --ignore-not-found=true -f manifests/ -f manifests/setup\n\n不過這個安裝方式，會包含\n\nThe Prometheus Operator\nHighly available Prometheus\nHighly available Alertmanager\nPrometheus node-exporter\nPrometheus Adapter for Kubernetes Metrics APIs\nkube-state-metrics\nGrafana\n\n自定安裝的話，也可以利用helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n自訂安裝,設定參考\nhelm install test prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace --values values.yaml\n\n上面指令使用test當關鍵字，所以部署時也會在上面增加\n補充\n如果不想自己架Prometheus，也不想用Google Cloud Monitor，\n可以考慮Google自己出的 Google Managed Prometheus 官方文件\nref.\n\nGithub- kube-prometheus\nHelm安裝 Prometheus Operator — 為 Kubernetes 設定及管理 Prometheus\nGoogle Managed Prometheus (GMP)",
		"tags": [ "note","👁"]
},

{
		"title": "install Bindplane",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/179.  install Bindplane/",
		"content": "前言\n最近看完2023 Google Cloud summit ，\n看到一個新東西，OpenTelemetry，可以不用埋程式用sidecar的方式，\n就能做到Observability的trace，\n一開始安裝OpenTelemetry，後來發現，\n他好肥（測試cluster空間不足），之後還搞錯了一件事情。\n後來改測試Bindplane。\n正文\nWhat is Bindplane?\n\nBindplane is an open source solution utilizing the OpAMP protocol to configure, monitor, and deploy various collectors.\nBindplane 是一個開源解決方案，利用 OpAMP 協定來設定、監控和部署各種收集器。\n\n他是一個監控跟log的收集器，他並不是像efk或google cloud那樣的東西。\nref. What is OpAMP? And what is Bindplane?\n安裝\n\nHelm add source\n\nhelm repo add &quot;bindplane&quot; \\ &quot;https://observiq.github.io/bindplane-op-helm&quot;\nhelm repo update\n\n建立組態 values.yaml\n\nconfig:\nusername: &quot;admin&quot;\npassword: &quot;admin&quot;\nsecret_key: &quot;13c15d7c-233e-49e3-bef4-f7498f33518f&quot;\nsessions_secret: &quot;a4e5aa85-5df4-4edd-8cfe-d8a7257d09dd&quot;\n\n完整設定參數\n\n安裝\n\nhelm upgrade --install &quot;bindplane&quot; \\\n--values &quot;values.yaml&quot; \\\n--namespace bindplane \\\n--create-namespace \\\nbindplane/bindplane\n\nref. Deploy BindPlane OP Server on K8s\n設定\n\n設定port forward轉過去服務上\n\nkubectl port-forward -n bindplane svc/bindplane 3001",
		"tags": [ "note","👁"]
},

{
		"title": "grafana error reverse proxy settings on istio",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/184. grafana error  reverse proxy settings on istio/",
		"content": "前言\n搬project，就想乾脆上istio了，\n然後就卡到怎麼以前的grafana開的起來，現在的開不起來。\n正文\n照以前的方式設定完 root_url，\n還是出現相同錯誤\n\n查文件後才發現，\n應該是新版(9.5.5)需要加上serve_from_sub_path的參數，\n如果碰到只加了root_url也是看到錯誤，就再加個參數吧。\ngrafana.ini\n[server]\nroot_url = &quot;http://&lt;ip&gt;/grafana/&quot;\nserve_from_sub_path = true\n\nvirtualservice.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: istio-virtualservice\nnamespace: istio-system\nspec:\nhosts:\n- &quot;&lt;ip&gt;&quot;\ngateways:\n- istio-system/istio-gateway\nhttp:\n- name: &quot;grafana&quot;\nmatch:\n- uri:\nprefix: /grafana/\n- uri:\nexact: /grafana\nroute:\n- destination:\nport:\nnumber: 3000\nhost: grafana.istio-system.svc.cluster.local\n\nref. Run Grafana behind a reverse proxy",
		"tags": [ "note","👁"]
},

{
		"title": "SRE可觀測性三大支柱",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/195. SRE可觀測性三大支柱/",
		"content": "前言\n去年底看google的研討會就有講到OpenTelemetry，\n今天又剛好碰到有人在講這個東西，\n挑個時間把它弄出來了。\n正文\nLogs, Tracing, Metrics\nOpenTelemetry 推動可觀測性三大支柱W3C數據採集的標準化\n\n範例\n\nref. 微服務在雲原生環境的可觀測性應用",
		"tags": [ "note","👁"]
},

{
		"title": "Grafana以及Prometheus 除錯方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/197. Grafana 以及Prometheus 除錯方式/",
		"content": "前言\n最近在查個問題，然後就碰到grafana的圖表產不出來，\n要撈出每個pod目前的記憶體使用量百分比是多少。\n每個pod的記憶體使用量是多個，但總記憶體只有一個，\n就開始找問題了。\n正文\n這個圖來自 Prometheus &amp; Grafana No Data 排查手冊\n\n我目前的排查方式，\n通常都是先開prometheus，看指令有沒有資料，沒資料就表示寫錯。\n有資料，在grafana 無法顯示，那就是grafana的問題。\n目前碰到的問題是單個query都有資料，但湊在一起就沒了。\n後來有問了一下GPT，\n發現相除的話，要符合一些條件。\n\n在 Grafana 中進行查詢的相除，需要確保以下條件：\n\n兩個查詢的時間戳匹配： 確保兩個查詢的時間戳是對應的。如果時間戳不匹配，則查詢結果無法正確進行相除。\n\n標籤匹配： 確保兩個查詢的標籤匹配。這包括時間戳的標籤，以及在使用 by() 子句進行分組時的標籤。如果標籤不匹配，查詢結果可能無法正確合併。\n\n使用 ignoring() 或 on() 子句： 如果有一些標籤不需要匹配，可以使用 ignoring() 或 on() 子句。例如：\nA / ignoring(label_name) B\n\n使用 group_left 或 group_right 子句： 如果需要在查詢中保留某些標籤，可以使用 group_left 或 group_right 以確保這些標籤在合併時保持一致。例如：\nA / B on(label_name) group_left(label_name)\n\n確保查詢結果是數值： 確保兩個查詢返回的結果是數值，並且支持相除操作。\n\n以下是一個簡單的例子，假設你有兩個查詢 A 和 B，要計算 A 中每個時間戳的值除以 B 中對應時間戳的值：\nA / B\n\n根據你的數據源和具體的需求，可能需要根據上述條件進行調整。這些條件有助於確保查詢結果的正確性。",
		"tags": [ "note","👁"]
},

{
		"title": "SRE好文連結",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/198. SRE好文連結/",
		"content": "前言\n看別人SRE的文章，感覺這個需要記錄一下。\n正文\n\n卡瓦邦噶-最近的工作感悟\n\n這篇主要介紹，他是怎麼解決一直報錯的方法。\n當告警太多，只會麻痹而已。\n我目前把通知分成兩種，第三種，工單我還沒有碰到這個案例。\n通知：一般訊息，不重要，但必須要知道（朕，知道了）這種感覺。\n緊急：現在、立刻、馬上處理（護駕！！）這種感覺\n工單：需要人力介入的操作（還沒用過這種）\n\nSRE 的工作介紹\n\n跟上一篇同一個作者，講SRE的工作內容，不同公司會有不同的權重。\n但臺灣應該都是跟DevOps混在一起。可能要更大的公司才有差？\n但我能力不足沒去過大公司😢\n\nCNCF簡易分類",
		"tags": [ "note","👁"]
},

{
		"title": "grafana設計篇-第二個Y軸",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/24.grafana設計篇-第二個Y軸/",
		"content": "前言\n應該是個很常見的功能，但是我卻找了很久，一直找不到。\n正文\n要做出來的東西像（Fig.1) 一樣，左右都各有一個Y軸數值。\n(Fig.1)\n\n首先要先在右邊的圖表將show打開，，這裡分成左右兩邊的Y軸\n\n(Fig.2)\n\n點一下label的標籤，選擇 Y-Axis ，將 use y-axis 打開\n\n(Fig.3)\n\nref.\nLearn Grafana: How to use dual axis graphs",
		"tags": [ "note","👁"]
},

{
		"title": "在GKE上的grafana上安裝grafana-image-renderer",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/25.在GKE上的grafana上安裝grafana-image-renderer/",
		"content": "前言\n在寫grafana的時候，才發現告警的圖片無法寄出，\n必須要額外安裝plugin : grafana-image-renderer ，\n然後，我又卡住了...\n正文\n如果你看到這張圖(fig.1)，就表示你該裝Grafana Image Renderer了。\n\n(fig.1)\n先說結論，我測過『在pod裡面利用grafana-cli 安裝』、『改成使用ubuntu的image預先安裝plugin』，都失敗，最後只能建立 grafana-image-renderer的 服務，透過這個服務，負責宣染當下的告警圖，再隨着telegram送出。\n佈署的yaml， grafana-image-render.yaml\napiVersion: apps/v1\nkind: Deployment\n\nmetadata:\nnamespace: istio-system\nname: grafana-image-renderer\nlabels:\napp: grafana-image-renderer\nversion: v1\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: grafana-image-renderer\nversion: v1\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\nmaxUnavailable: 1\ntemplate:\nmetadata:\nlabels:\napp: grafana-image-renderer\nversion: v1\nspec:\ncontainers:\n- image: grafana/grafana-image-renderer:latest\nimagePullPolicy: &quot;Always&quot;\nname: renderer\nports:\n- containerPort: 80\nenv:\n- name: &quot;RENDERING_VERBOSE_LOGGING&quot;\nvalue: &quot;true&quot;\nresources:\nrequests:\nmemory: &quot;30Mi&quot;\ncpu: &quot;30m&quot;\nrestartPolicy: Always\nserviceAccountName: &quot;&quot;\n---\napiVersion: v1\nkind: Service\nmetadata:\nnamespace: istio-system\nlabels:\napp: grafana-image-renderer\nname: grafana-image-renderer\nspec:\nports:\n- name: &quot;http&quot;\nport: 8081\nselector:\napp: grafana-image-renderer\n\n然後在佈署 grafana的yaml上面，增加環境參數\n- name: &quot;GF_RENDERING_SERVER_URL&quot;\nvalue: &quot;http://grafana-image-renderer.istio-system:8081/render&quot;\n- name: &quot;GF_RENDERING_CALLBACK_URL&quot;\nvalue: &quot;http://grafana.istio-system:3000/grafana/&quot;\n\n如果要開啟debug的話，加上\n- name: &quot;GF_LOG_FILTERS&quot;\nvalue: &quot;rendering:debug&quot;\n\n如果需要更多的log訊息，可參考 grafana-image-renderer，這邊就是改加至grafana-image-renderer上面了。\n一開始在測試的時候有碰到一個問題，\n那就是抓圖的時候，一直抓到grafana的首頁(Fig.2)\n[[25-fig.2.jpg]]\n(Fig.2)\n後來才發現是 GF_RENDERING_CALLBACK_URL 錯誤，由於我的grafana.ini 的 root_url 是 http://localhost/grafana，\n所以 我的網址後面也要加上grafana，但是，如果在grafana.ini上面 的 root_url 寫 http://localhost/grafana\n那telegram的告警，顯示的url也會變成localhost (fig.3)\n[[25-fig.3.jpg]]\n(Fig.3)\n所以，grafana.ini的root_url 最好還是寫實際的ip位置。",
		"tags": [ "note","👁"]
},

{
		"title": "grafana alert message parameters傳入",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/26.grafana alert message parameters傳入/",
		"content": "前言\n在設計grafana alert message的時候，\n一直想傳參數進去，看官方文章顯示是可行的，\n但是不知道是我理解能力太爛還是文章寫的太簡單...弄了半天才生出來。\n正文\n先決條件， Grafana v7.4 以上，才能夠使用 Notification templating 。\nref. Notification templating\n這邊的parameters只限定於 圖表上的 Legend 的value\n\n(fig.1)，\n需使用 ，將值顯示在上面。\n所以這上面沒有資料，就無法傳過去給alert message。\n\n(fig.1)\n之後在alert上面則可以使用 ${label}，將值傳入。\n\ntelegram 呈現\n\n缺點是告警訊息的Metrics上面的數值，我目前還找不到方式可以自定。\n所以長的不怎麼討喜。\n另外可以在圖表上直接將Legend隱藏，這樣至少畫面會好看許多。\nref.\nAlert notification templating",
		"tags": [ "note","👁"]
},

{
		"title": "Prometheus的專用語言promQL",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/👁 Observability/28.Prometheus的專用語言promQL/",
		"content": "前言\n在寫監控告警的時候，看別人寫好的dashboard裡面有一堆函數，\n但卻偏偏不知道該怎麼使用，後來才知道這是 Prometheus的語言 PromQL，\n這邊會將寫監控時用到的東西記下，基本上都是針對metrics做的處理。\n正文\n聚合函數\n\nsum (求和)\nmin (最小值)\nmax (最大值)\navg (平均值)\nstddev (標准差)\nstdvar (標准差異)\ncount (計數)\ncount_values (對value進行計數)\nbottomk (後n條時序)\ntopk (前n條時序)\ntopk(10,prometheus_http_requests_total) ：前10筆資料\nquantile (分佈統計)\n\n聚合函數可與by 或 without共同使用 ，可取得metrics裡的欄位名稱\n例：\n\nsum(rate(istio_requests_total{reporter=&quot;destination&quot;}[1m])) by (destination_workload)\n\n內建函數\n\nabs(絕對值)\nabsent(判斷指標名稱或標籤是否有值)\nabsent_over_time(同上，多了時間範圍)\nceil(四捨五入)\nchanges(返回區間變數每個樣本值變化的次數)\ndelta(計算區間向量第一個值和最後一個值的差值)\nfloor(取整數)\nincrease(返回區間向量第一個和最後一個樣本的增加值)\nrate(計算區間向量 v 在時間視窗內平均每時間增長速率)\nrate(prometheus_http_requests_total[5m]) ： HTTP請求量的增長率\nirate(計算區間向量的增長率，但是它反應的是瞬時增長率)\nderiv(計算樣本的線性回歸模型)\npredict_linear(預測時間序列在n秒後的值)\nsort(對向量按元素的值進行升序排序)\nsort_desc(對向量按元素的值進行降序排序)\n\n實際演練\n可以先在prometheus上面測試語法，當測試完成後，再放到grafana上面看實際的圖表。\n也可透過現有的grafana的dashboard裡面的Query Inspect 將Metric撈出來（這邊可以直接將變數取出）(fig.1)\n撈出來的資料要再經過 urldecode 將符號轉成正常的符號\n\n(fig.1)\n所有的Metric格式如下，\n指標名稱只能由ASCII字符、數字、下劃線以及冒號組成並必須符合正則表達式[a-zA-Z_:][a-zA-Z0-9_:]*\n&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;, ...}\n\n匹配函數\n\n= 等於\n!= 不等於\n=~ 開頭等於\n!~ 開頭不等於\n\n有時也會在label name上面使用 正規表示法，這邊使用的是RE2語法\n例如，我們要取得memory的資料，其中的image的開頭只要包含 gcr.io 跟 istio。\ncontainer_memory_working_set_bytes{id!=&quot;/&quot;,pod=~&quot;.*&quot;,image=~&quot;[gcr.io | istio].*&quot;}\n\n變數設定\n在grafana上面通常會設定一些參數，這是為了看指定的指標\n有四種可選，\n\nlabel_values(label) :\n返回Promthues所有監控指標中，標籤稱為label的所有可選值\nlabel_values(metric, label) :\n返回Promthues所有監控指標metric中，標籤稱為label的所有可選值\nmetrics(metric) :\n返回所有指標名稱滿足metric定義正則表達式的指標名稱\nquery_result(query) :\n返回prometheus查詢語句的查詢結果\n\n常用的label_value\n$Node label_values(kubernetes_io_hostname)\n$Pod label_values(kube_pod_info, pod)\n$Pod_ip label_values(kube_pod_info, pod_ip)\n$phase label_values(kube_pod_status_phase, phase)\n$container label_values(kube_pod_container_info, container)\n\nquery_result範例，\nquery_result(kube_deployment_status_replicas{namespace !~ &quot;istio-system|kube-system&quot;}&gt;0)\n\n解釋：排除Deployment裡面的namespace 不是 istio-system或 kube-system，並且value&gt;0的metrics。\n這邊會顯示全部的資料，故在regex要再指定抓取 deployment的資料\n/.*deployment=&quot;(.*?)&quot;.*/\n\nref.\nGrafana templating with Prometheus labels\n模板化Dashboard\nPromQL的簡單使用\nref.\nPrometheus-基礎系列-(四)-PromQL語句實踐-2\nPromQL 內置函數\nPromQL操作符",
		"tags": [ "note","👁"]
},

{
		"title": " 💻 Code MOC ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/0.Code MOC/",
		"content": "API串接\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/22.telegram api使用/\">22.telegram api使用</a>\n\nGolang\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/Golang/40. golang 重新讀取設定檔的方法/\">40. golang 重新讀取設定檔的方法</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/Golang/47. GO get 參數/\">47. GO get 參數</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/Golang/155. scrapy爬暗黑編年史的世界王時間/\">155. scrapy爬暗黑編年史的世界王時間</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">156. Golang簡略筆記（一）</a>\n\n正則表達式\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/38.正則取代，留下特定字串/\">38.正則取代，留下特定字串</a>\n\nGit\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/59. git refs，branch 跟 tag是一樣的。/\">59. git refs，branch 跟 tag是一樣的。</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/110.github action pipeline筆記/\">110.github action pipeline筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/145. git submodule 下載失敗/\">145. git submodule 下載失敗</a>\n\nIDE\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/💻 Code/107. vscode 忽略設定同步/\">107. vscode 忽略設定同步</a>",
		"tags": [ "note","💻"]
},

{
		"title": "vscode 忽略設定同步",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/107. vscode 忽略設定同步/",
		"content": "碎碎念\n公司一臺電腦，我自己帶了一臺mac過去，\n因為我已經linux用習慣了（X）\n不得不說mac用習慣真的好順...\n正文\n\n應該都知道linux的檔案系統跟windows的檔案系統不一樣。\n前者沒有C、D、E的概念，\n所以vscode的 background\n在圖片路徑的設定上會不一樣，mac是使用下麵這種方式設定vscode的背景\nfile:///Users/daimom/Pictures/1275337.jpeg\n\nwindows則是\nfile://c:\\Users\\user\\Pictures\\1275337.jpeg\n\n一旦同步的話，兩邊的路徑就會衝突，導致一邊沒圖片。\n所以現在要設定，忽略vscode的同步設定，讓他不要把圖片設定的地方也同步。\n在設定的查詢欄位，輸入 『同步』，\n選擇忽略的設定。\n\n應該會看到 settingsSync.ignoredSettings ，\n底下輸入background.customImages\n&quot;settingsSync.ignoredSettings&quot;: [\n\n&quot;background.customImages&quot;\n\n],",
		"tags": [ "note","💻"]
},

{
		"title": "github action pipeline筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/110.github action pipeline筆記/",
		"content": "前言\n因為之前用的巴哈自動簽到，那個github被封了，\n說違反服務條款，還好我主程式有下載下來過。\n未來把他打包成image後在本機跑了吧，\n不過要先解析github的workflow看是怎麼打包成image的\n正文\nname: Publish\n\non:\npush:\nbranches:\n- main\npaths-ignore:\n- &quot;**.md&quot;\nworkflow_dispatch:\n\njobs:\n\npublish_docker:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout Source Code\nuses: actions/checkout@v3\n\n- name: Install PNPM\nuses: pnpm/action-setup@v2.2.2\nwith:\nversion: latest\nrun_install: true\n\n- name: Build Package\nrun: pnpm build:package\n\n- name: Set up QEMU\nuses: docker/setup-qemu-action@v2\n\n- name: Set up Docker Buildx\nuses: docker/setup-buildx-action@v2\n\n- name: Login to Docker Hub\nuses: docker/login-action@v2\nwith:\nusername: $\npassword: $\n\n- name: Build Image and Push\nrun: |\npnpm build:package\npnpm build:docker\n\nname : action的名稱\non ：什麼時候觸發\njob: 要執行的動作\npublish_docker : 作業的名稱\nruns-on: 執行的os\nsteps：執行的步驟\n\n結論\n用了一陣子以後，\n那個專案被github封掉了，\n最後改用chrome 的外掛 ，\n缺點就是電腦要一直開著，反正我電腦也一直開着所以沒關係",
		"tags": [ "note","💻"]
},

{
		"title": "git submodule 下載失敗",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/145. git submodule 下載失敗/",
		"content": "睡睡念\n因為他們的新專案有用到 git的submodule，\n之前也用過一次了，想說應該還好，\n但這次就是怎麼驗證，怎麼不過。\n正文\n都是出現這個錯誤\n\nCloning into '/drone/src/BackendSharedLib'...\nHost key verification failed.\nfatal: Could not read from remote repository.\n\n但之前寫的專案就沒這個問題，找阿找，最後終於發現，\n差異在 建立 submodule的時候。\n打開 .gitmodules 上面會寫 url的位置\n[submodule &quot;SMSBackendSharedLib&quot;]\n\tpath = SMSBackendSharedLib\n\turl = git@github.com:ABC/sms-backend-sharedlib.git\n\n本來是上面這種寫法，他會用ssh的方式去抓submodule的檔案。\n所以要把url改成\n[submodule &quot;SMSBackendSharedLib&quot;]\n\tpath = SMSBackendSharedLib\n\turl = https://github.com/ABC/sms-backend-sharedlib.git\n\n再來用\ngit submodule update --init --recursive\n\n本機也可以下載了。\n本來還懷疑是不是drone clone submodule的時候，用到錯誤的使用者。\n結果一查根本沒有使用者。\n可以用下面的指令查查\ngit config --list\n\ngit config user.name\ngit config user.email\n\n簡單解釋一下 submodule，\n以下從chatGPT抄來的。\n\n在 Git 中，Submodule 是一個允許您在 Git 倉庫中包含另一個 Git 倉庫的機制。通過使用 Submodule，您可以將一個 Git 倉庫作為另一個 Git 倉庫的子目錄進行管理。這對於管理大型專案或共享程式庫非常有用。\n當您在 Git 倉庫中新增 Submodule 時，它實際上是包含另一個 Git 倉庫的指向。當您 clone 或 pull 主倉庫時，Git 只會下載 Submodule 的指向，而不會將 Submodule 的內容一併下載。這樣可以節省空間和時間，同時還可以使主倉庫更加輕便。\n如果您想要更新 Submodule 的內容，可以使用 git submodule update 命令來下載 Submodule 的內容。當您修改 Submodule 的內容時，您需要先進入 Submodule 的目錄，對其進行 commit 和 push，然後返回主倉庫，對 Submodule 的指向進行 commit 和 push。這樣可以確保 Submodule 的指向指向正確的 commit。\n總之，Submodule 是一個非常有用的機制，可以使 Git 倉庫管理更加靈活和有效。",
		"tags": [ "note","💻"]
},

{
		"title": "telegram api使用方法",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/22.telegram api使用/",
		"content": "前言\n上一篇寫到用flagger，建立webhook發送通知。\n公司有在用的就skype跟 telegram，所以就....開始吧\n正文\n\n首先你要有telegram帳號 XDDD\n\n加入 @BotFather ，輸入指令 /newBot，開始命名，注意機器人名稱可以不用bot結尾，但@帳號最後一定是Bot結尾\n\n建立好了後，會有一組token，請不要隨意給人，因為只要有這組就能發送訊息出去了。\n\n到此已經完成一半了，再來是使用取得channel的id，先將channel的頻道設公開，並把機器人加入到channel裡面。\n這邊需要記得你的chatid，不能跟其他人的重複，所以需要自己試試。\n\nezioflaggernotfiy就是我的chatid，\n然後組合一下api，送出去就能取得channel的id，請先將機器人加入到channel裡面。\nhttps://api.telegram.org/bot{token}/sendMessage?chat_id={chatid}&amp;text=Hello,world\n\n{token} 在 step.3，前面記得加bot\n{chatid} 在step.4 ，記得前面要加 @\n送出去後，回傳的資料上面會顯示 chat.id，此時就能把channel 設成 private了。\n\n如果是公開的channel，就到第四步就結束了。\n如果是私人的話，將chatid改成 ，上面step.4拿到的 chat.id ，即可。\n不用再加@ 在chat_id前面了。此時所用的已是唯一的channel sn。\n\nhttps://api.telegram.org/bot1234:JMwL6qw/sendMessage\\?chat_id\\=-12345\\&amp;text\\=hello,telegram\n\n如果訊息太長，想要有明顯一點的訊息，可加上parse_mode=html\n並在字的前後加上abc，在電腦上看到的字會明顯許多。\n\n(fig.5)\n但在手機上的telegram，看不到此特效。\nref.\n\nTelegram Bot機器人申請與Webhook指令全紀錄\n如何獲得正確的telegram channel id?？\nAn introduction for developers\ntelegram-sendMessage\nsendMessage: Send text messages\n關於 Telegram 上的解析模式（Markdown、HTML）、文字格式",
		"tags": [ "note","💻"]
},

{
		"title": "正則取代，留下特定字串",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/38.正則取代，留下特定字串/",
		"content": "前言\n懶人的方式又來了，\n有一整批的資料，一個一個改我覺得好麻煩，\n那就用正則整批修吧\n正文\n先來看個範例，\n&lt;code&gt;\n\t&lt;field name=&quot;videoId&quot;&gt;&lt;![CDATA[28713]]&gt;&lt;/field&gt;\n\t&lt;field name=&quot;code&quot;&gt;&lt;![CDATA[CWPBD-62]]&gt;&lt;/field&gt;\n\t&lt;field name=&quot;artistId&quot;&gt;&lt;![CDATA[5005]]&gt;&lt;/field&gt;\n\t&lt;field name=&quot;sort&quot;&gt;&lt;![CDATA[9054]]&gt;&lt;/field&gt;\n\t&lt;field name=&quot;views&quot;&gt;&lt;![CDATA[23004]]&gt;&lt;/field&gt;\n&lt;/code&gt;\n\n要將![CDATA[]] 中間的字串留下來，\n這邊其實有兩種做法，\n\n一種是用取代的，先把 ![CDATA[ 刪除，再把 ]] 刪除\n\n還有另一種就是下面的範例，直接用正則取代，留下中間的字串。\n先寫好一個正則，可以抓到 CDATA的資料\n(\\!\\[CDATA\\[)(.*)(\\]\\])\n\n可以到 正則在線測試 測試\n在 ATOM的話，於取代的位置寫上$2 ，這樣就會 留下中間的字了。(fig.1)\n\n如果是在微軟的excel，則是使用 \\2 ，這樣才會留下中間的字。\nref.\nRegular Expression（正則表達式）\n入門修練 Ver3.50",
		"tags": [ "note","💻"]
},

{
		"title": "git refs，branch 跟 tag是一樣的",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/59. git refs，branch 跟 tag是一樣的。/",
		"content": "前言\n正文\ngit的主要架構其實都圍繞着 refs ，\nbranch 跟 tag 其實是一樣的東西，\n直接去看 .git的資料夾，可以看到下面的架構。\n\ttree .git/refs\n\n├── heads\n│   ├── apm2\n│   └── testapmlog\n├── remotes\n│   └── origin\n│   ├── HEAD\n│   ├── apm2\n│   ├── dev_spring20211027\n└── tags\nheads 目錄，描述在本機上的所有分支，每一個檔案對應相應的目錄。\n開啟檔案的話，會看到一個commit的雜湊值，\n而不管是branch 或 tag 都是根據此雜湊值，來決定你程式目前是哪個版本。\ncat .git/refs/heads/apm2\n\ngit log -1 apm2\nor\ngit show apm2\n\n這兩個的雜湊值會一樣，\n所以建立一個新的分支，對git來說，也只是將當下的雜湊值寫到一個檔案內。\nref.\n\nGit系列之Refs 與 Reflog\n原文- Refs and the Reflog",
		"tags": [ "note","💻"]
},

{
		"title": "爬蟲暗黑編年史的世界王時間",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/Golang/155. scrapy爬暗黑編年史的世界王時間/",
		"content": "睡睡念\n好久沒寫爬蟲了，\n最近玩暗黑IV，開始接觸到世界王的時候，時間很不固定，\n然後隔了幾天發現暗黑編年史，\n有世界王的通知時間倒數了。\n有資料就好辦了。\n正文\n\n開啓scrapy專案\n\nscrapy start &lt;project_name&gt;\n\n初始化(在上一個步驟執行完後，就有相關指令會顯示在螢幕上)\n\nscrapy genspider d4spider https://diablo4.cc/tw/\n\n結構如下\n├── LICENSE\n├── README.md\n├── d4notify\n│   ├── __init__.py\n│   ├── __pycache__\n│   │   ├── __init__.cpython-310.pyc\n│   │   └── settings.cpython-310.pyc\n│   ├── items.py\n│   ├── middlewares.py\n│   ├── pipelines.py\n│   ├── settings.py\n│   └── spiders\n│   ├── __init__.py\n│   ├── __pycache__\n│   │   └── __init__.cpython-310.pyc\n│   └── d4spider.py\n└── scrapy.cfg\n\n執行\n\nscrapy crawl d4spider\n\nGITHUB-D4Notfiy\nTroubleshooting\n\n取得屬性\n碰到個奇怪的語法，div裏面有個自訂的屬性，\n用scrapy要取值的話，\n\n&lt;div data-displaytime=&quot;1687258903&quot;&gt;2023-06-20 19:01&lt;/div&gt;\n\ndiv::attr(data-displaytime).get()\n\n這樣會取得 1687258903 這個數字\nref. scrapy-scraping-html-custom-attributes\n\nextract_first() 跟 extract() ,可用get()跟getall()取代\n\nref. 選擇器(Selectors)",
		"tags": [ "note","💻"]
},

{
		"title": "Golang 重新讀取設定檔",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/Golang/40. golang 重新讀取設定檔的方法/",
		"content": "前言\n因為碰到一個奇怪的問題，需要一直去改golang的config檔，\n所以就再想了，能不能像以前的php或html，單純改config，重新整理就能抓到資料。\n結果.......\n正文\n先說結論，直接修改config，然後強制關閉golang的process ，是行不通的。\n環境，golang pod，執行程式的方式，\n是先 打包成一個檔案，再放到alpine linux上面的pod直接執行。\n目標，不重新打包程式修改config\n實驗的方法如下：\n\n先查詢是不是有golang的程式在執行\n\n\tps\n\n由於此程式是直接抓同資料夾的config\n\n悲劇得來了，修改完appsettings.json後，只要刪除/app/main ，就會直接跳出，pod自動重啟。\n\n\tkill -s 26\n\nref.\n在 Linux 中使用 kill、killall 與 xkill 等指令強迫關閉程式\n最後有幾個方案，golang有\n熱更新配置文件\nGolang熱加載配置實踐\n不動程式的話，目前想到的是，直接重新打包，把config丟進去image裡面。\n來源的映像檔，直接改成本來的image，\n然後copy 改完的appsetings.json到目的，\n這樣程式不用重新編譯，但要重新打包image。\ndocker build\nFROM gcr.io/rproject/source:adminQA-0.0.27\nARG website\nARG location\nARG type\nWORKDIR /app\n\nCOPY appsettings.json /app/\n\nEXPOSE 80\n# 不更改權限會發生權限錯誤\nRUN chmod +x /init.sh\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;/init.sh&quot;]\nENTRYPOINT [&quot;sh&quot;,&quot;-x&quot;,&quot;/init.sh&quot;]",
		"tags": [ "note","💻"]
},

{
		"title": "GO get 參數",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/💻 Code/Golang/47. GO get 參數/",
		"content": "前言\n正文\n參數\n描述\n-d\n執行下載，但不執行安裝動作。\n-f\n只有在使用 -u 參數時，才會有效。可以忽略檢查以下載 package 的導入路徑。如果代碼是從其他項目 fork ，就可以採取這樣的做法。\n-fix\n下載 package 先執行修正在進行編譯安裝。\n-insecure\n允許使用 http (非安全) 來下載 package，通常是在內部開發環境使用。\n-t\n下載 package ，在安裝過程同時下載安裝相關依賴代碼包。\n-u\n強制更新已經下載的 package 以及依賴代碼包。（go get 預設指會不會更新已經下載的package)\nref.\nGO MODULE與 GO GET 常用參數說明",
		"tags": [ "note","💻"]
},

{
		"title": "微軟官方office 大量授權",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🖼 Window/113. 微軟官方office 大量授權/",
		"content": "碎碎念\nYT亂看的時候，看到有一個影片，再講如何獲取免費的office，\n這其實不是新聞，但有問題的通常都是再破解上面，\n可能被人惡意亂塞間諜程式或木馬。\n但現在驗證直接從官方來了\n正文\n微軟官方SN：\n以 KMS 和 Active Directory 為基礎的 Office、Project 和 Visio 啟用 GVLK\n不想看圖片的，請直接看影片\n如何免費獲取Office軟件？這是最強的安裝教學！ | 零度解說\n下面快速說明過程，\n\n先到 微軟官方下載 Office Deployment Tool，然後安裝到新的資料夾，這邊指定安裝到\nD:\\office\n\n然後開啓 office 自訂工具\n\n選擇你要的產品、項目\n（附註，LTSC代表的是Long Time Service Channel，長期服務通道)\n產品版本：自行挑選\n語言：繁體中文\n授權與啓用：確認是KMS\n其他預設即可。\n好了後，按右上匯出，然後存到剛剛 1. 的位置\nD:\\office\n\n以最高權限開啓命令提示字元，切換目錄到 剛剛下載的位置，然後下載office，下載完後安裝。\ncd D:\\office\nsetup /download config.xml\nsetup /configure config.xml\n\n安裝完成後應該就會啓動了，如果沒有啓動，到剛剛office安裝的位置，通常在 下面位置，版本不同，後面的數字也會不同。\nC:\\Program Files\\Microsoft Office\\Office16\n\n以最高權限開啓命令提示字元，切換目錄到 5. 的位置，然後依序執行指令，完成後『重開電腦』。\n\ncd C:\\Program Files\\Microsoft Office\\Office16\nslmgr /skms kms.03k.org\nslmgr /ato",
		"tags": [ "note","🖼"]
},

{
		"title": "win10 下載Microsoft Store的檔案",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🖼 Window/174. win10 下載Microsoft Store的檔案/",
		"content": "睡睡唸\n看到XDM有新版的，因爲舊版只要下載有中文字的檔案，\n檔案名稱就是亂碼，但是新版又只能從微軟商店下載...\n正文\n\n找到網址\n首先你要知道你要的程式的微軟官方商店網址。\n直接用google搜尋即可。\n\n到網站 ，貼上你的網址\n\n選擇副檔名為 『appxbundle』 下載\n\nref. Win10/Win11 如何下載 Microsoft Store 離線安裝檔？",
		"tags": [ "note","🖼"]
},

{
		"title": " 🗄 Database MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗄 Database/0.Database MOC/",
		"content": "MySQL\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗄 Database/150. mysql備份還原指令/\">150. mysql備份還原指令</a>\n\nMSSQL\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗄 Database/153. CloudSQL MSSQL減少資料庫大小/\">153. CloudSQL MSSQL減少資料庫大小</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/☁︎ GCP/120. 指令備份cloudSQL及下載/\">120. 指令備份cloudSQL及下載</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗄 Database/188. CloudSQL server 查詢記憶體/\">188. CloudSQL server 查詢記憶體</a>",
		"tags": [ "note","🗄"]
},

{
		"title": "150. mysql備份還原指令",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗄 Database/150. mysql備份還原指令/",
		"content": "睡睡念\n調整自動取得letsEncript的架構，\n所以就獨立開了一個server，要把服務移過去。\n然後才發現，我的上一任是個很成熟的工程師，最愛造輪子\n正文\n以下都是由docker建立，也有其他方式可以做。\n\n備份全部db的指令\n\ndocker exec percona sh -c &quot;mysqldump -u root -p&lt;password&gt; -A --opt&quot; &gt; backup.sql\n\n當然，也可以選擇進入到container裏面，執行指令，再用docker cp的方式把檔案拉出來。\n\n還原DB的指令\n\ndocker exec -i mysql sh -c &quot;mysql -u root -p&lt;password&gt;&quot; &lt; backup.sql\n\n連線至mysql\n\nmysql -uroot -p&lt;password&gt;\n\n顯示全部DB\n\nshow databases;\n\n顯示全部table\n\nshow tables;\n\n建立使用者\n\nCREATE USER 'totp-auth'@'localhost' IDENTIFIED BY 'password';\n\n這邊注意，如果使用者是從非本地連過來資料庫的話，不要使用localhost。我忘了這點。\n\n修改使用者\n\nRENAME USER 'totp-auth'@'localhost' TO 'totp-auth'@'%';\n\n承接上一段語法，如果已經新增的話，用此方式修改。\n%為任意ip，或者可以改成192.168.1.% 諸如此類的。\n\n授予使用者DB權限\n\nGRANT ALL PRIVILEGES ON db.* TO 'totp-auth'@'localhost';\n\nref.\n\npercona(dockerhub)\n[MySQL]資料庫的備份與還原\nMySQL 新增使用者及建立資料庫權限\nMySQL users changed their IP address. What's the best way to deal with this?\n每天定時自動備份Docker內的MySQL、MariaDB與MongoDB，Linux、macOS與Windows都可用",
		"tags": [ "note","🗄"]
},

{
		"title": "153. CloudSQL MSSQL減少資料庫大小",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗄 Database/153. CloudSQL MSSQL減少資料庫大小/",
		"content": "睡睡念\n也不知道我怎麼也兼DBA了，有種MIS的感覺，啥都管。\n如果資料量太多，刪除資料後，是不會把空間釋放的，\n需要對資料庫做壓縮，容量才會還出來。\n正文\n順便說一下查容量的步驟\n\n查資料庫的容量\n\nSELECT DB_NAME(database_id) N'資料庫', physical_name N'實體檔案', type_desc N'檔案類型', state_desc N'檔案狀態', size*8.0/1024 N'檔案大小(MB)'\nFROM sys.master_files\n\n查詢資料表大小\n\nSELECT\nt.NAME AS TableName,\ns.Name AS SchemaName,\np.rows,\nSUM(a.total_pages) * 8 AS TotalSpaceKB,\nCAST(ROUND(((SUM(a.total_pages) * 8) / 1024.00), 2) AS NUMERIC(36, 2)) AS TotalSpaceMB,\nSUM(a.used_pages) * 8 AS UsedSpaceKB,\nCAST(ROUND(((SUM(a.used_pages) * 8) / 1024.00), 2) AS NUMERIC(36, 2)) AS UsedSpaceMB,\n(SUM(a.total_pages) - SUM(a.used_pages)) * 8 AS UnusedSpaceKB,\nCAST(ROUND(((SUM(a.total_pages) - SUM(a.used_pages)) * 8) / 1024.00, 2) AS NUMERIC(36, 2)) AS UnusedSpaceMB\nFROM\nsys.tables t\nINNER JOIN\nsys.indexes i ON t.OBJECT_ID = i.object_id\nINNER JOIN\nsys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id\nINNER JOIN\nsys.allocation_units a ON p.partition_id = a.container_id\nLEFT OUTER JOIN\nsys.schemas s ON t.schema_id = s.schema_id\nWHERE\nt.NAME NOT LIKE 'dt%'\nAND t.is_ms_shipped = 0\nAND i.OBJECT_ID &gt; 255\nGROUP BY\nt.Name, s.Name, p.Rows\nORDER BY\nTotalSpaceMB DESC, t.Name\n\n刪除完後，可以先做交易記錄檔壓縮\n\nALTER DATABASE DB_name SET RECOVERY simple\nuse DB_name\ngo\ndbcc shrinkfile('DB_name_log',2)\n\nALTER DATABASE table_name SET RECOVERY FULL\n\n壓縮DB，將空間釋放，可以用指令，也可以直接用SSMS操作\n指令，可以跟step 3 一起動作\n\n1. DBCC SHRINKDATABASE(N'DB_name' )\n\n或是直接照圖操作\n\nref.\n\nSQL SERVER 壓縮資料庫後MDF沒變小\nMsSql 壓縮資料庫(含修復資料庫)\n[SQL]斯斯有三種，SQL Server 上資料庫的「壓縮」也有三種喔 !\nDBCC SHRINKDATABASE (Transact-SQL)\n清除SQL Server Log檔 (交易紀錄)\n解決大量資料刪除，造成資料庫交易紀錄檔案容量過大且耗費時間之處理",
		"tags": [ "note","🗄"]
},

{
		"title": "MSSQL版本差異",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗄 Database/166. MSSQL 2017版本差異/",
		"content": "前言\n今天在查linux的MSSQL是什麼版本，才發現預設是開發版。\n又延伸一個問題，開發板的上限到哪？\n正文\n因為微軟的網站，連結常死掉，就先節錄過來了，\n在linux上面安裝的是Developer開發者版本，\n功能跟Enterprise一致。\n差異\n指列出資源使用的比較，其他細部功能，請參考下面網址\n\n功能\nEnterprise\n標準\nWeb\nExpress\n快速\n\n單一執行個體所使用的計算容量上限 - SQL Server 資料庫引擎1\n作業系統最大值\n限制為 4 個插槽或 24 個核心的較小者\n限制為 4 個插槽或 16 個核心的較小者\n限制為 1 個插槽或 4 個核心的較小者\n限制為 1 個插槽或 4 個核心的較小者\n\n單一執行個體所使用的計算容量上限 - Analysis Services 或 Reporting Services\n作業系統最大值\n限制為 4 個插槽或 24 個核心的較小者\n限制為 4 個插槽或 16 個核心的較小者\n限制為 1 個插槽或 4 個核心的較小者\n限制為 1 個插槽或 4 個核心的較小者\n\n每個 SQL Server 資料庫引擎執行個體的緩衝集區記憶體上限\n作業系統最大值\n128 GB\n64 GB\n1410 MB\n1410 MB\n\n每個 SQL Server 資料庫引擎執行個體的緩衝集區延伸容量上限\n32 * (最大伺服器記憶體設定)\n4 * (最大伺服器記憶體設定)\nN/A\nN/A\nN/A\n\n每個 SQL Server 資料庫引擎執行個體的資料行存放區區段快取的記憶體上限\n無限制的記憶體\n32 GB\n16 GB\n352 MB\n352 MB\n\nSQL Server 資料庫引擎中每個資料庫的記憶體最佳化資料大小上限\n無限制的記憶體\n32 GB\n16 GB\n352 MB\n352 MB\n\n每個 Analysis Services 執行個體使用的記憶體上限\n作業系統最大值\n表格式：16 GB MOLAP：64 GB\nN/A\nN/A\nN/A\n\n每個 Reporting Services 執行個體使用的記憶體上限\n作業系統最大值\n64 GB\n64 GB\n4 GB\nN/A\n\n關聯式資料庫大小上限\n524 PB\n524 PB\n524 PB\n10 GB\n10 GB\n\n訂價\n\nSQL Server 2017 版本\n適合...\n授權模型\n通路供應情況\n公開無級別價格 (美元)\n\nEnterprise\n全方位的任務關鍵性效能，可滿足嚴苛的資料庫和商業智慧需求。提供最高級的服務和效能，足以應付第 1 層工作負載。[7]\n每個核心[8]\n大量授權、代管\n14,256 美元[9]\n\nStandard - 每一核心\n核心資料管理和商業智慧功能，能以最少的 IT 資源來處理非關鍵的工作負載。\n每個核心[8]\n大量授權、代管\n3,717 美元[9]\n\nStandard - 伺服器 + CAL\n核心資料管理和商業智慧功能，能以最少的 IT 資源來處理非關鍵的工作負載。\n伺服器 + CAL[10]\n大量授權、代管、零售 FPP\n931 美元[9]\n\nDeveloper\n具有完整功能的 SQL Server 軟體版本，可讓開發人員以符合成本效益的方式建置、測試和展示以 SQL Server 軟體為基礎的應用程式。\n每個使用者\n免費下載\n免費\n\nWeb\n安全、符合成本效益且可高度調整的資料平台，可用來打造公用網站。僅供第三方軟體服務提供者使用。\n不適用\n僅提供代管\n請參閱代管合作夥伴的定價\n\nExpress\n免費的入門級的資料庫，非常適合用來學習和建置最多 10 GB 的桌面與小型伺服器資料驅動應用程式。\n不適用\n免費下載\n免費\n\nref.\n- SQLServer 2017\n- SQL Server 2017 的版本及支援功能",
		"tags": [ "note","🗄"]
},

{
		"title": "188. SQL server 查詢記憶體",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗄 Database/188. CloudSQL server 查詢記憶體/",
		"content": "前言\n週五碰到sql server的記憶體飆漲到90%，\n找後端看了一下，看不出個所以然。\n很不幸的，禮拜六又衝到95%，\n只好緊急維護重開了，\n禮拜一開始找問題。\n正文\n實際上要把記憶體操到100%，還真的很難。\n試了好幾天，最多只能到97%。\n然後服務都沒什麼症狀，\n讓我們開始想，是不是cloudSQL真的強，\n這要操下去都沒事。\n再來查到cloudSQL的一篇文章，\nCloudSQL的記憶體分成下面幾種\n\nCaches  快取\nFixed memory  固定記憶體\nSQL Server overhead  SQL Server 開銷\nIn-Memory online transactional processing (OLTP)\n記憶體中線上事務處理 (OLTP)\n\n然後，我們得知可以更改 max server memory，\n強迫SQL不要使用那麼多的記憶體。\n\n然後，再照之前的壓測方式打下去，暴了。\n確定會影響服務。\n再針對 max server memory ，增加容量測試，\n我們得到下面的結論。\n當 Page life expectancy 這個值只要&lt;300，\n就有可能造成服務的延遲。\n當掉的當下，這個值都是處於0。\nPage life expectancy 表示最舊的頁面在緩衝池中停留的時間（以秒為單位），微軟建議這個值再300以上。\nSELECT   [object_name],  [counter_name],  [cntr_value] FROM   sys.dm_os_performance_countersWHERE   [object_name] LIKE   '%Manager%'AND   [counter_name] = 'Page life expectancy'\n\n我們可以同時看一下有沒有SQL處於pending狀態。\nSELECT  @@SERVERNAME AS [Server Name],  RTRIM([object_name]) AS [Object Name],  cntr_value AS [Memory Grants Pending]FROM   sys.dm_os_performance_counters WITH(NOLOCK)WHERE  [object_name] LIKE   N'%Memory Manager%'  -- Handles named instancesAND   counter_name = N'Memory Grants Pending'\n\n當這個值&gt;0時，表示sql服務已經面臨崩潰邊緣，\n該準備重啓資料庫了。\nref. Optimize high memory consumption in instances",
		"tags": [ "note","🗄"]
},

{
		"title": " 🗒 Drone MOC  ",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/0.EFK MOC/",
		"content": "概念\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/55. es基本概念篇/\">55. es基本概念篇</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">81. Beat vs elastic agent</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">83. Elasticsearch 權威指南 修正</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/84.Elasticsearch筆記/\">84.Elasticsearch筆記</a>\n\n建置\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/43.自建ECK on GKE/\">43.自建ECK on GKE</a>\n\n設定\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/⛵️ istio/54. istio 掛載ECK kibana/\">54. istio 掛載ECK kibana</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/52. ILM 設定/\">52. ILM 設定</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/57.filebeat 補充說明/\">57.filebeat 補充說明</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/80. ECK 8.1 ,APM intergration安裝方式/\">80. ECK 8.1 ,APM intergration安裝方式</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/90. elasticsearch ingest pipeline/\">90. elasticsearch ingest pipeline</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/91. elasticsearch ingest processor補充/\">91. elasticsearch ingest processor補充</a>\n\nUsage\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/68.ECK , api筆記/\">68.ECK , api筆記</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/61. kibana 查詢語法/\">Kibana一般使用者查詢語法</a>\n<a class=\"internal-link is-unresolved\" href=\"/404\">86. efk apm圖表</a>\n\nTroubleshooting\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/53. apm error/\">53. apm error</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/60. ECK的 kibana 警告， server.publicBaseUrl is missing/\">60. ECK的 kibana 警告， server.publicBaseUrl is missing</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/67. ECK，filebeat LOG蒐集遺失/\">67. ECK，filebeat LOG蒐集遺失</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/69. kibana錯誤解法 , parent Data too large.../\">69. kibana錯誤解法 , parent Data too large...</a>\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/111. filebeat error/\">111. filebeat error</a>",
		"tags": [ "note","🗒"]
},

{
		"title": "filebeat error",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/111. filebeat error/",
		"content": "碎碎念\n禮拜一上班就聽同事再說，filebeat stage環境有問題，\n我還在禮拜一症候群阿~~\n哎～SRE的宿命，先修吧\n正文\n一開始的問題很簡單，就output service設定錯誤，\n無法連線。\n改完後，換另一個錯誤。\n\nValidation Failed: 1: this action would add [2] total shards, but this cluster currently has [1000]/[1000] maximum shards open\n\ngoogle一下，就看到Elasticsearch 7.x node 開放 1000 個 shards 限制了。\n上面連結是用curl 打網址直接改，下面是到dev Tools下指令，\n所以有點不一樣。\n到kibana上的dev tools加上這段，把1000的上限拉大。\nPUT _cluster/settings\n{\n&quot;persistent&quot;: {\n&quot;cluster.max_shards_per_node&quot;: &quot;3000&quot;\n}\n}\n\nref. cluster-update-settings\nNon-zero metrics in the last 30s\n看filebeat裏面的訊息，一直出現這串。\n關掉吧。\n# If enabled, filebeat periodically logs its internal metrics that have changed\n# in the last period. For each metric that changed, the delta from the value at\n# the beginning of the period is logged. Also, the total values for\n# all non-zero internal metrics are logged on shutdown. The default is true.\n#logging.metrics.enabled: true\n\n# The period after which to log the internal metrics. The default is 30s.\n#logging.metrics.period: 30s\n\n但在filebeat yaml中，是這樣的。\nlogging:\nmetrics.enabled: false\nlevel: info\nto_files: true\nfiles:\npath: /var/log/filebeat\nname: filebeat\nkeepfiles: 7\npermissions: 0644\n\nref. Non-zero metrics in the last 30s: meaning",
		"tags": ["logging", "logging", "note","🗒"]
},

{
		"title": "187. ECK的filebeat",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/187. ECK的filebeat/",
		"content": "前言\n一般filebeat，我之前都是做成daemonset，\n直接抓標準輸出，傳去elasticserach裏面。\n但目前，他們還是習慣寫文字檔到特定目錄，\nfilebeat再去那個目錄抓資料寫入，\n所以變成也要掛載一個readwritemany的nfs。\n那邊設定我還有點困惑，還需要測試。\n正文\nElasticsearch 8.* 版本後，都要求加密憑證，\n所以要用http傳很麻煩，\n可能還要限定版本，最後我也放棄了。\n使用ECK的好處是不用特別設定ca，\n但filebeat這邊的設定就要指定kibana跟elasticsearch了\n但在不同的ns，故需特別指定ns。\napiVersion: beat.k8s.elastic.co/v1beta1\nkind: Beat\nmetadata:\nname: filebeat\nspec:\ntype: filebeat\nversion: 8.11.0\nelasticsearchRef:\nname: fixed\nnamespace: elastic-system\nkibanaRef:\nname: fixed\nnamespace: elastic-system\nconfig:\nfilebeat.inputs:\n#websocketclient\n- type: log\nignore_older: 24h\nenabled: true\npaths:\n- /var/log/app-logs/websocketclient/*.log\njson.keys_under_root: true\njson.add_error_key: true\nfields_under_root: true\ntags: [&quot;prod-sms-back-websocketclient&quot;]\n#filebeat_self\n- type: log\nignore_older: 24h\nenabled: true\npaths:\n- /var/log/filebeat/*.ndjson\njson.keys_under_root: true\njson.add_error_key: true\nfields_under_root: true\ntags: [&quot;filebeat_self&quot;]\nprocessors:\n- timestamp:\nfield: LoggingTime\nlayouts:\n- '2006-01-02T15:04:05Z'\n- '2006-01-02T15:04:05.999Z'\n- '2006-01-02T15:04:05.999-07:00'\ntest:\n- '2019-06-22T16:33:51Z'\n- '2019-11-18T04:59:51.123Z'\n- '2020-08-03T07:10:20.123456+02:00'\n- add_locale:\nformat: offset\n- drop_fields:\nfields: [&quot;agent&quot;, &quot;input&quot;, &quot;host&quot;, &quot;log&quot;, &quot;ecs&quot;, &quot;data_stream&quot;, &quot;event.timezone&quot;, &quot;LoggingTime&quot;]\noutput.elasticsearch:\nusername: &quot;elastic&quot;\npassword: &quot;abc&quot;\nindices:\n#websocketclient\n- index: &quot;prod-sms-back-websocketclient-%{+yyyy.MM.dd}&quot;\nwhen.contains:\ntags: &quot;prod-sms-back-websocketclient&quot;\n#filebeat_self\n- index: &quot;filebeat_self-%{+yyyy.MM.dd}&quot;\nwhen.contains:\ntags: &quot;filebeat_self&quot;\nlogging:\nmetrics.enabled: false\nlevel: info\nto_files: true\nfiles:\npath: /var/log/filebeat\nname: filebeat\nkeepfiles: 7\npermissions: 0644\ndeployment:\npodTemplate:\nspec:\nautomountServiceAccountToken: true\nterminationGracePeriodSeconds: 30\ndnsPolicy: ClusterFirstWithHostNet\nhostNetwork: true # Allows to provide richer host metadata\ncontainers:\n- name: filebeat\nsecurityContext:\nrunAsUser: 0\n# If using Red Hat OpenShift uncomment this:\n#privileged: true\nvolumeMounts:\n- name: applog\nmountPath: /var/log/app-logs\nvolumes:\n- name: applog\npersistentVolumeClaim:\nclaimName: logs-nfs-test-pvc\n\n備註一下，完整ECK安裝的yaml\n# 1. setup eck operator and crd\n# kubectl create -f https://download.elastic.co/downloads/eck/2.10.0/crds.yaml\n# kubectl apply -f https://download.elastic.co/downloads/eck/2.10.0/operator.yaml\n\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\nname: fixed\n# name: yabo\nnamespace: elastic-system\nspec:\nversion: 8.11.0\nnodeSets:\n- name: all\ncount: 1\npodTemplate:\nspec:\ncontainers:\n- name: elasticsearch\nenv:\n- name: ES_JAVA_OPTS\nvalue: -Xms2g -Xmx2g\nresources:\nrequests:\nmemory: 4Gi\nlimits:\nmemory: 4Gi\nconfig:\nnode.roles:\n- master\n- data\n- ingest\nnode.attr.attr_name: attr_value\nnode.store.allow_mmap: false\nvolumeClaimTemplates:\n- metadata:\nname: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 100Gi\nstorageClassName: standard\n# 儲存空間需要加大，預設只有1G 不夠\n\n---\n\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\nname: fixed\nnamespace: elastic-system\nspec:\nversion: 8.11.0\ncount: 1\nelasticsearchRef:\nname: fixed\nhttp:\ntls:\nselfSignedCertificate:\ndisabled: true\nservice:\nspec:\nports:\n- name: http\nport: 5601\ntargetPort: 5601\n\n---\n\nref. ECK部署",
		"tags": ["websocketclient", "filebeat_self", "websocketclient", "filebeat_self", "privileged", "note","🗒"]
},

{
		"title": "自建ECK on GKE",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/43.自建ECK on GKE/",
		"content": "前言\n正文\nElastic Cloud on Kubernetes 簡化了在 Kubernetes 中運行 Elasticsearch 和 Kibana 的作業，包括設置、升級、快照、擴展、高可用性、安全性等\nistio 安裝時，開log\nistioctl manifest install -f loadBalancerSourceRange.yaml --set meshConfig.accessLogFile=/dev/stdout\n\n或是更改yaml ，加上 meshConfig這段\nkind: IstioOperator\napiVersion: install.istio.io/v1alpha1\nmetadata:\nannotations:\ninstall.istio.io/ignoreReconcile: 'true'\nname: istio-external\nnamespace: istio-system\nspec:\nmeshConfig:\naccessLogFile: /dev/stdout\n\nECK\nKubernetes before 1.16以上版本\nkubectl create -f https://download.elastic.co/downloads/eck/2.1.0/crds.yaml\nkubectl apply -f https://download.elastic.co/downloads/eck/2.1.0/operator.yaml\n\n需先安裝 elasticsearch Operator\nref.Elasticsearch (ECK) Operator\n\n佈署 elasticsearch\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\nname: quickstart\nspec:\nversion: 7.15.0\nnodeSets:\n- name: default\ncount: 1\nconfig:\nnode.store.allow_mmap: false\nEOF\n\n檢查狀態\n\tkubectl get elasticsearch\n\n到pod裡面測試elastic是否正常\n密碼需使用 kubectl get secret 取得，<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/43.自建ECK on GKE/#取得密碼：\">#取得密碼：</a>\ncurl https://10.107.201.126:9200 -u 'elastic:J1fO9bu8adfsepYK8rIu91a73o' -k\n\n正常的話，會顯示\n\n佈署kibana\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\nname: quickstart\nspec:\nversion: 7.15.0\ncount: 1\nelasticsearchRef:\nname: quickstart\nEOF\n\n檢查狀態\n\tkubectl get kibana\n\n登入\n帳號：elastic\n取得密碼：\nkubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo\n\n轉port\nkubectl port-forward service/quickstart-kb-http 5601\n\nurl : https://localhost:5601\nbeat\nBeats 其實是一系列工具的總稱，可以解決下列的問題：\n\n讀取檔案\n提取指標\n提取網路資料\n測試服務可用性\n\n有這些 Filebeat, Metricbeat,Heartbeat,Packetbeat...等\n\n查詢目前有哪些index了\n\n到DevTools執行\nGET _cat/indices?s=index\n\nref.\nBeats\n【ES新手，破門而入！】Day9 - 我見故我在！Observability 基礎之趴特睡\n【ES新手，破門而入！】Day13 - 關於 Metrics 的 23456 事\nFilebeat\n日誌處理工具，只要會產生日誌的地方都可以使用它，它可以幫助你將日誌傳送到 Elasticsearch 或其他的系統。\n\nlog 位置\n/var/log/containers\n查詢傳了哪些資料到elastic search裡面。\n同樣使用 Devtools\nGET filebeat-*/_search\n\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/57.filebeat 補充說明/\">57.filebeat 補充說明</a>\nref.\nElastic Logging X Filebeat 深入理解\nMetricsbeat\n將指標傳送到 Elasticsearch 分析，並搭配 Kibana 做視覺化。\n指標著重在資訊的週期性測量，讓我們可以瞭解系統的狀態如硬碟空間、CPU使用率等等。\nref. ES新手，破門而入！】Day10 - 眼見為憑！Observability 基礎之趴特佛\nAPM\nAPM 的全名是 Application Performance Monitoring（應用程式效能監控），是用來回答下面兩個主要的問題：\n\n應用程式要花多久時間來答覆請求（request）？\n應用程式遇到什麼種類的錯誤？\n\nElastic APM 是由下列 4 個組件所構成：\n\nAPM Agents\nAPM Server\nElasticsearch\nKibana\n\n佈署APM server\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apm.k8s.elastic.co/v1\nkind: ApmServer\nmetadata:\nname: apm-server-quickstart\nnamespace: default\nspec:\nversion: 7.15.0\ncount: 1\nelasticsearchRef:\nname: quickstart\nkibanaRef:\nname: quickstart\nconfig:\noutput:\nelasticsearch:\nhost: [quickstart-es-http.default:9200]\nusername: elastic\npassword: &quot;E1R10XXXXXXfr40&quot;\nprotocol: &quot;http&quot;\nEOF\n\n這邊的password要直接下指令查詢，這個是前面登入kibana的密碼。\nkubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo\n\n為了環境單純，所以只使用http，要用https請參考連結 Reference an existing Elasticsearch cluster。\nref.\nElastic APM 基礎教學\nES新手，破門而入！】Day10 - 眼見為憑！Observability 基礎之趴特佛\nRun APM Server on ECK\nGO-APM agent\n在建立 apm-server的時候，\n會同時建立好 secret token ，這個同樣需要下指令取得，\n之後在 agent上面設定完，才能將資料傳到 APM server上面。\n這邊取config name為 ( [apm name]apm-token )\nkubectl get secret apm-server-quickstart-apm-token -o=jsonpath='{.data.secret-token}' | base64 --decode; echo\n\n設定APM server環境變數，由於我的服務都是docker image，\n所以要把變數寫在Dockerfile裡面，當打包時，一併設定好環境變數。\n\n...\nENV ELASTIC_APM_SERVER_URL=https://apm-server-quickstart-apm-http.default:8200\nENV ELASTIC_APM_SERVICE_NAME=webhook\nENV ELASTIC_APM_SECRET_TOKEN=9m9XXXXXXGjwdIk41\nENV ELASTIC_APM_VERIFY_SERVER_CERT=false\nENV ELASTIC_APM_ENVIRONMENT=qa\n...\n\n然後再 main.go ，加上下面幾行。\n這份go的程式，本身使用了 gorilla/mux的web框架\n所以照官方文件的說明，加上下面幾行。\nimport (\n\t&quot;github.com/gorilla/mux&quot;\n\n\t&quot;go.elastic.co/apm/module/apmgorilla&quot;\n)\n\nfunc main() {\n\trouter := mux.NewRouter()\n\tapmgorilla.Instrument(router)\n\thttp.ListenAndServe(&quot;:80&quot;, router)\n}\n\n主要只有 apmgorilla.Instrument(router) 這行，\n然後只要有經過80 port的資料就通通都會進入到APM裡面了。\n\n這張圖裡面的TPS意思，可參考 <a class=\"internal-link is-unresolved\" href=\"/404\">blog.45.網站效能指標一覽（未）</a>\nref.\nAPM Server secret token\n如何使用 Elastic APM Go 代理為 Go 應用裝載測量工具\n[Docker 環境變數使用筆記](https://myapollo.com.tw/zh-tw/docker-env/)\nElastic APM-Go Agent介紹(中文翻譯)\nconnect to the APM Server\nSolutions：安全的APM服務器訪問\nvue-APM agent\n安裝依賴套件\nKibana\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/61. kibana 查詢語法/\">61. kibana 查詢語法</a>\nref.\nKibana Query Language",
		"tags": ["取得密碼：", "note","🗒"]
},

{
		"title": "ILM 設定",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/52. ILM 設定/",
		"content": "前言\n使用預設的filebeat.yaml設定，\n導致現在elasticsearch的log爆炸了。\n連開都開不起來。\n所以這次要解決兩個問題，\n\n自動刪除舊的log\n將log根據日期分配\n\n經過一早上的研究，發現我想得跟我要解決的問題不一樣XDDD\n所以本文會在探討ILM的設定\n正文\n有些觀念跟我想得不一樣，\n下圖看到的 indices，是在elasticSearch（簡稱ES）上，\n所建立的。\nES上的indices會長這樣\n\n所以跟fileBeat沒有關係，fileBeat只負責把資料丟過去ES上。\n當然裡面也有一些設定，等以後有空再來研究。\n首先到 kibana的畫面，\n到 Management 中去設定ILM\n\n裡面的設定，請參考下面文章，\n或是直接看英文也可以懂。\n但最好先看一下文章，es的 indices 區分成三個階段，\n改完設定後，並不會立即生效，只有當過度到下一個階段的時候，配置才會生效。\n也就是說，如果你一直在 Hot phase，\n那永遠不會執行你剛設定的參數。\n但一當你進入Warm phase時，\n你剛剛設定的參數就都會執行了。\n\n另外在各個 phase ，能做的事情也都不一樣。\n\nIndex Lifecycle Management 的階段\n此階段中可以執行的操作有什麼\n\nHot\nForce merge, Rollover, Set priority, Unfollow\n\nWarm\nAllocate, Force merge, Read only, Set priority, Shrink, Unfollow\n\nCold\nAllocate, Freeze, Set priority, Unfollow\n\nDelete\nDelete, Wait for snapshot\n\n像要刪除 indices 的話，就必須要到 Delete phase的階段才可以。\nps. 如果發現沒有看到 delete phase，\n要先把下圖的開關切換過去，delete phase 才會出現。\n\n設定完後，可以到 Dev Tools ，\n\n查詢有哪些 indices\n\nGET /_cat/indices\n\n查indices狀態\n將右邊的 indics，取代下面的url 。\n這邊使用的 indices 名稱為 filebeat-7.15.0-2021.10.07-000001 ，如果要知道json內的內容意義，請參考下面連結『Explain lifecycle API』。\n\nGET filebeat-7.15.0-2021.10.07-000001/_ilm/explain\n\n啟動 ILM\n\nPOST /_ilm/start\n\nref.\n\n喬叔教 Elastic - 11 - 管理 Index 的 Best Practices (3/7) - Index Lifecycle Management (ILM)\n使用索引生命週期管理實現熱溫冷架構\nExplain lifecycle API\nAutomatically delete old indices\n\n跟這次的問題沒關係，但還是留個連結\n\nFilebeat \bharvester 的 file handler close 與 clean 機制\nfilebeat進程寫滿磁盤的情況處理\n一篇文章搞懂filebeat（ELK）\nFilebeat配置參考手冊]\n\ncurator\n\nelasticsearch curator安裝及應用\nInstall and configure Elasticsearch Curator to delete the old indices.\nELK 教學 - 定期清除 Elasticsearch 資料",
		"tags": [ "note","🗒"]
},

{
		"title": "apm error",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/53. apm error/",
		"content": "前言\n正文\n錯誤訊息\n\nThere are no ingest nodes in this cluster, unable to forward request to an ingest node.\n\nES的yaml\n增加\nnode.roles:\n- ingest\n\nref.\n\nNode\n如何在 Elasticsearch 中使用 pipeline API 來對事件進行處理\nThere are no ingest nodes in this cluster, unable to forward request to an ingest node",
		"tags": [ "note","🗒"]
},

{
		"title": "es基本概念篇",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/55. es基本概念篇/",
		"content": "前言\n正文\n先來個比較容易對比的表格，但不完全一樣，\n有幾個是我自己加上去的。\n\nRDBMS\nElasticsearch\n\nVM\nNode\n\nTable\nIndex\n\nRow\nDocument\n\nColumn\nField\n\nSchema\nMapping\n\nSQL\nDSL\n\nref. [Elasticsearch] 基本概念 &amp; 搜尋入門\n摘要\nNode\n\nMaster Node\n\n處理建立/刪除 index 的 request，並實際執行\n決定每個 shard 要被分配到哪個 data node 上\n維護 &amp; 更新 cluster state\n當 master node出問題，master eligible node（從cluster設定多個）會開始選舉，選出下一個 master node\n\nCoordinating Node(協調節點)\n\n所有node預設都是 Coordinating Node\n處理request的node 以及 最後進行結果的彙整\ncoordinating node 可以直接接收 search request 並處理，不需要透過 master node 轉過來\n\nData Node\n\n可以保存資料的 node，每個 node 啟動後都會預設是 data node，可以透過設定 node.data: false 停用 data node 功能\n透過增加 data node 可以解決資料水平擴展 &amp; 解決單點故障導致資料遺失的問題\n\nCluster State（集群狀態)\n\n包含所有node資訊\n包含所有index 及 相對應的 mapping/setting配置\nshard的路由資訊\n每個node上都有Cluster State，只有master 可修改\n\nShard\n將索引劃分成多份的能力，這些份就叫做分片（shard）\n例如你有一個index （1T) ，要怎麼存到不同的Node，\n此時就需要使用(shard)\n\nshard 是 ES分散式儲存的基礎，包含 primary shard 及 replica shard\n\n每一個shard 就是一個 Lucene instance(用於全文檢索和搜尋 e.g. Solr,Elasticsearch...)\n\nPrimary Shard(提升系統儲存容量)\n\n將一份被索引後的資料，分散到多個data node上存放\nprimary shard 的數量在建立 index時就會指定，無法後續修改\n\nReplica Shard（提高資料可用性)\n\n當primary shard遺失，Replica shard可以被推成 primary shard\nreplica shard數量可動態調整，讓每個data node上都有完整資料\n可一定程度提高查詢的效能\nES7.0開始，primary shard 預設為1 ,replica shard預設 為0\n\n如不設定replica shard，一旦data node 故障導致 primary shard遺失，資料無法復原\n\nShard設定\n\nreplica shard設定過多，會降低clsuter整體的寫入效能\n\nreplica shard 必須和 primary shard 被分配在不同的 data node 上；但所有的 primary shard 可以在同一個 data node 上\n\nCommand\n\n取得Cluster 健康狀態\n\tGET _cluster/health/\n\nStatus\n- Green ：表示 Primary shard &amp; Replica Shard可正常分配\n- Yellow：表示 Primay shard可正常分配，但Replica shard 分配有問題\n- Red：有Primary shard 無法正常分配\n\n取得 Shard狀態\n\tGET _cat/shards\n\np ：代表 primary shard\nr ：代表 replica shard\n2. shard的分佈情形，在哪個node上\n3. 每個shard包含的document數量 &amp; 空間\n情境\n\n如果cluster 只有一個node ，全部會被 primary shard佔據，導致 replica shard 無法被分配，因此cluster健康狀態為 yellow\n\n資料寫入\n\ncache\n\n資料寫入後無法變動，避免lock機制所帶來的效能問題\n因不可變動，若要讓新的document 可被搜尋，需要重建index\n寫入document前，會先寫到 Index Buffer的儲存空間\n滿足條件(Index Buffer佔滿) or 特定時間(預設一秒一次)，會將 Buffer 寫入 Segment 。寫入的過程叫Refresh。\n\ntransaction log\n\n每個shard都有對應的transaction log\n寫入document的時候，同時也會寫入 transaction log\n\nref.\n\n[Elasticsearch] 分散式特性 &amp; 分散式搜尋的機制\nElasticsearch 中的一些重要概念: cluster, node, index, document, shards 及 replica\nElasticsearch 基本原理及規劃",
		"tags": [ "note","🗒"]
},

{
		"title": "filebeat 補充說明",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/57.filebeat 補充說明/",
		"content": "內容\n<a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/57.filebeat 補充說明/#2024-05-28\">#2024/05/28補充</a>\nfilebeat抓資料的方式\n\nautodiscover\nwithout autodiscover\n\n要注意的是，如果用(2) 的方式，在GKE上面，你要使用 add_kubernetes_metadata，去取得pod name,namespace..等，\n通通抓不到。\n因為(2)是直接去node的log裡面抓資料，並沒有透過api的方式，\n所以如果要取得 pod name的話，可能要從log path着手。\n應該是可以從ingest 去處理。\n但用(1)，資料就通通進來了。\n\n所以還是用(1)吧。\nref.\n\nConfiguration Examples\nAdd Kubernetes metadata\n\n排除資料的方式\n沒有記錄log不知道，一記錄了才知道。\n資料有夠肥。\n開始跟同仁說，沒有必要的不要留。\n但有些是先天限制，就是會拼命印，那該怎麼解決。\n\n排除訊息內的特定文字，通通不收\n在autodiscover 底下使用 exclude_lines\n\nautodiscover:\nproviders:\n- type: kubernetes\nnode: ${NODE_NAME}\nhints:\nenabled: true\ndefault_config:\ntype: container\npaths:\n- /var/log/containers/*${data.kubernetes.container.id}.log\nexclude_lines:\n- &quot;.*(NAT rule custom-iptables|unknown operand).*&quot; # 排除 k8s-custom-iptables\n\nref. Log input\n\n排除特定namespace\n在 processors 底下加 drop_event，這邊有用到condition的條件，建議看一下文件比較清楚。\n\nprocessors:\n- add_cloud_metadata: {}\n- drop_event.when:\nor:\n- equals:\nkubernetes.namespace: &quot;kube-system&quot;\n- equals:\nkubernetes.namespace: &quot;elastic-system&quot;\n\nref.\n\nDrop events\nhow-we-can-filter-namespace-in-filebeat-kubernetes\nconditions\n\n移除特定欄位\n有些欄位得到的值是重複的，\n所以這些也都可以砍掉。\nprocessors 底下加 drop_fields\nprocessors:\n- drop_fields:\nfields:\n- 'kubernetes.node'\n- 'kubernetes.namespace_uid'\n- 'agent.hostname'\n- 'cloud.account'\n- 'cloud.provider'\n- 'cloud.instance.name'\n- 'container.id'\n\nref. Drop fields from events\n\n詳細參數說明\ntype: log #input類型為log\nenable: true #表示是該log類型配置生效\npaths： #指定要監控的日誌，目前按照Go語言的glob函數處理。沒有對配置目錄做遞歸處理，比如配置的如果是：\n- /var/log/* /*.log #則只會去/var/log目錄的所有子目錄中尋找以&quot;.log&quot;結尾的文件，而不會尋找/var/log目錄下以&quot;.log&quot;結尾的文件。\nrecursive_glob.enabled: #啟用全局遞歸模式，例如/foo/**包括/foo, /foo/*, /foo/*/*\nencoding：#指定被監控的文件的編碼類型，使用plain和utf-8都是可以處理中文日誌的\nexclude_lines: ['^DBG'] #不包含匹配正則的行\ninclude_lines: ['^ERR', '^WARN'] #包含匹配正則的行\nharvester_buffer_size: 16384 #每個harvester在獲取文件時使用的緩沖區的字節大小\nmax_bytes: 10485760 #單個日誌消息可以擁有的最大字節數。max_bytes之後的所有字節都被丟棄而不發送。默認值為10MB (10485760)\nexclude_files: ['\\.gz$'] #用於匹配希望Filebeat忽略的文件的正則表達式列表\ningore_older: 0 #默認為0，表示禁用，可以配置2h，2m等，注意ignore_older必須大於close_inactive的值.表示忽略超過設置值未更新的\n文件或者文件從來沒有被harvester收集\nclose_* #close_ *配置選項用於在特定標准或時間之後關閉harvester。 關閉harvester意味著關閉文件處理程序。 如果在harvester關閉\n後文件被更新，則在scan_frequency過後，文件將被重新拾取。 但是，如果在harvester關閉時移動或刪除文件，Filebeat將無法再次接收文件\n，並且harvester未讀取的任何數據都將丟失。\nclose_inactive #啟動選項時，如果在制定時間沒有被讀取，將關閉文件句柄\n讀取的最後一條日誌定義為下一次讀取的起始點，而不是基於文件的修改時間\n如果關閉的文件發生變化，一個新的harverster將在scan_frequency運行後被啟動\n建議至少設置一個大於讀取日誌頻率的值，配置多個prospector來實現針對不同更新速度的日誌文件\n使用內部時間戳機制，來反映記錄日誌的讀取，每次讀取到最後一行日誌時開始倒計時使用2h 5m 來表示\nclose_rename #當選項啟動，如果文件被重命名和移動，filebeat關閉文件的處理讀取\nclose_removed #當選項啟動，文件被刪除時，filebeat關閉文件的處理讀取這個選項啟動後，必須啟動clean_removed\nclose_eof #適合只寫一次日誌的文件，然後filebeat關閉文件的處理讀取\nclose_timeout #當選項啟動時，filebeat會給每個harvester設置預定義時間，不管這個文件是否被讀取，達到設定時間後，將被關閉\nclose_timeout 不能等於ignore_older,會導致文件更新時，不會被讀取如果output一直沒有輸出日誌事件，這個timeout是不會被啟動的，\n至少要要有一個事件發送，然後haverter將被關閉\n設置0 表示不啟動\nclean_inactived #從注冊表文件中刪除先前收獲的文件的狀態\n設置必須大於ignore_older+scan_frequency，以確保在文件仍在收集時沒有刪除任何狀態\n配置選項有助於減小注冊表文件的大小，特別是如果每天都生成大量的新文件\n此配置選項也可用於防止在Linux上重用inode的Filebeat問題\nclean_removed #啟動選項後，如果文件在磁盤上找不到，將從注冊表中清除filebeat\n如果關閉close removed 必須關閉clean removed\nscan_frequency #prospector檢查指定用於收獲的路徑中的新文件的頻率,默認10s\ntail_files：#如果設置為true，Filebeat從文件尾開始監控文件新增內容，把新增的每一行文件作為一個事件依次發送，\n而不是從文件開始處重新發送所有內容。\nsymlinks：#符號鏈接選項允許Filebeat除常規文件外,可以收集符號鏈接。收集符號鏈接時，即使報告了符號鏈接的路徑，\nFilebeat也會打開並讀取原始文件。\nbackoff： #backoff選項指定Filebeat如何積極地抓取新文件進行更新。默認1s，backoff選項定義Filebeat在達到EOF之後\n再次檢查文件之間等待的時間。\nmax_backoff： #在達到EOF之後再次檢查文件之前Filebeat等待的最長時間\nbackoff_factor： #指定backoff嘗試等待時間幾次，默認是2\nharvester_limit：#harvester_limit選項限制一個prospector並行啟動的harvester數量，直接影響文件打開數\n\ntags #列表中添加標簽，用過過濾，例如：tags: [&quot;json&quot;]\nfields #可選字段，選擇額外的字段進行輸出可以是標量值，元組，字典等嵌套類型\n默認在sub-dictionary位置\nfilebeat.inputs:\nfields:\napp_id: query_engine_12\nfields_under_root #如果值為ture，那麼fields存儲在輸出文檔的頂級位置\n\nmultiline.pattern #必須匹配的regexp模式\nmultiline.negate #定義上面的模式匹配條件的動作是 否定的，默認是false\n假如模式匹配條件'^b'，默認是false模式，表示講按照模式匹配進行匹配 將不是以b開頭的日誌行進行合並\n如果是true，表示將不以b開頭的日誌行進行合並\nmultiline.match # 指定Filebeat如何將匹配行組合成事件,在之前或者之後，取決於上面所指定的negate\nmultiline.max_lines #可以組合成一個事件的最大行數，超過將丟棄，默認500\nmultiline.timeout #定義超時時間，如果開始一個新的事件在超時時間內沒有發現匹配，也將發送日誌，默認是5smax_procs #設置可以同時執行的最大CPU數。默認值為系統中可用的邏輯CPU的數量。name #為該filebeat指定名字，默認為主機的hostname\n\nref.\n\n一篇文章搞懂filebeat（ELK）\n\nfilebeat log寫入到檔案\nfilebeat在運作時，會產生一堆log，這些我原本都不想讓他輸出，直接寫在自己的檔案就好。\n\n但沒測試成功，最後只好採用治標不治本的方式，\n在該namespace底下的log通通不進ES。\n留一下查到的資料。\nlogging:\nto_files: true\nmetrics.enabled: false\nfiles:\npath: '/usr/share/filebeat/logs'\nname: 'filebeat'\nrotateeverybytes: 10485760 # = 10MB\nkeepfiles: 7\npermission: '0640'\n\nref.\n\nConfigure logging\n\n完整yaml，使用operator\napiVersion: beat.k8s.elastic.co/v1beta1\nkind: Beat\nmetadata:\nname: yabo\nnamespace: elastic-system\nspec:\ntype: filebeat\nversion: 7.15.0\nelasticsearchRef:\nname: yabo\nkibanaRef:\nname: yabo\nconfig:\nfilebeat:\nautodiscover:\nproviders:\n- type: kubernetes\nnode: ${NODE_NAME}\nhints:\nenabled: true\ndefault_config:\ntype: container\npaths:\n- /var/log/containers/*${data.kubernetes.container.id}.log\nexclude_lines:\n- &quot;.*(NAT rule custom-iptables|unknown operand).*&quot; # 排除 k8s-custom-iptables\n- &quot;.*(YABO_WS.Services.PromoWebSocketService|YABO_WS.BLL.ActivityEventLogic).*&quot;\nprocessors:\n- add_cloud_metadata: {}\n# - add_host_metadata: {}\n- drop_event.when:\nor:\n- equals:\nkubernetes.namespace: &quot;kube-system&quot;\n- equals:\nkubernetes.namespace: &quot;elastic-system&quot;\n- drop_fields:\nfields:\n- 'kubernetes.node'\n- 'kubernetes.namespace_uid'\n- 'agent.hostname'\n- 'cloud.account'\n- 'cloud.provider'\n- 'cloud.instance.name'\n- 'container.id'\n- decode_json_fields:\nfields: [&quot;message&quot;]\ntarget: &quot;&quot;\noverwrite_keys: true\n# 測試沒用，無法寫到\n# logging:\n# to_files: true\n# metrics.enabled: false\n# files:\n# path: '/usr/share/filebeat/logs'\n# name: 'filebeat'\n# rotateeverybytes: 10485760 # = 10MB\n# keepfiles: 7\n# permission: '0644'\ndaemonSet:\npodTemplate:\nspec:\nserviceAccountName: filebeat\nautomountServiceAccountToken: true\nterminationGracePeriodSeconds: 30\ndnsPolicy: ClusterFirstWithHostNet\nhostNetwork: true # Allows to provide richer host metadata\ncontainers:\n- name: filebeat\nsecurityContext:\nrunAsUser: 0\n# If using Red Hat OpenShift uncomment this:\n#privileged: true\nvolumeMounts:\n- name: varlogcontainers\nmountPath: /var/log/containers\n- name: varlogpods\nmountPath: /var/log/pods\n- name: varlibdockercontainers\nmountPath: /var/lib/docker/containers\nenv:\n- name: NODE_NAME\nvalueFrom:\nfieldRef:\nfieldPath: spec.nodeName\nvolumes:\n- name: varlogcontainers\nhostPath:\npath: /var/log/containers\n- name: varlogpods\nhostPath:\npath: /var/log/pods\n- name: varlibdockercontainers\nhostPath:\npath: /var/lib/docker/containers\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: filebeat\nrules:\n- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group\nresources:\n- namespaces\n- pods\n- nodes\nverbs:\n- get\n- watch\n- list\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: filebeat\nnamespace: elastic-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: filebeat\nsubjects:\n- kind: ServiceAccount\nname: filebeat\nnamespace: elastic-system\nroleRef:\nkind: ClusterRole\nname: filebeat\napiGroup: rbac.authorization.k8s.io\n\n...\n\nref.\n\nECK部署\nFileBeat\nElastic Beats on Google Kubernetes Engine (GKE)\nfilebeat-kubernetes.yaml\nHow to Setup an ELK Stack and Filebeat on Kubernetes\n\n2024/05/28補充\n使用autodiscover，拆分兩個log indices\n以下程式碼為『片段』。\napiVersion: beat.k8s.elastic.co/v1beta1\nkind: Beat\nmetadata:\nname: prod\nnamespace: elastic-system\nspec:\ntype: filebeat\nversion: 8.13.2\nelasticsearchRef:\nname: prod\nnamespace: elastic-system\nkibanaRef:\nname: prod\nnamespace: elastic-system\nconfig:\nfilebeat:\nautodiscover:\nproviders:\n- type: kubernetes\nnode: ${NODE_NAME}\ntemplates:\n- condition:\nequals:\nkubernetes.labels.app: &quot;video&quot;\nconfig:\n- type: container\npaths:\n- /var/log/containers/*${data.kubernetes.container.id}.log\nprocessors:\n- add_kubernetes_metadata:\nhost: ${NODE_NAME}\n- condition:\nequals:\nkubernetes.labels.app: &quot;auth&quot;\nconfig:\n- type: container\npaths:\n- /var/log/containers/*${data.kubernetes.container.id}.log\nprocessors:\n- add_kubernetes_metadata:\nhost: ${NODE_NAME}\nprocessors:\n- add_cloud_metadata: {}\noutput.elasticsearch:\nusername: &quot;elastic&quot;\npassword: &quot;password&quot;\nindex: &quot;logs-%{[kubernetes.labels.app]}-%{+yyyy.MM.dd}&quot;\nindices:\n- index: &quot;videoworker-%{+yyyy.MM.dd}&quot;\nwhen.equals:\nkubernetes.labels.app: &quot;videoworker&quot;\n- index: &quot;authcontrol-%{+yyyy.MM.dd}&quot;\nwhen.equals:\nkubernetes.labels.app: &quot;authcontrol&quot;\nsetup.template.enabled: true\nsetup.template.name: &quot;filebeat-8.13.2&quot;\nsetup.template.pattern: &quot;filebeat-8.13.2&quot;\n\n說明：\n\nconfig.filebeat.autodiscover.providers ：底下使用template，作為拆分的範本，原先是使用hints機制，使用關鍵label作為擷取的方式。\n\noutput.elasticsearch: 輸出到elasticsearch ，預設的index使用 logs作為datastream，底下的indices則是再細分規則， 如果符合規則，則使用其他的datastream名稱。\n官方文件，是不用寫output.elasticsearch，但我搞不出來。\n改天在繼續追了。\n\nref.\n\nHints based autodiscover\nFilebeat example",
		"tags": ["2024/05/28補充", "input類型為log", "表示是該log類型配置生效", "指定要監控的日誌，目前按照Go語言的glob函數處理。沒有對配置目錄做遞歸處理，比如配置的如果是：", "則只會去/var/log目錄的所有子目錄中尋找以", "啟用全局遞歸模式，例如/foo/", "不包含匹配正則的行", "包含匹配正則的行", "每個harvester在獲取文件時使用的緩沖區的字節大小", "單個日誌消息可以擁有的最大字節數。max_bytes之後的所有字節都被丟棄而不發送。默認值為10MB", "用於匹配希望Filebeat忽略的文件的正則表達式列表", "默認為0，表示禁用，可以配置2h，2m等，注意ignore_older必須大於close_inactive的值", "close_", "啟動選項時，如果在制定時間沒有被讀取，將關閉文件句柄", "當選項啟動，如果文件被重命名和移動，filebeat關閉文件的處理讀取", "當選項啟動，文件被刪除時，filebeat關閉文件的處理讀取這個選項啟動後，必須啟動clean_removed", "適合只寫一次日誌的文件，然後filebeat關閉文件的處理讀取", "當選項啟動時，filebeat會給每個harvester設置預定義時間，不管這個文件是否被讀取，達到設定時間後，將被關閉", "從注冊表文件中刪除先前收獲的文件的狀態", "啟動選項後，如果文件在磁盤上找不到，將從注冊表中清除filebeat", "prospector檢查指定用於收獲的路徑中的新文件的頻率", "backoff選項指定Filebeat如何積極地抓取新文件進行更新。默認1s，backoff選項定義Filebeat在達到EOF之後", "在達到EOF之後再次檢查文件之前Filebeat等待的最長時間", "指定backoff嘗試等待時間幾次，默認是2", "列表中添加標簽，用過過濾，例如：tags", "可選字段，選擇額外的字段進行輸出可以是標量值，元組，字典等嵌套類型", "如果值為ture，那麼fields存儲在輸出文檔的頂級位置", "必須匹配的regexp模式", "定義上面的模式匹配條件的動作是", "可以組合成一個事件的最大行數，超過將丟棄，默認500", "定義超時時間，如果開始一個新的事件在超時時間內沒有發現匹配，也將發送日誌，默認是5smax_procs", "設置可以同時執行的最大CPU數。默認值為系統中可用的邏輯CPU的數量。name", "為該filebeat指定名字，默認為主機的hostname", "privileged", "note","🗒"]
},

{
		"title": "除錯用的ECK API",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/58. 除錯用的ECK API/",
		"content": "前言\n很久之前要寫一直沒寫，\n最近碰到問題，來還了。\n正文\nPrimary Shard (提昇系統儲存容量)\n\nshard 是 Elasticsearch 分散式儲存的基礎，包含 primary shard &amp; replica shard\n\n每一個 shard 就是一個 Lucene instance\n\nprimary shard 功能是將一份被索引後的資料，分散到多個 data node 上存放，實現儲存方面的水平擴展\n\nprimary shard 的數量在建立 index 時就會指定，後續是無法修改的，若要修改就必須要進行 reindex\n\nReplica Shard (提高資料可用性)\n\nreplica shard 用來提供資料高可用性，當 primary shard 遺失時，replica shard 就可以被 promote 成 primary shard 來保持資料完整性\n\nreplica shard 數量可以動態調整，讓每個 data node 上都有完整的資料\n\nreplica shard 可以一定程度的提高讀取(查詢)的效能\n\n若不設定 replica shard，一旦有 data node 故障導致 primary shard 遺失，資料可能就無法恢復了\n\nES 7.0 開始，primary shard 預設為 1，replica shard 預設為 0\n\nref. [Elasticsearch] 分散式特性 &amp; 分散式搜尋的機制\nShard 的規劃 &amp; 設定\n\nprimary shard 數量設定太小會遇到以下問題：\n\n若 index 資料增加很快時，cluster 無法通過增加 node 數量對 index 進行資料擴展\n單一 shard 資料太大，導致資料重新分配耗時\n\nprimary shard 數量設定太大會遇到以下問題：\n\n導致每個 shard 容量很小，讓一個 data node 上有過多 shard 而影響效能\n影響搜尋時的相關性算分，會讓統計結果失準\n\nreplica shard 若設定過多，會降低 cluster 整體的寫入效能\n\n# 查node的狀態\nGET _cluster/health/\n\n# 取得目前所有的shard，加上?v 可顯示欄位名稱\nGET _cat/shards?v\n\n# 取得目前的shard設定數量（好幾次都是這個值暴了，導致log無法寫入)\nGET /_cluster/settings\n\n# 設定node的shard最大數量\nPUT /_cluster/settings\n{\n&quot;persistent&quot;: {\n&quot;cluster.max_shards_per_node&quot;: 1000\n}\n}\n\nref.\n\ncat shards API欄位介紹",
		"tags": [ "note","🗒"]
},

{
		"title": "ECK的 kibana 警告， server.publicBaseUrl is missing",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/60. ECK的 kibana 警告， server.publicBaseUrl is missing/",
		"content": "前言\n正文\nkibana時常這個警告出來，\n但我一直找不到config要在哪設定。\n\n之後問了另外一組的大大，他之前也在弄ECK，\n給了我這段。\n加一個config在yaml裡面\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\nname: yabo\nnamespace: elastic-system\nspec:\nversion: 7.15.0\ncount: 1\nconfig:\nserver.publicBaseUrl: http://123.232.162/\nelasticsearchRef:\nname: yabo\nhttp:\ntls:\nselfSignedCertificate:\ndisabled: true\nservice:\nspec:\nports:\n- name: http\nport: 5601\ntargetPort: 5601\n\nref.\n\n[Solved] Kibana 7.14.0 error: server.publicBaseUrl is missing and should be configured when running in a production environment.\nConfigure Kibana",
		"tags": [ "note","🗒"]
},

{
		"title": "kibana 查詢語法",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/61. kibana 查詢語法/",
		"content": "前言\n正文\nKQL其實沒很複雜，\n建議把介紹Kibana Query Language看完後基本上就會了。\n主要常用的是，使用 特定欄位搜尋，\n\n萬用字元 *\n例如，\n今天要搜尋一個 pod Name 叫 grafana\n那KQL 可輸入\n\nkubernetes.pod.name: *grafana*\n\n搜尋出來的名稱，只要是 中間有 grafana的通通會列出。\n覺得前面名稱太長的話，輸入pod.name，用下拉選擇就好。\n\n引號使用&quot; &quot;\n將裡面的字變成一個詞搜尋\n\n\tmessage: &quot;hello world&quot;\n\n不使用引號\n裡面的字都會分別搜尋，已下面來說就是 會搜尋 hello 跟 world\n\nkubernetes.pod.name: hello world\n\nand 跟 or 條件\n多條件查詢\n\n\tmessage: &quot;hello world&quot; and kubernetes.pod.name:*grafana*\n\nnot 否定\n\n\tnot (kubernetes.pod.name: *grafana*)\n\n大於 小於 等於 判斷式 &gt;, &gt;= , &lt;,&lt;=\n\n\t@timestamp &lt; &quot;2021-01-30&quot;\n\n還有一種不使用KQL的查詢，Lucene\n這種語法可支援正則表達式，但KQL不支援。\nref.Lucene query syntax\n\nnested fields 看來是可以查詢整串的json\n但我想不到可以運用在哪\n\n搜尋x-forward-for ip\n有一個log如下圖，這個特別奇怪，不能加雙引號，\n『.』 在kibana裡面似乎有特殊的用法，\n\nkubernetes.namespace: istio-sigua and message: 103.170.26.90*\n\n如果要加雙引號的話，就必須搜尋完整的ip。\nkubernetes.namespace: istio-sigua and message: &quot;221.210.89.170,10.120.17.1&quot;\n\n追根究底，還是要把message對應到欄位...\n沒時間用阿QQ...\n\n搜尋url path\n\nkubernetes.namespace: istio-sigua and message: &quot;img/huawei_ans02.c7bcdbdf.jpg&quot;\nkubernetes.namespace: istio-sigua and message: &quot;img*&quot;\n\n特殊字元\n如果要用特殊字元查詢的話，要加上 \\ ，\n但只限於下列這些關鍵字+ - &amp;&amp; || ! ( ) { } [ ] ^ &quot; ~ * ? : \\ /\n\nref.\n\nKibana Query Language\nKibana查詢語言（KQL）\nKibana查詢語法詳解\nkibana查詢語法\n大神都這麼做，讓 Kibana 搜索語法 query string 也能輕松上手",
		"tags": [ "note","🗒"]
},

{
		"title": "ECK，filebeat LOG蒐集遺失",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/67. ECK，filebeat LOG蒐集遺失/",
		"content": "前言\n正文\n最近同事回報說，pod 有些log沒有進到ECK裏面去，\n一查還真的是這樣，\n但QA環境沒問題，\n偏偏Prod就炸了。\n本來也有想說是不是真的沒有log，\n到filebeat的pod去查，那些沒進去的pod log也都是存在。\nlog路徑\n/var/log/containers/\n\n故先排除，服務沒有寫入log的可能性。\n\n會不會是filebeat在讀取log時的暫存空間不足。\n\n但如果是此情況的話，應該是全部的filebeat都會有log遺失，這次的情況是在某些node上的服務 log都有進到ES，\n其他的node都沒有。但也是有此可能性。\n調整filebeat內部資源\nref.\n\nLogging with EFK in GKE\nConfigure the internal queue\n\nfilebeat一直重啟\n\n之後跑去看filebeat的pod狀態，\n發現有很多個pod常常不斷的重新啟動，\n再到裡面去看詳細內容，\n發現cpu跟 記憶體的使用量很高，\n推測可能是由於資源不足導致。\n查詢filebeat官方建議的使用資源是多少。\nfilebeat建議使用\ncpu : 500Mi\nmemory: 500Mi\n改完後，觀察了約一個禮拜，lod都有持續進去，應該是沒問題，但容量也因此爆增了。\nref. Manage compute resources",
		"tags": [ "note","🗒"]
},

{
		"title": "ECK , api筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/68.ECK , api筆記/",
		"content": "前言\n正文\n在所有的api後面，加上\nv=true 顯示標題\nsort=column name 排序\n\n查詢目前的es記憶體使用量\n\nGET _cat/nodes?v=true&amp;h=name,node*,heap*\n\n詳細的資料\nGET _nodes/stats?filter_path=nodes.*.jvm.mem.pools.old\n\nref.Fix common cluster issues\n\n顯示cluster狀態\n\nGET _cluster/health/\n\n顯示shard 明細\n\nGET _cat/shards/filebeat*?v=true&amp;s=index\n\n顯示index明細\n\nGET _cat/indices/filebeat*?v=true&amp;s=index",
		"tags": [ "note","🗒"]
},

{
		"title": "kibana錯誤解法 , parent Data too large...",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/69. kibana錯誤解法 , parent Data too large.../",
		"content": "前言\n正文\n最近用kibana做查詢或任何的API取資料時，\n有機率會發生下列的錯誤\n\n[parent] Data too large, data for [&lt;http_request&gt;] would be [1054296800/1005.4mb],\nwhich is larger than the limit of [1020054732/972.7mb],\nreal usage: [1054296800/1005.4mb],\nnew bytes reserved: [0/0b], usages [request=83853312/79.9mb,\nfielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b,\neql_sequence=0/0b, accounting=23686052/22.5mb]\n\n雖然說很偶發，但最近有越來越嚴重的趨勢。\n所以，鼻子摸一摸，來解決吧。\n首先看到這一篇 What does this error mean - Data too large, data for [&lt;transport_request&gt;]\n這邊有很好的解釋，發生的原因。\n簡單說就是ES本身的斷路器發生作用，\n導致請求失敗。\n斷路器的文章可參考下面\nref .circuit breaker 斷路器\n上面的錯誤的意思表示，\n\n[&lt;http_request&gt;] would be [1054296800/1005.4mb],\n\nhttp_request的請求需要 1005.4 mb\n\nwhich is larger than the limit of [1020054732/972.7mb],\n\n請求的上限是 972.7mb\n此時可以到 DevTools ，輸入下面指令[1]\nGET _cat/nodes?v=true&amp;h=name,node*,heap*\n\n可以看到目前的記憶體使用量（注意，此圖並不是事發當下的記憶體使用量）\n解決方式[2]：\n\n當下查看heap.max 只有1G ，故先加大記憶體到2G。\n\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\nname: yabo\nnamespace: elastic-system\nspec:\nversion: 7.16.2\nnodeSets:\n- name: all\ncount: 3\npodTemplate:\nspec:\ncontainers:\n- name: elasticsearch\nenv:\n- name: ES_JAVA_OPTS\nvalue: -Xms2g -Xmx2g\nresources:\nrequests:\nmemory: 4Gi\nlimits:\nmemory: 4Gi\n\nref. Manage compute resources\n\nshards數量太多\n\n同樣50G的檔案，5個10G，跟1個50G shards，\n前者的ram使用量會比較多。\n但shards設太高，當node 掛掉時，\n回復的時間會變長。\n修改ILM的shard大小，建議範圍是在 10G~50G 之間，\n但仍要視情況而定。\nref. Size your shards\nref.\n[1] : <a class=\"internal-link\" data-note-icon=\"\" href=\"/🗒 EFK/68.ECK , api筆記/\">68.ECK , api筆記</a>\n[2] : Fix common cluster issues",
		"tags": [ "note","🗒"]
},

{
		"title": "ECK 8.1 ,APM intergration安裝方式",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/80. ECK 8.1 ,APM intergration安裝方式/",
		"content": "前言\n一兩個月沒看，沒想到版本就衝到8.1了，\n照以前的做法，發現一堆錯誤，\nprecondition 'apm integration installed' failed: error querying Elasticsearch for integration index templates: unexpected HTTP status: 404 Not Found..\n正文\n版本升級後，架構變成下圖(1)\n\n所以才導致下圖的錯誤發生，\n\n原因是，ECK升級，\n所以統一要先在kibana上面先安裝 APM intergration。\n\nIf you install version 8.2.0 of APM Server before installing the APM integration, you will see error logs similar to the following. You must go back and install the APM integration before data can be ingested into Elasticsearch.\nref. Upgrade a self-installation of APM Server standalone to 8.2.0\nStep 1.\n\nStep 2.\n安裝APM server即可，安裝完後，\n選擇 『Add Elastic Agent later』。\n\nref.\n1. Upgrade to the Elastic APM integration",
		"tags": [ "note","🗒"]
},

{
		"title": "Elasticsearch筆記",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/84.Elasticsearch筆記/",
		"content": "前言\n上一篇是拿來修正目前版本與2.x版本的差異\n這篇是拿來當筆記\n正文\n\n兩種查詢方式\n\nGET /megacorp/_search?q=last_name:Smith\nGET /megacorp/_search\n{\n&quot;query&quot; : {\n&quot;match&quot; : {\n&quot;last_name&quot; : &quot;Smith&quot;\n}\n}\n}\n\n搜尋方式\n如果用kibana搜尋的話，差異就在一個有加 &quot;&quot;(雙引號)，另一個沒有\nmatch_phrase ： 是有加雙引號的短句搜尋\nmatch ： 是全文搜尋，分數越高的表示關連度越高\n\naggreations(聚合)\n類似sql的 group by 功能\n\n叢集健康度\n\nGET _cluster/health/\n\nstatus 的狀態\ngreen：所有的主分片和副本分片都正常運行。\nyellow：所有的主分片都正常運行，但不是所有的副本分片都正常運行。\nred：有主分片沒能正常運行。\n\n集群node配對方式\ncluster.name相同，就會自動發現集群並加入。",
		"tags": [ "note","🗒"]
},

{
		"title": "elasticsearch ingest pipeline",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/90. elasticsearch ingest pipeline/",
		"content": "前言\n用filebeat 蒐集 nginx的資料，取得的Nginx資料沒再經過分解，\n導致沒辦法運用在efk上面。\n正文\n\n先用UI講解步驟。\n\n先來看一下nginx的message，然後再分析裡面的欄位\n127.0.0.1 - - [06/May/2022:06:44:01 +0000] , http-host: \\&quot;abc.boxing.com\\&quot; , URL: \\&quot;GET /static/tpl/analytics/index.html?v=51817715.1 HTTP/1.1\\&quot; , request-status : \\&quot;200\\&quot; , body-byte: 0 ,http-referer: \\&quot;https://abc.boxing.com/mobile\\&quot; ,user-agent: \\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\\&quot; , X-Forwarded-For : \\&quot;132.235.211.18,10.107.10.14\\&quot; , request-time: \\&quot;0.000\\&quot; , response_time : \\&quot;-\\&quot;\n\n這邊的格式有特別去調整過，詳情可參考，<a class=\"internal-link\" data-note-icon=\"\" href=\"/⎈ k8s/GKE/56. GKE記錄 nginx log/\">56. GKE記錄 nginx log</a>。\n\ningest pipeline\nUI畫面，Stack Management -&gt; Ingest Pipelines-&gt; Create pipeline\n\nAdd a processor\n\nProcessor ： Grok\nField ： message\npatterns:\n\n%{IPORHOST:nginx.source.ip} %{USER:nginx.user.id} %{USER:nginx.user.name} \\\\[%{HTTPDATE:nginx.@timestamp}\\\\] , http-host: \\&quot;%{DATA:nginx.url.host}\\&quot; , URL: \\&quot;%{WORD:nginx.http.request.method} %{DATA:nginx.url.path} HTTP/%{NUMBER:nginx.http.version}\\&quot; , request-status : \\&quot;%{NUMBER:nginx.http.response.status_code:int}\\&quot; , body-byte: %{NUMBER:nginx.http.response.body.bytes:int} ,http-referer: \\&quot;%{DATA:nginx.http.request.referer}\\&quot; ,user-agent: %{QS:nginx.user_agent} , X-Forwarded-For : \\&quot;%{DATA:nginx.http.request.xff}\\&quot; , request-time: \\&quot;%{NUMBER:nginx.http.request.time}\\&quot; , response_time : \\&quot;(?:-|%{NUMBER:nginx.http.request.time:int})\\&quot;\n\n此處的patterns是符合我上面(1)的訊息，所產生的。\nps. Grok Debugger 記得要8.1後才有，如果版本過低，可以參考下方，用DevTools下指令的方式除錯。\nGrok簡單說，\n\n%\n\n型態有哪些，參考連結『 grok-patterns』\nref.\n\nGrok Debugger\nGrok processor\nGrokking grok\ngrok-patterns\n\n測試\n寫好了後，不測試一下也不知道是對或錯，\n到新建的畫面，右邊的中間，有Add Document的選項。\n\n之後輸入要測試的範例\n\n內容為：\n\n{\n&quot;_source&quot;: {\n&quot;message&quot;: &quot;127.0.0.1 - - [06/May/2022:06:44:01 +0000] , http-host: \\&quot;abc.boxing.com\\&quot; , URL: \\&quot;GET /static/tpl/analytics/index.html?v=51817715.1 HTTP/1.1\\&quot; , request-status : \\&quot;200\\&quot; , body-byte: 0 ,http-referer: \\&quot;https://abc.boxing.com/mobile\\&quot; ,user-agent: \\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\\&quot; , X-Forwarded-For : \\&quot;132.235.211.18,10.107.10.14\\&quot; , request-time: \\&quot;0.000\\&quot; , response_time : \\&quot;-\\&quot;&quot;\n}\n}\n\n按下 Run the pipeline，有成功就會跑結果出來了。\n\n失敗的話，會變成下面的圖。\n\n此處皆參考官方範例\nref. Example: Parse logs in the Common Log Format\n使用api測試\n在UI上面測試 Grok實在很累，除了用剛剛提到的 Grok Debug以外。\n就是直接用api方式直接驗證了。\n\nNote, 8.1 我用Grok Debugger 有些特殊字元會無法match ，但用API沒問題。\n\nNginx 為 ingest Pipeline Name\n以下API 皆在 DevTools驗證\n\n驗證Document\n\nPOST _ingest/pipeline/Nginx/_simulate\n{\n&quot;docs&quot;: [\n{\n&quot;_source&quot;: {\n&quot;message&quot;: &quot;127.0.0.1 - - [06/May/2022:06:44:01 +0000] , http-host: \\&quot;abc.boxing.com\\&quot; , URL: \\&quot;GET /static/tpl/analytics/index.html?v=51817715.1 HTTP/1.1\\&quot; , request-status : \\&quot;200\\&quot; , body-byte: 0 ,http-referer: \\&quot;https://abc.boxing.com/mobile\\&quot; ,user-agent: \\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\\&quot; , X-Forwarded-For : \\&quot;132.235.211.18,10.107.10.14\\&quot; , request-time: \\&quot;0.000\\&quot; , response_time : \\&quot;-\\&quot;&quot;\n}\n}\n]\n}\n\n修改grok\n\nPUT _ingest/pipeline/Nginx\n{\n&quot;description&quot;: &quot;My optional pipeline description&quot;,\n&quot;processors&quot;: [\n{\n&quot;grok&quot;: {\n&quot;description&quot;: &quot;Extract fields from 'message'&quot;,\n&quot;field&quot;: &quot;message&quot;,\n&quot;patterns&quot;: [&quot;%{IPORHOST:nginx.source.ip} %{USER:nginx.user.id} %{USER:nginx.user.name} \\\\[%{HTTPDATE:nginx.@timestamp}\\\\] , http-host: \\&quot;%{DATA:nginx.url.host}\\&quot; , URL: \\&quot;%{WORD:nginx.http.request.method} %{DATA:nginx.url.path} HTTP/%{NUMBER:nginx.http.version}\\&quot; , request-status : \\&quot;%{NUMBER:nginx.http.response.status_code:int}\\&quot; , body-byte: %{NUMBER:nginx.http.response.body.bytes:int} ,http-referer: \\&quot;%{DATA:nginx.http.request.referer}\\&quot; ,user-agent: %{QS:nginx.user_agent} , X-Forwarded-For : \\&quot;%{DATA:nginx.http.request.xff}\\&quot; , request-time: \\&quot;%{NUMBER:nginx.http.request.time}\\&quot; , response_time : \\&quot;(?:-|%{NUMBER:nginx.http.request.time:int})\\&quot;&quot;],\n&quot;ignore_failure&quot;: false\n}\n}\n]\n}\n\nignore_failure 為忽略錯誤，false的話，只要有一個沒過，\ningest pipeline就會停止。\n結論\n上述兩種方法，都可以建立且測試，就看哪種習慣。\n但要將ingest 套用到data stream 或 index的話，\n又是另一篇了....。\n不趕時間的方法，到 Index Managemenet 修改 Index Templates\n\n到 index settings 將 default_pipeline 修改為你的 ingest pipeline name\n\n然後就等 lifecycle條件達到時，建立新的index，就會套用上去了。\n趕時間的話，請看下一篇。\nref.\n\nset-default-pipeline\nindex.default_pipeline\nElasticsearch：Elastic可觀測性 - 運用 pipeline 使數據結構化",
		"tags": [ "note","🗒"]
},

{
		"title": "elasticsearch ingest processor補充",
		"date":"Fri Jan 03 2025 10:22:37 GMT+0000 (Coordinated Universal Time)",
		"url":"/🗒 EFK/91. elasticsearch ingest processor補充/",
		"content": "前言\n補充ingest 的 processor\n正文\n上篇提到，正常要等ILM跑完後，所設定的default_pipeline才會寫入。\n但今天是可以提早讓他跑ILM，這樣就會把預設的default_pipeline寫進去了。\n而只要在default_pipeline 設定好，後續再改pipeline的設定時，都會直接影響當下的設定。\n\n在index template設定pipeline\n\n到 Devtools 執行下面指令\n強制datastream執行ILM\n\nPOST /filebeat-8.1.0/_rollover\n\nref.\n- DataStream\n- Elasticsearch 7.X data stream 深入詳解\ningest 補充\nprocessor分成好幾個類型\n上一篇是使用grok(這個是使用正則去篩選字串)，\n有人建議使用Dissect，速度比grok快，但只適用於有固定分隔字串的message。\n下面簡單敘述用途，詳細就點連結過去看了。\n\nAppend\n在現有的字串內增加值\nBytes\n把空間計算方式(kb,mb,gb...)轉成byte\nCircle\n把圓形轉爲近似他們的規則多邊形（我也不知道這能做啥）\nCommunity ID\n計算網路資料流的id，能透過此id關聯相關的網路事件\nConvert\n將欄位內容轉爲integer、long、float、double、string、boolean、ip\nCSV\n將欄位中的的資料當作csv去辨別\nDate\n將欄位內容轉爲日期格式\nDate index name\n將日期取出作爲index的名稱\nDissect\n與grok相似，但不使用正則作爲結構化欄位\nDot expander\n會將 . 視爲一個物件的名稱，\n\nfoo.bar :value\nfoo : {\n\tbar: value\n}\n\nDrop\n根據條件刪除document\nEnrich\n根據欄位規則，自動新增資料\nFail\n當錯誤時，傳送訊息\nFingerprint\n計算document的hash值\nForeach\n當元素數量不確定時使用\nGeoIP\n根據地理位置提供更多資訊\nGrok\n從document中截取結構化的字串出來(正則)\nGsub\n使用正則或取代來轉換字串\nHTML strip\n從欄位中移除html的tag\nInference\nUses a pre-trained data frame analytics model to infer against the data that is being ingested in the pipeline.\nJoin\n將欄位中的array值用分隔符串起來\nJSON\n將json字串轉成json格式\nKV\n指定欄位自動解析訊息\nLowercase\n將字串轉成小寫\nNetwork direction\n根據來源IP及目的IP計算是 inbound或 outbonud\nPipeline\n在pipeline內執行其他的pipeline\nRegistered domain\n解析domain，區分出 sub-doamin、top-level domain\nRemove\n移除欄位\nRename\n更改欄位名稱\nScript\n執行script\nSet\n指令欄位寫值\nSet security user\n通過預處理程序，從目前驗證的使用者截取資料\nSort\n對欄位內的array做排序\nSplit\n使用分隔符號，將字串切成array\nTrim\n移除欄位空白\nUppercase\n將字串轉成大寫\nURL decode\n將URL網址解碼\nURI parts\n將URL請求拆開成URI object\nUser agent\n解析user_agent",
		"tags": [ "note","🗒"]
}
]